{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import logsumexp\n",
    "%pdb on\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  (a * (1 - a ))\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        y = np.atleast_2d(x)\n",
    "        axis = -1\n",
    "        y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "        y = np.exp(y)\n",
    "        summ = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "        out = y / summ\n",
    "        if len(X.shape) == 1: out = out.flatten()    \n",
    "        return out\n",
    "    \n",
    "    def __softmax_deriv(self, a):\n",
    "        #a = softmax(x)\n",
    "        return a * (1 - a)\n",
    "    \n",
    "    def __ReLU(self,x):\n",
    "        return x * (x > 0)\n",
    "    \n",
    "    def __ReLU_deriv(self,a):\n",
    "        return 1 * (a > 0)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__ReLU\n",
    "            self.f_deriv = self.__ReLU_deriv\n",
    "            \n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', dropout=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout=dropout\n",
    "        self.dropout_vector = None\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        self.vel_W = np.zeros(self.W.shape)\n",
    "        self.vel_b = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, mode, batch_norm=False):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        if mode=='train':\n",
    "            if self.dropout:\n",
    "                self.dropout_vector = np.random.binomial(1, 1-self.dropout, size=input.shape[-1])/(1-self.dropout)\n",
    "            lin_output = np.dot(input*self.dropout_vector if self.dropout else input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            lin_output = np.dot(input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "                \n",
    "        self.input=input\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False, batch_norm=False):\n",
    "        \n",
    "        self.grad_W = np.atleast_2d(self.input*self.dropout_vector if self.dropout else self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = np.sum(delta,axis=0)\n",
    "        \n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], dropout=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.mode = 'train'\n",
    "        self.activation=activation\n",
    "        self.dropout=dropout\n",
    "        self.batch_size = 1\n",
    "        self.weight_decay = 0\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],self.dropout[i]))\n",
    "            \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input=input, mode=self.mode, batch_norm=self.batch_norm)\n",
    "            input=output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=error**2\n",
    "        # calculate the delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)\n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    def criterion_CELoss(self,y,y_hat):\n",
    "        error = y * np.log(y_hat)\n",
    "        loss = -np.sum(error)\n",
    "        delta = y_hat-y\n",
    "        return loss,delta\n",
    "        \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True, batch_norm=self.batch_norm)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta,self.batch_norm)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            if self.momentum!=0:\n",
    "                layer.vel_W = layer.vel_W * self.momentum + layer.grad_W * self.lr\n",
    "                layer.vel_b = layer.vel_b * self.momentum + layer.grad_b * self.lr\n",
    "                \n",
    "                layer.W -= (layer.vel_W + layer.W * self.weight_decay)\n",
    "                layer.b -= (layer.vel_b + layer.b * self.weight_decay)\n",
    "            else:\n",
    "                layer.W -= (lr * layer.grad_W + layer.W * self.weight_decay)\n",
    "                layer.b -= (lr * layer.grad_b + layer.b * self.weight_decay)\n",
    "            \n",
    "    def get_batches(self,X, y, batch_size):\n",
    "        batches = []\n",
    "\n",
    "        X, y = shuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_batch = X[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "            \n",
    "            batches.append((X_batch, y_batch))\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=10, batch_size=1, momentum=0, weight_decay=0, batch_norm=0):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.batch_size=batch_size\n",
    "        self.momentum = momentum\n",
    "        self.lr = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        epoch_av_loss = np.zeros(epochs)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "        # Differentiate Stochastic Gradient Descent vs Batch Gradient Descent\n",
    "        if batch_size>1:\n",
    "            batches = self.get_batches(X, y_dummies, batch_size)\n",
    "            for k in range(epochs):\n",
    "                losses = []\n",
    "                for X_batch,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X_batch)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        loss,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                    \n",
    "                    losses.append(loss)\n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                epoch_av_loss[k] = np.sum(losses)/X.shape[0]\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss=np.zeros(X.shape[0])\n",
    "                for it in range(X.shape[0]):\n",
    "                    i=np.random.randint(X.shape[0])\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X[i])\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y_dummies[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y_dummies[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                epoch_av_loss[k] = np.sum(loss)/X.shape[0]\n",
    "        return epoch_av_loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.test()\n",
    "        x = np.array(x)\n",
    "        yhat = self.forward(x)\n",
    "        output = np.argmax(yhat,axis=1)\n",
    "        return output\n",
    "    \n",
    "    def optimize(self, X, y, learning_rate=0.01, test_size=0.25, batch_size=1,epochs=10, momentum=0, weight_decay=0, batch_norm=False, verbose=True):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        self.momentum = momentum\n",
    "        self.lr = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        epoch_av_loss = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        if batch_size>1:\n",
    "            batches = self.get_batches(X_train, y_train, batch_size)\n",
    "            for k in range(epochs):\n",
    "                losses = []\n",
    "                \n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                \n",
    "                self.train()\n",
    "                for X_batch,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X_batch)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        loss,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                    \n",
    "                    losses.append(loss)                      \n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                losses[k] = np.sum(loss)/X_train.shape[0]\n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "                if verbose:\n",
    "                    print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                          format(k, accuracy_train, accuracy_val, losses[k]))\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss = np.zeros(X_train.shape[0])\n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                self.train()\n",
    "                for it in range(X_train.shape[0]):\n",
    "                    i=np.random.randint(X_train.shape[0])\n",
    "                \n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X_train[i])\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y_train[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y_train[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                \n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "                \n",
    "                epoch_av_loss[k] = np.sum(loss)/X_train.shape[0]\n",
    "\n",
    "                if verbose:\n",
    "                    print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                          format(k, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "            \n",
    "                epoch_av_loss[k] = np.mean(loss)\n",
    "        return epoch_av_loss, accuracies_val, accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ndnu2\\AppData\\Local\\Continuum\\anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:189: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\ndnu2\\AppData\\Local\\Continuum\\anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:189: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "X=np.array(data)\n",
    "y=np.array(label)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.8, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "    \n",
    "mlp = MLP([128,256,64,32,10],activation=[None,'ReLU','ReLU','ReLU','softmax'], dropout=[0.0, 0.2, 0.2, 0.2, 0.2])\n",
    "\n",
    "mlp.fit(X_train, y_train, learning_rate=0.001, batch_size=4, momentum=0.9, weight_decay=0.000, epochs=40)\n",
    "predictions = mlp.predict(X_val)\n",
    "#predictions.to_csv('output/Predicted_labels.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy for these settings : {}'.format((predictions==y_val).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and validation sets, use cross validation to optimize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.7602222222222222 \n",
      "Validation Accuracy: 0.7613333333333333 \n",
      "Loss: 0.0006680674091368295 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.8347777777777777 \n",
      "Validation Accuracy: 0.825 \n",
      "Loss: 0.0004403183550218236 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.8548888888888889 \n",
      "Validation Accuracy: 0.8346666666666667 \n",
      "Loss: 0.0003034870892291693 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.8652222222222222 \n",
      "Validation Accuracy: 0.84 \n",
      "Loss: 0.0006503896175060757 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.8712222222222222 \n",
      "Validation Accuracy: 0.846 \n",
      "Loss: 0.0002632946974336045 \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.8725555555555555 \n",
      "Validation Accuracy: 0.848 \n",
      "Loss: 0.00023587130174992883 \n",
      "\n",
      "Epoch: 6..\n",
      "train Accuracy: 0.8886666666666667 \n",
      "Validation Accuracy: 0.858 \n",
      "Loss: 0.00029557181226750354 \n",
      "\n",
      "Epoch: 7..\n",
      "train Accuracy: 0.8853333333333333 \n",
      "Validation Accuracy: 0.8543333333333333 \n",
      "Loss: 0.0002316626498377919 \n",
      "\n",
      "Epoch: 8..\n",
      "train Accuracy: 0.8865555555555555 \n",
      "Validation Accuracy: 0.8546666666666667 \n",
      "Loss: 0.00030173102230208446 \n",
      "\n",
      "Epoch: 9..\n",
      "train Accuracy: 0.8921111111111111 \n",
      "Validation Accuracy: 0.8573333333333333 \n",
      "Loss: 0.0001916585653989613 \n",
      "\n",
      "Epoch: 10..\n",
      "train Accuracy: 0.8958888888888888 \n",
      "Validation Accuracy: 0.856 \n",
      "Loss: 0.00021506344430653886 \n",
      "\n",
      "Epoch: 11..\n",
      "train Accuracy: 0.9011111111111111 \n",
      "Validation Accuracy: 0.8683333333333333 \n",
      "Loss: 0.00018443908458924167 \n",
      "\n",
      "Epoch: 12..\n",
      "train Accuracy: 0.8962222222222223 \n",
      "Validation Accuracy: 0.8523333333333334 \n",
      "Loss: 0.00017099956860541286 \n",
      "\n",
      "Epoch: 13..\n",
      "train Accuracy: 0.9025555555555556 \n",
      "Validation Accuracy: 0.8643333333333333 \n",
      "Loss: 0.00019290255185026227 \n",
      "\n",
      "Epoch: 14..\n",
      "train Accuracy: 0.9052222222222223 \n",
      "Validation Accuracy: 0.865 \n",
      "Loss: 0.00017233591411191395 \n",
      "\n",
      "Epoch: 15..\n",
      "train Accuracy: 0.9078888888888889 \n",
      "Validation Accuracy: 0.863 \n",
      "Loss: 0.00019264008409951197 \n",
      "\n",
      "Epoch: 16..\n",
      "train Accuracy: 0.9014444444444445 \n",
      "Validation Accuracy: 0.856 \n",
      "Loss: 0.00016956389618570585 \n",
      "\n",
      "Epoch: 17..\n",
      "train Accuracy: 0.9084444444444445 \n",
      "Validation Accuracy: 0.8633333333333333 \n",
      "Loss: 0.0002540960357875988 \n",
      "\n",
      "Epoch: 18..\n",
      "train Accuracy: 0.9134444444444444 \n",
      "Validation Accuracy: 0.8626666666666667 \n",
      "Loss: 0.0002663167998913885 \n",
      "\n",
      "Epoch: 19..\n",
      "train Accuracy: 0.9132222222222223 \n",
      "Validation Accuracy: 0.8663333333333333 \n",
      "Loss: 0.00022739760245056687 \n",
      "\n",
      "Epoch: 20..\n",
      "train Accuracy: 0.9134444444444444 \n",
      "Validation Accuracy: 0.8623333333333333 \n",
      "Loss: 0.0002671237459982573 \n",
      "\n",
      "Epoch: 21..\n",
      "train Accuracy: 0.9093333333333333 \n",
      "Validation Accuracy: 0.8593333333333333 \n",
      "Loss: 0.0003678952427878296 \n",
      "\n",
      "Epoch: 22..\n",
      "train Accuracy: 0.9153333333333333 \n",
      "Validation Accuracy: 0.8616666666666667 \n",
      "Loss: 0.0003626523001656145 \n",
      "\n",
      "Epoch: 23..\n",
      "train Accuracy: 0.9102222222222223 \n",
      "Validation Accuracy: 0.8596666666666667 \n",
      "Loss: 0.00018517466531229956 \n",
      "\n",
      "Epoch: 24..\n",
      "train Accuracy: 0.922 \n",
      "Validation Accuracy: 0.868 \n",
      "Loss: 0.0001529179653000673 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "X=np.array(data)\n",
    "y=np.array(label)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.8, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "mlp = MLP([128,256,64,32,10],activation=[None,'ReLU', 'ReLU', 'ReLU','softmax'], dropout=[0.1, 0.2, 0.2, 0])\n",
    "losses, accuracies_train, accuracies_val = mlp.optimize(X_train, y_train, learning_rate=0.0001, batch_size=5, momentum=0.9, weight_decay=0.0, epochs=25)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_val, label='val')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_relu.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A good performing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'batch_norm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-92634df9a609>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-4e0e31c82ea0>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, batch_size, epochs, momentum, weight_decay, batch_norm, verbose)\u001b[0m\n\u001b[0;32m    365\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m                 \u001b[0myhat_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m                 \u001b[0myhat_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m                 \u001b[1;31m# Calculate train and Test Accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-4e0e31c82ea0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'batch_norm'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-1-4e0e31c82ea0>\u001b[0m(173)\u001b[0;36mforward\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m    171 \u001b[1;33m    \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    172 \u001b[1;33m        \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m--> 173 \u001b[1;33m            \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    174 \u001b[1;33m            \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    175 \u001b[1;33m        \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,512,128,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, batch_size=1,learning_rate=0.01,epochs=50)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.6924 - acc: 0.7761 - categorical_accuracy: 0.7761\n",
      "Epoch 2/10\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3802 - acc: 0.8643 - categorical_accuracy: 0.8643\n",
      "Epoch 3/10\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3382 - acc: 0.8790 - categorical_accuracy: 0.8790\n",
      "Epoch 4/10\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3134 - acc: 0.8856 - categorical_accuracy: 0.8856\n",
      "Epoch 5/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2941 - acc: 0.8928 - categorical_accuracy: 0.8928\n",
      "Epoch 6/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2786 - acc: 0.8996 - categorical_accuracy: 0.8996\n",
      "Epoch 7/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2652 - acc: 0.9025 - categorical_accuracy: 0.9025\n",
      "Epoch 8/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2534 - acc: 0.9081 - categorical_accuracy: 0.9081\n",
      "Epoch 9/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2440 - acc: 0.9110 - categorical_accuracy: 0.9110\n",
      "Epoch 10/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2352 - acc: 0.9141 - categorical_accuracy: 0.9141\n",
      "Keras model accuracy : 0.8776666666666667\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras import optimizers, metrics, Sequential\n",
    "\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tensorflow.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \n",
    "sess = tensorflow.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "y = label\n",
    "X = data\n",
    "\n",
    "y_dummies = np.array(pd.get_dummies(y))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=0.25, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "with tf.device('GPU'):\n",
    "    sgd = optimizers.sgd(lr=0.01)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=10)\n",
    "\n",
    "#Predict and calculate accuracy\n",
    "yhat_val = model.predict(X_val)\n",
    "\n",
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "\n",
    "print('Keras model accuracy : {}'.format(accuracy_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'yhat_train' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-21e7dcd9f6ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-129-77e6a3e7d998>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, batch_size, epochs, verbose)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;31m# Calculate train and Test Accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m         \u001b[0maccuracy_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m         \u001b[0maccuracy_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'yhat_train' referenced before assignment"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.1,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(32,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (32,) and (1,1) not aligned: 32 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-9e969c7d6dab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, epochs, verbose)\u001b[0m\n\u001b[0;32m    314\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion_MSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[1;31m# update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta, output_layer)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_vector\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (32,) and (1,1) not aligned: 32 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,512,128,32,10],activation=[None, 'logistic', 'logistic', 'logistic', 'softmax'], dropout=[0.0, 0.0, 0.0, 0.0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.02,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tensorflow.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \n",
    "sess = tensorflow.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_24 to have shape (10,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-8f69afa4341a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical_accuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#Predict and calculate accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_24 to have shape (10,) but got array with shape (1,)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\dnuho\\anaconda3\\envs\\data\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m(138)\u001b[0;36mstandardize_input_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m    136 \u001b[1;33m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    137 \u001b[1;33m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m--> 138 \u001b[1;33m                            str(data_shape))\n",
      "\u001b[0m\u001b[1;32m    139 \u001b[1;33m    \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    140 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras import optimizers, metrics, Sequential\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "y = label\n",
    "X = data\n",
    "\n",
    "y_dummies = np.array(pd.get_dummies(y))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=0.25, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "with tf.device('GPU'):\n",
    "    sgd = optimizers.sgd(lr=0.1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=20)\n",
    "\n",
    "#Predict and calculate accuracy\n",
    "yhat_val = model.predict(X_val)\n",
    "\n",
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "\n",
    "print('Keras model accuracy : {}'.format(accuracy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosscheck with Keras Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-4c2f6c2ef006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0msgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-117-4c2f6c2ef006>\u001b[0m(1)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m----> 1 \u001b[1;33m\u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      2 \u001b[1;33m    \u001b[0msgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      3 \u001b[1;33m    \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      4 \u001b[1;33m    \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      5 \u001b[1;33m    \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with tf.device('GPU'):\n",
    "    sgd = optimizers.sgd()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict and calculate accuracy\n",
    "yhat_val = model.predict(X_val)\n",
    "\n",
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "\n",
    "print('Keras model accuracy : {}'.format(accuracy_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import logsumexp\n",
    "%pdb on\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  (a * (1 - a ))\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        y = np.atleast_2d(x)\n",
    "        axis = -1\n",
    "        y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "        y = np.exp(y)\n",
    "        summ = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "        out = y / summ\n",
    "        if len(X.shape) == 1: out = out.flatten()    \n",
    "        return out\n",
    "    \n",
    "    def __softmax_deriv(self, a):\n",
    "        #a = softmax(x)\n",
    "        return a * (1 - a)\n",
    "    \n",
    "    def __ReLU(self,x):\n",
    "        return x * (x > 0)\n",
    "    \n",
    "    def __ReLU_deriv(self,a):\n",
    "        return 1 * (a > 0)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__ReLU\n",
    "            self.f_deriv = self.__ReLU_deriv\n",
    "            \n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', dropout=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout=dropout\n",
    "        self.dropout_vector = None\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        self.vel_W = np.zeros(self.W.shape)\n",
    "        self.vel_b = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, mode):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        if (mode=='train' and self.dropout>0):\n",
    "            self.dropout_vector = np.random.binomial(1, 1-self.dropout, size=input.shape)/(1-self.dropout)\n",
    "            lin_output = np.dot(self.dropout_vector*input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            lin_output = np.dot(input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "                \n",
    "        self.input=input\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):\n",
    "        \n",
    "        self.grad_W = np.atleast_2d(self.dropout_vector*self.input if self.dropout>0 else self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = np.sum(delta,axis=0)\n",
    "        \n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], dropout=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.mode = 'train'\n",
    "        self.activation=activation\n",
    "        self.dropout=dropout\n",
    "        self.batch_size = 1\n",
    "        self.weight_decay = 0\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],self.dropout[i]))\n",
    "            \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input=input, mode=self.mode, batch_norm=False)\n",
    "            input=output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=error**2\n",
    "        # calculate the delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)\n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    def criterion_CELoss(self,y,y_hat):\n",
    "        error = y * np.log(y_hat)\n",
    "        loss = -np.sum(error)\n",
    "        delta = y_hat-y\n",
    "        return loss,delta\n",
    "        \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            if self.momentum!=0:\n",
    "                layer.vel_W = layer.vel_W * self.momentum + layer.grad_W * self.lr\n",
    "                layer.vel_b = layer.vel_b * self.momentum + layer.grad_b * self.lr\n",
    "                \n",
    "                layer.W -= (layer.vel_W + layer.W * self.weight_decay)\n",
    "                layer.b -= (layer.vel_b + layer.b * self.weight_decay)\n",
    "            else:\n",
    "                layer.W -= (lr * layer.grad_W + layer.W * self.weight_decay)\n",
    "                layer.b -= (lr * layer.grad_b + layer.b * self.weight_decay)\n",
    "            \n",
    "    def get_batches(self,X, y, batch_size):\n",
    "        batches = []\n",
    "\n",
    "        X, y = shuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_batch = X[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "            \n",
    "            batches.append((X_batch, y_batch))\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=10, batch_size=1, momentum=0, weight_decay=0, batch_norm=0):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.batch_size=batch_size\n",
    "        self.momentum = momentum\n",
    "        self.lr = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        epoch_av_loss = np.zeros(epochs)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "        # Differentiate Stochastic Gradient Descent vs Batch Gradient Descent\n",
    "        if batch_size>1:\n",
    "            batches = self.get_batches(X, y_dummies, batch_size)\n",
    "            for k in range(epochs):\n",
    "                losses = []\n",
    "                for X_batch,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X_batch)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        loss,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                    \n",
    "                    losses.append(loss)\n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                epoch_av_loss[k] = np.sum(losses)/X.shape[0]\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss=np.zeros(X.shape[0])\n",
    "                for it in range(X.shape[0]):\n",
    "                    i=np.random.randint(X.shape[0])\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X[i])\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y_dummies[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y_dummies[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                epoch_av_loss[k] = np.sum(loss)/X.shape[0]\n",
    "        return epoch_av_loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.test()\n",
    "        x = np.array(x)\n",
    "        yhat = self.forward(x)\n",
    "        output = np.argmax(yhat,axis=1)\n",
    "        return output\n",
    "    \n",
    "    def optimize(self, X, y, learning_rate=0.01, test_size=0.25, batch_size=1,epochs=10, momentum=0, weight_decay=0, batch_norm=False, verbose=True):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        self.momentum = momentum\n",
    "        self.lr = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        epoch_av_loss = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        if batch_size>1:\n",
    "            batches = self.get_batches(X_train, y_train, batch_size)\n",
    "            for k in range(epochs):\n",
    "                losses = []\n",
    "                \n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                \n",
    "                self.train()\n",
    "                for X_batch,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X_batch)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        loss,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                    \n",
    "                    losses.append(loss)                      \n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                losses[k] = np.sum(loss)/X_train.shape[0]\n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "                if verbose:\n",
    "                    print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                          format(k, accuracy_train, accuracy_val, losses[k]))\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss = np.zeros(X_train.shape[0])\n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                self.train()\n",
    "                for it in range(X_train.shape[0]):\n",
    "                    i=np.random.randint(X_train.shape[0])\n",
    "                \n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X_train[i])\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y_train[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y_train[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                \n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "                \n",
    "                epoch_av_loss[k] = np.sum(loss)/X_train.shape[0]\n",
    "\n",
    "                if verbose:\n",
    "                    print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                          format(k, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "            \n",
    "                epoch_av_loss[k] = np.mean(loss)\n",
    "        return epoch_av_loss, accuracies_val, accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "X=np.array(data)\n",
    "y=np.array(label)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.8, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "    \n",
    "mlp = MLP([128,512,128,32,10],activation=[None,'ReLU','ReLU','ReLU','softmax'], dropout=[0, 0.1, 0.1, 0.1])\n",
    "\n",
    "mlp.fit(X_train, y_train, learning_rate=0.001, batch_size=16, momentum=0.5, weight_decay=0.001, epochs=20)\n",
    "predictions = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for these settings : 0.8302708333333333\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for these settings : {}'.format((np.argmax(predictions,axis=1)==y_val).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and validation sets, use cross validation to optimize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.8692222222222222 \n",
      "Validation Accuracy: 0.8426666666666667 \n",
      "Loss: 0.661531557065787 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.9065555555555556 \n",
      "Validation Accuracy: 0.8566666666666667 \n",
      "Loss: 0.3965654753324722 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.914 \n",
      "Validation Accuracy: 0.846 \n",
      "Loss: 0.3159149037357991 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.9252222222222222 \n",
      "Validation Accuracy: 0.846 \n",
      "Loss: 0.2865796331873359 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.919 \n",
      "Validation Accuracy: 0.839 \n",
      "Loss: 0.24887520792588635 \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.9463333333333334 \n",
      "Validation Accuracy: 0.8546666666666667 \n",
      "Loss: 0.24131069568644695 \n",
      "\n",
      "Epoch: 6..\n",
      "train Accuracy: 0.9412222222222222 \n",
      "Validation Accuracy: 0.8463333333333334 \n",
      "Loss: 0.25034676971417774 \n",
      "\n",
      "Epoch: 7..\n",
      "train Accuracy: 0.955 \n",
      "Validation Accuracy: 0.858 \n",
      "Loss: 0.2608310840043615 \n",
      "\n",
      "Epoch: 8..\n",
      "train Accuracy: 0.9436666666666667 \n",
      "Validation Accuracy: 0.8363333333333334 \n",
      "Loss: 0.26792041776850695 \n",
      "\n",
      "Epoch: 9..\n",
      "train Accuracy: 0.9528888888888889 \n",
      "Validation Accuracy: 0.8556666666666667 \n",
      "Loss: 0.2461673631808396 \n",
      "\n",
      "Epoch: 10..\n",
      "train Accuracy: 0.9553333333333334 \n",
      "Validation Accuracy: 0.85 \n",
      "Loss: 0.26150885998983686 \n",
      "\n",
      "Epoch: 11..\n",
      "train Accuracy: 0.9584444444444444 \n",
      "Validation Accuracy: 0.8416666666666667 \n",
      "Loss: 0.28157409399721 \n",
      "\n",
      "Epoch: 12..\n",
      "train Accuracy: 0.9562222222222222 \n",
      "Validation Accuracy: 0.8443333333333334 \n",
      "Loss: 0.32589129640052383 \n",
      "\n",
      "Epoch: 13..\n",
      "train Accuracy: 0.9441111111111111 \n",
      "Validation Accuracy: 0.835 \n",
      "Loss: 0.41045272495841295 \n",
      "\n",
      "Epoch: 14..\n",
      "train Accuracy: 0.9414444444444444 \n",
      "Validation Accuracy: 0.849 \n",
      "Loss: 0.427273430120981 \n",
      "\n",
      "Epoch: 15..\n",
      "train Accuracy: 0.9446666666666667 \n",
      "Validation Accuracy: 0.828 \n",
      "Loss: 0.4952492381907413 \n",
      "\n",
      "Epoch: 16..\n",
      "train Accuracy: 0.9306666666666666 \n",
      "Validation Accuracy: 0.8286666666666667 \n",
      "Loss: 0.5107801329673883 \n",
      "\n",
      "Epoch: 17..\n",
      "train Accuracy: 0.94 \n",
      "Validation Accuracy: 0.845 \n",
      "Loss: 0.5854682790714529 \n",
      "\n",
      "Epoch: 18..\n",
      "train Accuracy: 0.9418888888888889 \n",
      "Validation Accuracy: 0.8466666666666667 \n",
      "Loss: 0.6776061880345079 \n",
      "\n",
      "Epoch: 19..\n",
      "train Accuracy: 0.9375555555555556 \n",
      "Validation Accuracy: 0.8426666666666667 \n",
      "Loss: 0.6359199636502784 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:188: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:188: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20..\n",
      "train Accuracy: 0.9265555555555556 \n",
      "Validation Accuracy: 0.8193333333333334 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 21..\n",
      "train Accuracy: 0.9092222222222223 \n",
      "Validation Accuracy: 0.8276666666666667 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 22..\n",
      "train Accuracy: 0.8922222222222222 \n",
      "Validation Accuracy: 0.798 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 23..\n",
      "train Accuracy: 0.8896666666666667 \n",
      "Validation Accuracy: 0.8196666666666667 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 24..\n",
      "train Accuracy: 0.8676666666666667 \n",
      "Validation Accuracy: 0.7866666666666666 \n",
      "Loss: nan \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3XlYVdX6wPHvYlZBkEFUcMABFRVRcTaHbHAeyxyyupVaWTe7t3m+/Zpu3eayTDMbHNImrWwOnBUxcUqccMIJEEFUZtbvj3UoQpQDHDgHeD/PwyPss/fa7yk9L2vttd6ltNYIIYQQjsbJ3gEIIYQQJZEEJYQQwiFJghJCCOGQJEEJIYRwSJKghBBCOCRJUEIIIRySJCghhBAOSRKUEEIIhyQJSgghhENysXcAZeHv769btGhh7zCEEEJUwJYtW1K01gGlnVetElSLFi2IjY21dxhCCCEqQCl12JrzZIhPCCGEQ5IEJYQQwiFJghJCCOGQqtUzKCGEqO5yc3NJTEwkKyvL3qFUOg8PD4KDg3F1dS3X9ZKghBCiCiUmJuLl5UWLFi1QStk7nEqjteb06dMkJiYSEhJSrjZkiE8IIapQVlYWfn5+NTo5ASil8PPzq1BPURKUEEJUsZqenApV9H1KghJCCOGQJEEJYSMFBZrtiWm88cs+Hli2jflrD7LlcCqZOfn2Dk2Iv0lLS2P27Nllvm7YsGGkpaVVQkQlk0kSQlRA+oVcVu9LJmpPEqv3JpNyLgelwLeuG8u2JALg7KQIDfQiPMib8KbedA72ITTQCzcX+f1Q2Edhgrrrrrv+djw/Px9nZ+dLXrdy5crKDu1vJEEJUQZaa/44cZboPclExSfx+5EzFGjwqevKgNAABrVtyBVt/PHzdOfU2Sy2J6azPTGNbYnp/PTHST6LPQqAm4sT7RvXp3OwN52CvOnc1IdWAZ44O9WOZxPC+M83u/jj+FmbthnWpD5Pjexw2XMefvhhDhw4QEREBK6urnh6etK4cWPi4uL4448/GDNmDEePHiUrK4t7772X6dOnA3+Vmzt37hxDhw6lX79+rF+/nqCgIJYvX06dOnVs+l4kQQlRirNZuazbl0LUniSi9ySTlJENQKcgb+4e1JoBbRsS0dTnouQSWN+Dq8M8uDosEDDJLfFMJtsS09iRmM62xDS+/P0YH28wZcnqujnTp5Ufr0yIwLtO+daNCGGNF198kZ07dxIXF0d0dDTDhw9n586df04Hnz9/Pr6+vmRmZtK9e3fGjx+Pn5/f39rYt28fixcvZu7cuUyYMIEvvviCG2+80aZxSoISVS4rN5/8Ak09d8f863cuO49tR9PYcvgM6w+kEHvoDHkFGi8PF/qHBjAwNIABbQNo6OVRpnaVUjT1rUtT37qMCG8CmOdWCSnn2J6YzrajaSyKOcKMT2L56NYeuLtceqhF1Ayl9XSqSo8ePf62VunNN9/kq6++AuDo0aPs27fvogQVEhJCREQEAN26dePQoUM2j8sxPyFEjXU+O4/x764n7UIuy+7oTVPfunaNR2vN0dRMthxJZcvhM2w5nMaek2cp0KAUtGtUn2n9WzKobUO6NvPBxdm2z42cnBStG3rRuqEX47oG06VZA2Z9Fse/l27jzYldcJIhP1EF6tWr9+f30dHR/PLLL2zYsIG6desycODAEtcyubu7//m9s7MzmZmZNo9LEpSoMlprHvpiO3tPZVDP3YXJ8zaybEYfGnmXrSdSEVm5+ew6nm5JRiYhpZwzQ3ae7i50aebDNVe2oVvzBkQ086G+R9UOtY3pEsSps1m88H08jep78PiIsCq9v6gdvLy8yMjIKPG19PR0GjRoQN26dYmPj2fjxo1VHN1fJEGJKjN/3SG+3X6CB65tS7/W/kyZt4kp8zby2Yze+Hu6l95AOe07lcHS2KNsOXyGncfOkpNfAEBzv7r0b+NP1+YN6Na8AaGBXg4xSWF6/5acSM9i3tqDNPL24PYrWto7JFHD+Pn50bdvXzp27EidOnUIDAz887UhQ4bw3nvvER4eTtu2benVq5fd4lRaa7vdvKwiIyO1bFhYPcUcTGXS3I0MbteQOVO7oZRiU8Jpbv4whhB/T5ZM64V3Xdv3Vn6LP8Xdi7aSV6AJD/KmW/MGdG3egK7NGhDgVXlJsaLyCzT3LP6dlTtO8uakLozq3MTeIQkb2b17N+3bt7d3GFWmpPerlNqitY4s7VqrBtSVUkOUUnuUUvuVUg+X8HpzpdSvSqntSqlopVRwkdfylVJxlq8VRY6HKKU2KaX2KaU+U0q5WROLqH5Onc3iroW/09y3Lv+b0PnP8ic9W/oxZ2okB5LOcfOHMZzLzrPpfT9cd5DbP4qlZUA91jw4iM/v7MMjw9pzbYdGDp2cwKydenVCBD1CfPn30jjW70+xd0hCVLlSE5RSyhl4BxgKhAGTlFLFB8b/B3ystQ4HngFeKPJaptY6wvI1qsjx/wKvaa3bAGeA2yrwPoSDyskr4K6Fv3MhJ4/3pna76JnOgNAA3prchR3H0rltwWabVF3Iyy/gqeU7+c83fzC4fSBLZ/QmsH7VPeeyFQ9XZ+ZOjSTEvx4zPtnC7hO2XS8jhKOzpgfVA9ivtU7QWucAS4DRxc4JA361fB9Vwut/o8yv0FcCn1sOfQSMsTZoUX08v3I3Ww6f4b/jwwkN9CrxnGs7NOLVCZ2JOZTKnQu3kJNXUO77ncvOY9rHsXy04TDTrgjhvRu7Udet+j5q9a7ryoJ/9KCeuwu3fBjDsTTbz5QSwlFZk6CCgKNFfk60HCtqGzDe8v1YwEspVThp3kMpFauU2qiUKkxCfkCa1rpwTKekNgFQSk23XB+bnJxsRbjCUXy1NZEF6w9xW78QRpbyDGV0RBAvjO1E9J5k7l2ylbz8siep42mZXPfuelbvS+G5sR15bHiYQ0x6qKgmPnVYcGt3LuTkc/P8GNIu5Ng7JCGqhDUJqqR/4cVnVtwPDFBKbQUGAMeAwuTTzPIwbDLwulKqlZVtmoNav6+1jtRaRwYEBFgRrnAEu0+c5ZEvd9AjxJeHh7az6pqJPZrxxIgwvt95kgc/305BgfUTeHYkpjPmnXUcO5PJh7d0Z0rP5uUN3SG1a1Sf96dGcuT0BW7/KJasXClAK2o+axJUItC0yM/BwPGiJ2itj2utx2mtuwCPWY6lF75m+TMBiAa6ACmAj1LK5VJtiuorPTOXOz7dgncdV96Z3BXXMixuva1fCPdfE8qXW4/xxPKdWDPL9MddJ5kwZwOuzk58fmcf+ofWzF9kerfy49UbOrPlyBlmLYkjvwwJXIjqyJpPjs1AG8usOzdgIrCi6AlKKX+lVGFbjwDzLccbKKXcC88B+gJ/aPOpEwVcZ7nmZmB5Rd+MsL+CAs2/PovjeFoms6d0LddsuZmDWnPnwFYs3HSE51fuvmSS0lozd3UCd3y6hdBGXnw9sy9tG5X8nKumGBHehCeGh/HDrpP855tdViVwISrK09PTLvct9emx1jpPKXU38CPgDMzXWu9SSj0DxGqtVwADgReUUhpYDcy0XN4emKOUKsAkwxe11n9YXnsIWKKUehbYCnxgw/clyuCRL7dzPC2LGQNa0rtlxbaifjtqP7/GJ/HM6A50a+5brjaUUjx4bVsuZOcxd81B6rm7MOuq0L+dk5tfwFMrdrFo0xGGdWrEK9dHUMetdtSuu7VfCCfPZvH+6gQaeXtw18DW9g5JiEph1fQmrfVKYGWxY08W+f5z/pqRV/Sc9UCnS7SZgJkhKOzo8OnzLI45iquzYtXeZCKa+nDXwFZc1T6wzHXgovYk8dovexnXJYipvSr2DEgpxVMjO3AhJ5/Xf9lHXTdnpvdvBZjq4jMX/s6afSncObAVD1zTttbVrHt4SDtOpmfx0g97CPTyYHy34NIvEo7n+4fh5A7bttmoEwx98bKnPPTQQzRv3vzP/aCefvpplFKsXr2aM2fOkJuby7PPPsvo0ZedkF3pqu/8W2ETSzYfxUnBL/8awJp9KcxZfYDpn2whNNCTOwa0YmTnJlY9QzqaeoFZS+Jo16g+z43tVKFeWCEnJ8WL48O5kJvP8yvjqePmwsDQAG77aDMJyed5aXw4E7o3Lb2hGsjJSfHy9eGknMvmoS+2E+DlXunP3rTW7Dx2ll/jT7H3VAZPj+xAw2q4vkzAxIkTmTVr1p8JaunSpfzwww/cd9991K9fn5SUFHr16sWoUaNs8m+5vCRB1WK5+QUsi03kynYNae5Xj+Z+9ZjYvSnfbj/Bu9EH+NfSbbz6815m9G/J9ZFN8XAteQgtKzefGZ9sQWvNezd2telQm7OT4rUJEWTl5PPE1zvxruOK1pqPb+1Bn9b+NrtPdeTu4sycqd2YMGcjt320mS5NG9CzpS89Q/zo2tzHJuu/MnPyWbs/hd/iT/FbfBKnzmajFDgrRUpGDgun9SzTJBhRTCk9ncrSpUsXkpKSOH78OMnJyTRo0IDGjRtz3333sXr1apycnDh27BinTp2iUaNGdokRJEHVar/uTiLlXDYTuzf785iLsxNjugQxqnMTfotPYnb0fp5Yvos3ft3Hrf1CuLFX879Vg9Ba89hXO/njxFnm3xJJc796Jd2qQtxcnHhnSldmfLKFI6kXmHtTJK0b2uehraPx8nDl41t7MG9tAhsTUpkdfYC3ftuPi5MiPNibXi396NnSj8jmDazef+t4Wia/xifx2+5TrD9wmuy8AjzdXegf6s+V7QIZ1DaANftSmPVZHC//uIdHh9WeunI1yXXXXcfnn3/OyZMnmThxIgsXLiQ5OZktW7bg6upKixYtStxmoypJgqrFlmw+QmB9dwa2vXhoyMlJcVVYIIPbN2TTwVTeidrPSz/s4d2oA0zt3Zxb+4Xg7+nOwk1H+OL3RO4d3IYr2wWWcBfb8HB1ZsE/uqM1te55U2kCvNx5ZKhJEuey84g9lMqmg6lsTDjN+6sTmB19AGcnRccgb3qF+NKrpR+RLRrgZflFo6BAsy0xjd/ik/hld9KfJZWa+dZlcs9mDG4XSI8QX9xc/uopjekSROzhVN5fnUDXZj4M6di46t94KQoKNGcu5OBbz82uw1SOauLEiUybNo2UlBRWrVrF0qVLadiwIa6urkRFRXH48GF7hygJqrY6lpbJqr3J3D2o9WU34VNK0aulH71a+rEjMZ13V+3n3VUH+GDtQUZ2bsLyuGMMahvAvYPbVHrMSinkc+byPN1dGNi2IQPbNgTgQk4eWw6fYVOCSVjz1x1kzuoEnBR0aOJNc7+6bEw4Tcq5HJwURLbw5ZGh7RjcviGtAjwv+8H+xIgwdiSm88Cy7bRtVJ8Qf9v3nq2Vej6H+JNn2Xsygz2nMog/mcHekxmcz8mnbaAXk3o0ZWzXYLzrVO3+Xo6sQ4cOZGRkEBQUROPGjZkyZQojR44kMjKSiIgI2rWzboF9ZZLtNmqp137ey5u/7WP1A4PKvKvtgeRzvBd9gK+2HqOxjwff3N0Pn7pSjL46yMzJZ+uRM2y09LAOnz5PjxA/rmrfkAGhAWX+/5h45gIj3lpLo/oefHVX30qf6p+Zk8++pL8SUGEySs7I/vMcn7qutA30ol0jLwK9Pfhh50m2J6bj4erEiPAmTO7ZjC5NfezWq5LtNqzfbkMSVC2UX6Dp99/faBPoxce3ln+mf1JGFq5OTjSoJ8mpNovek8Q/FmxmbJcgXrm+s80/+I+nZfLC9/HsPJbOodPnKfzIcndxok2gJ20D69OukRehjUxSaujlflEMO4+ls3DTEVbEHeN8Tj7tGnkxuWczxnQJqvJdkyVBWZ+gZIivFlq1N4kT6Vk8WcHtxBt6yRRjAQPbNuSfV7bhjV/3Edncl8k9m5V+kZV2Hkvn1gWbOZ+dxxVtAhjVucmfyaiFXz2riwF3DPLmhXGdeGx4e1bEHWdRzGGeXL6LF1bGM7JzYyb1aEaEHXtVomSSoGqhxTFH8fd0Y3D7ypvUIGqXfw5uw9ajaTy9Yhcdg+oTHuxT4TZ/3X2KexZvxaeOK1/eZZsyVp7uLkzu2YzJPZuxPTGNxTFHWB53nKWxibRvXN/0qiKa/DmBpLJorWtFMqzoCJ0sYKhlks5m8Vt8EuO7Bf9tVpYQFeHspHj9hgj8Pd2489PfK7wlyMcbDjHt41haBXhWWo3F8GAfXhgXzqZHB/PsmI4o4Imvd9Lz+V95+IvtJJ2tnCnWHh4enD59usbXUdRac/r0aTw8yj/SIs+gapl3ovbz8o97iLp/oF1nXYmaKe5oGte/t55+rf354ObuZV4SkF+geX7lbj5Ye5Cr2jfkzUldqmzDSa012xPTWbTpCF/HHSOoQR2WTO9l86Hs3NxcEhMT7b7GqCp4eHgQHByMq+vfe6QySUJcpKBAM+B/UQT51GHJ9N72DkfUUJ9sOMQTy3fx76tDuacMyw8yc/KZ9dlWftx1ilv6tOCJEfbbcDLmYCo3z48huEEdFk/vhb9n2avyi0uzNkHJGE8tsv7AaY6mZjKph+0eYgtR3I29mjMmogmv/rKXNfus2wU7OSObiXM38tMfp3hyRBhPj+pg192Qe4T4Mv+W7hw9c4Eb520i9bzsYmwPkqBqkcWbj+BT15VrO9ivtpao+ZRSPD+uE20aenLvErM32OXsO5XB2Nnr2HPyLHNu7Mat/UKqKNLL693Kjw9u7s7BlPPcOG9ThZ+ribKTBFVLnD6XzU+7TjKuS/Ali74KYSt13Vx498ZuZOfmM3PR7+TkFZR43vr9KYx7dz1ZuQUsndGbaxzsl6e+rf15/6ZI9iedY+oHMaRn5to7pFpFElQt8cXvieTmayb1qJ3bU4iq1yrAk5ev78zWI2k8v3L3Ra9/viWRm+bH0Njbg69n9rHJ1PTKMCA0gDlTuxF/8iw3zY/hbJYkqaoiCaoW0FqzZPNRujVvQJvAmr0lunAswzo15rZ+ISxYf4gV244D5u/jqz/v5f5l2+jZ0pdld/QhuEHZym1VtUHtGjJ7Sjd2HUvnlvkxnMvOs3dItYIkqFog5mAqCcnnmVhLN/cT9vXw0HZENm/Aw19sZ9fxdP69dBtv/rqP67sF8+EtPapNAderwwJ5e3IXtiWm848PYzgvSarSSYKqBZZsPoqXuwvDwx1vSwRR87k6O/H25K7UdXNm1Nvr+HLrMe6/JpSXrguvdovFh3RszBsTI9hy+Ay3fbSZzJx8e4dUo1Wvvx2izNIv5LJyxwlGd2lSZQsehSiukbcHb07qQmNvD96YGMHdV7aptqV+RoQ34bUbIog5mMrtH28mK1eSVGWRT6wa7qutiWTnFcjaJ2F3fVr5s/ahK+0dhk2MjggiL19z/+fbmPZxLHNvipTZsZVAelA1WOHkiPBgbzo08bZ3OELUKOO7BfPfceGs2ZfCnZ9uITtPelK2JgmqBos7mkb8yQwmdpfekxCVYUL3pjw/thNRe5KZuXDrJdd7ifKxKkEppYYopfYopfYrpR4u4fXmSqlflVLblVLRSqlgy/EIpdQGpdQuy2s3FLlmgVLqoFIqzvIVYbu3JQAWxxwxD6Yjmtg7FCFqrMk9m/HM6A78svsU/1y8ldx8SVK2UmqCUko5A+8AQ4EwYJJSqvhOd/8DPtZahwPPAC9Yjl8AbtJadwCGAK8rpYquxntAax1h+Yqr4HsRRWRk5fLNthOMDG+Cp7s8ahSiMt3UuwVPjgjjh10neeWnvfYOp8awpgfVA9ivtU7QWucAS4DRxc4JA361fB9V+LrWeq/Wep/l++NAEhBgi8BrsjX7kok/ebZCbazYdpzM3HwmSuUIIarErf1CmBAZzLw1CRX+9ysMaxJUEHC0yM+JlmNFbQPGW74fC3gppfyKnqCU6gG4AQeKHH7OMvT3mlJK6tljhuWmfhDD0DfW8MiXO0g5l12udpbEHKVdIy8imjpm+RghaqJHhranfh1XHvlyBwUF1WcrI0dlTYIqabFC8f/y9wMDlFJbgQHAMeDPZdZKqcbAJ8A/tNaFA7SPAO2A7oAv8FCJN1dqulIqVikVm5xsXen+6uqrrYk8+tUOBoQG8I8+ISyLPcqgl6OZtyahTA9fdx5LZ8exdCb1aFZt15oIUR01qOfGY8Pas/VIGos3H7F3ONWeNQkqESg6ThQMHC96gtb6uNZ6nNa6C/CY5Vg6gFKqPvAd8LjWemORa05oIxv4EDOUeBGt9fta60itdWRAQM0dHVy54wT/XrqN3i39mDO1G0+ODOOHWf3p1qIBz363myGvr+a3+FNWbRO9ZPMR3F2cGBNRvKMrhKhs47oG0bulHy9+H09SRs3fNbcyWZOgNgNtlFIhSik3YCKwougJSil/pVRhW48A8y3H3YCvMBMolhW7prHlTwWMAXZW5I1UZ79aZv90bdbgbwv+Wjf0ZME/evDhLd1Bwa0LYrn5w83sT8q4ZFsXcvJYvvU4wzs1xrtu9ahxJkRNopTi2bEdyc4t4NlvL67iLqxXaoLSWucBdwM/AruBpVrrXUqpZ5RSoyynDQT2KKX2AoHAc5bjE4D+wC0lTCdfqJTaAewA/IFnbfWmqpM1+5K589PfCWtSn/n/6E69EmbcDWrXkB/u7c/jw9uz9cgZrn19DU+v2EX6hYvL/n+3/QQZ2XlMlMoRQthNqwBP7hzYihXbjrN6b81+NFGZlDVDRo4iMjJSx8bG2jsMm9mUcJqbP4yhhV89lkzvhU9dt1KvOX0um1d/3svimCN413HlX1eHMqlHM1ycze8a499dT9qFHH751wB5/iSEHWXl5jPsjTXkFWh+uq+/lEIqQim1RWsdWdp5UknCTrYeOcOtCzYT5FOHT2/vaVVyAvDzdOe5sZ349p4raNvIiyeW72L4m2tZtz+Fvacy2HL4DBO7y+QIIezNw9WZZ8d25EjqBd7+bb+9w6mWJEHZwc5j6dw8PwZ/L3cWTeuFv2fZZ9iHNanP4mm9eO/GrpzPyWPKvE3c9EEMrs6KcV1lcoQQjqBPK3/GdQ1izuoD7Dt16WfHomSSoKrY3lMZ3DQ/Bk93Fxbe3pPA+h7lbkspxZCOjfnlXwN44Nq2nM3KZWTnJviVI+EJISrHY8PaU8/dhUe/krVRZSUJqgodTDnPlHmbcHFSLJrWy2bbXHu4OjNzUGtiH7+KF8eF26RNIYRt+Hm68+jQ9mw+dIZlW46WfoH4kySoKnI09QKT524kv0Cz8PaetPCvZ/N71HVzqXY7lApRG1wfGUyPFr48vzK+3NVhaiP5NKsCJ9OzmDxvI+ez8/j0tp60CfSyd0hCiCqklOK5sR25kJPH89/J2ihrSYKqZMkZ2Uyet5Ez53P5+LaehDWpb++QhBB20CbQixn9W/Hl1mOs359i73CqBUlQlejM+RymfrCJE2lZfPiP7lK4VYha7u4rW9Pcry6Pf72TrFzZgbc0kqAqyYHkc0yet4mElPPMuzmS7i187R2SEMLOPFydeXZMRxJSzvNu9IHSL6jlJEHZWEGB5qP1hxj+5hpOpGcy96ZI+rb2t3dYQggHcUWbAEZ1bsK70Qc4kHzO3uE4NElQNnQiPZOb5sfw1Ipd9G7px0+z+jMgtOZWYBdClM/jI9rj4erEY1/tsGqHgtpKEpQNaK35eusxrnltNb8fOcPzYzsx/5buNKzAIlwhRM3V0MuDh4a2Y2NCKl/+fsze4TgsSVAVdOZ8DjMX/c6sz+IIDfTi+3uvYHJPqYUnhLi8Sd2b0bWZD8+t3M2Z8zn2DschSYKqgKj4JK55fTU//3GKB4e0ZemM3jT3s/0CXCFEzePkpHh+XCfOZubywveyNqokkqDK4Xx2Ho98uYN/LNiMb103vp7Zl7sGtsbZSXpNQgjrtWtUn9uvaMnS2EQ2JZy2dzgORxJUGcUeSmXoG2tYsvkIM/q3ZMU9fenQxNveYQkhqql7B7ehqW8dHvlqB9l5sjaqKElQVsrOy+e/P8QzYc4GCrTms+m9eWRYe9xdZBMyIUT51XFz5tkxnUhIPs87UbI2qqiL9xcXF4k/eZZZS+KIP5nBDZFNeWJkGJ4lbM0uhBDlMSA0gDERTXg3ej8jwxtLvU4L6UGV4nx2Hte/t4GUc9nMuymS/14XLslJCGFzT4wIo567Cw9/KftGFZIEVYr1B06TkZXHGxO7cFVYoL3DEULUUH6e7jw+PIwth8+wMOaIvcNxCJKgShG1J4l6bs5SS08IUenGdw2iX2t/Xvo+npPpWfYOx+4kQV2G1pro+CT6tfGXjQCFEJWucN+onPwCnlqx097h2J186l7G3lPnOJ6exaC2De0dihCilmjuV49ZV4Xy465T/LDzpL3DsStJUJcRtScJgIGSoIQQVej2K0Jo37g+T63YydmsXHuHYzdWJSil1BCl1B6l1H6l1MMlvN5cKfWrUmq7UipaKRVc5LWblVL7LF83FzneTSm1w9Lmm8oBi9dFxSfRvnF9GnlL0VchRNVxdXbixXGdSM7I5qUf4u0djt2UmqCUUs7AO8BQIAyYpJQKK3ba/4CPtdbhwDPAC5ZrfYGngJ5AD+AppVQDyzXvAtOBNpavIRV+NzaUnplL7OEzDGor22UIIape56Y+3NInhE83HiH2UKq9w7ELa3pQPYD9WusErXUOsAQYXeycMOBXy/dRRV6/FvhZa52qtT4D/AwMUUo1BuprrTdosxnKx8CYCr4Xm1q7L4X8As2V7WR4TwhhH/++JpQgnzo88mXtLINkTYIKAo4W+TnRcqyobcB4y/djAS+llN9lrg2yfH+5NgFQSk1XSsUqpWKTk5OtCNc2ovYk4V3HlYimPlV2TyGEKKqeuwvPjunIvqRzvBedYO9wqpw1CaqkZ0PFlznfDwxQSm0FBgDHgLzLXGtNm+ag1u9rrSO11pEBAVUz3FZQoInek0z/0ABcnGUeiRDCfga1a8jIzk14J2o/+5Nq1xbx1nz6JgJNi/wcDBwveoLW+rjWepzWugvwmOVY+mWuTbR8f8k27WnX8bOknMuW509CCIfw5Igw6rg582gtK4NkTYLaDLRRSoUopdyAicCKoicopfyVUoVtPQLMt3z/I3CNUqqBZXLENcCPWusTQIZSqpdl9t5NwHIbvB+biNqThFLQP1QSlBDC/gJMmHwKAAAgAElEQVS83HlsWHtiDqXyWezR0i+oIUpNUFrrPOBuTLLZDSzVWu9SSj2jlBplOW0gsEcptRcIBJ6zXJsK/B8myW0GnrEcA7gTmAfsBw4A39vqTVVU1J4kwoN98Pd0t3coQggBwPWRwfRq6cvzK3eTdLZ2lEFSZhJd9RAZGaljY2Mr9R6p53Po9uzP3Du4DbOuCq3UewkhRFkcTDnPta+v5qr2DZk9pZu9wyk3pdQWrXVkaefJDIBiVu9NRmukvJEQwuGE+Nfj3sFtWLnjJD//ccre4VQ6SVDFRO1Jwq+eG52CZBt3IYTjmXZFS9oGevHk8p1k1PAySJKgisgv0Kzam8yAtgE4OTlc5SUhhMDNxYkXx3fi5NksXvlpr73DqVSSoIqIO5pG2oVcGd4TQji0Ls0acHPvFny04RCLNh3heFqmvUOqFLJ3eRHRe5JwUtC/jUwvF0I4tvuvbcvqfck8+tUOAIJ86tC9RQO6h/jSvYUvrQM8q/1IkCSoIqL2JNGteQO867raOxQhhLgsT3cXfprVn/iTGcQcTCX2cCpr95/m6zhT88CnriuRzRvQvYUvkS186RTkXe02XpUEZZF0Noudx87ywLVt7R2KEEJYxcXZiY5B3nQM8ubWfiForTl8+gIxh1KJPZTK5kNn+GW32dfO3cWJiKY+9LD0sHq38sPVwUu5SYKyiN5rCtHK8ychRHWllKKFfz1a+NdjQqSpMpeckf1nstp8KJV3ovZToGF812BemdDZzhFfniQoi+g9SQTWd6d9Yy97hyKEEDYT4OXO0E6NGdqpMQDnsvN4YeVuFsccYeagVrQM8LRzhJfm2P27KpKbX8CavSkMatsQB9zYVwghbMbT3YVZV4Xi5uLEO1EH7B3OZUmCArYcPkNGdh4DZXhPCFELBHi5M6Vnc76OO8bh0+ftHc4lSYLCzN5zdVb0be1n71CEEKJKzOjfEmcnxWwH7kVJggKi45Pp3sIXLw+ZXi6EqB0a1vdgUvemfPF7IolnLtg7nBLV+gR1LC2TPacyZPaeEKLWmTGgFUrBu9GO2Yuq9Qkqeo9ZIzConVSPEELULk186nB9ZFOWxSZyIt3xyiXV+gQVFZ9McIM6tHLgqZZCCFFZ7hzQigKtmbMqwd6hXKRWJ6jsvHzW7Zfp5UKI2qupb13GdQ1iUcwRh9upt1YnqJiDqWTm5svwnhCiVps5qDX5BZo5qx2rF1WrE1RUfDJuLk70bulv71CEEMJumvvVY3REExZuOkzKuWx7h/OnWp2govck0bulH3XcnO0dihBC2NXMQa3Jzitg7hrH6UXV2gR1KOU8CSnnGdRWhveEEKJVgCcjw5vwyYbDpJ7PsXc4QC1OUIXTy6W8kRBCGHdf2ZrM3Hzmrz1o71CAWpygovYk09JSll4IIQSEBnoxtGMjFqw/RPqFXHuHY12CUkoNUUrtUUrtV0o9XMLrzZRSUUqprUqp7UqpYZbjU5RScUW+CpRSEZbXoi1tFr5WZV2ZzJx8NiSclt6TEEIUc/egNpzLzuPD9fbvRZWaoJRSzsA7wFAgDJiklAordtrjwFKtdRdgIjAbQGu9UGsdobWOAKYCh7TWcUWum1L4utY6yQbvxyobElLIySuQ6eVCCFFMWJP6XBMWyPy1BzmbZd9elDU9qB7Afq11gtY6B1gCjC52jgbqW773Bo6X0M4kYHF5A7Wl3+KTqOvmTI8QX3uHIoQQDueeK9twNiuPj9cfsmsc1iSoIOBokZ8TLceKehq4USmVCKwE7imhnRu4OEF9aBnee0JVUSkHrTVR8cn0be2Pu4tMLxdCiOI6BXtzZbuGzFt7kHPZeXaLw5oEVVLi0MV+ngQs0FoHA8OAT5RSf7atlOoJXNBa7yxyzRStdSfgCsvX1BJvrtR0pVSsUio2OTnZinAvb3/SOY6lZUr1ciGEuIx7rmxN2oVcPt142G4xWJOgEoGmRX4O5uIhvNuApQBa6w2AB1C0PMNEivWetNbHLH9mAIswQ4kX0Vq/r7WO1FpHBgRU/JlR1J/Ty+X5kxBCXEqXZg24oo0/c1cncCHHPr0oaxLUZqCNUipEKeWGSTYrip1zBBgMoJRqj0lQyZafnYDrMc+usBxzUUr5W753BUYAO6kCUfHJtGvkRROfOlVxOyGEqLbuHdyG0+dzWLTpiF3uX2qC0lrnAXcDPwK7MbP1dimlnlFKjbKc9m9gmlJqG6andIvWunAYsD+QqLUuWj/DHfhRKbUdiAOOAXNt8o4uIyMrl82HUmV6uRBCWCGyhS99WvkxZ3UCWbn5VX5/F2tO0lqvxEx+KHrsySLf/wH0vcS10UCvYsfOA93KGGuFrdufQl6BlvJGQghhpXuubMOkuRtZEnOEW/qGVOm9a1Uliaj4ZLw8XOjavIG9QxFCiGqhV0tferTw5b1VCWTnVW0vqtYkKK01UXuS6N8mAFfnWvO2hRCiQpRS/HNwG06ezWJZbGKV3rvWfFKfPJtFgdYye08IIcqob2s/ujTz4d3oA+TkFVTZfWtNgmrsXYeYR69iTJfia4yFEEJcTmEv6lhaJl9trbpeVK1JUABOTkqG94QQohwGhgYQHuzNpoOpVXZPq2bxCSGEqN2UUnx6e0/qe7hW2T2lOyGEEMIqVZmcQBKUEEIIByUJSgghhEOSBCWEEMIhSYISQgjhkCRBCSGEcEiSoIQQQjgkSVBCCCEckiQoIYQQDkkSlBBCCIckCUpUrfxc+GIafDnD3pEIIRycJChrnD0B+Xn2jqL6KyiAr++CHUth+xI4stHeEQkhHJgkqNIc3Qyvd4JPx0LOeXtHU31pDT88bJJT/wegXgBEv2jvqIQQDkwS1OWcT4FlN0MdHzi0FhbdIEmqvFa9BDFzoNdMGPQY9PknJETBkU2Vf++1r8PcwRD/nUmUQohqQRLUpRTkwxe3myQ15XMY+z4cXgefXgfZ5+wdnW1tWwI7Pq+89je9D9HPQ+fJcM2zoBR0vw3q+sOqSu5FnTkMUc/ByR2wZDLMuwoSVlXuPYUQNiEJ6lKiXzS/4Q97GZpEQPj1MH4eHN0En46H7Ax7R2gbh9bBV3fAF7fB1zMhN9O27W9fBt8/AG2Hw6i3wMnyV86tHvT9Jxz4rXJ7Ub/+B5Qz3L3Z3D/jBHw8Cj4eDYlbKu++QogKkwRVkr0/weqXIOJG6HrTX8c7jofrPoDEzfDJOMg6a78YbSHrLHx9BzRoAf3ug7hP4YOrITXBNu3v/cm03+IKuG4+OBfbH7P77VDXr/J6UYmxsPML6HM3NGhu/l/e8ztc+4LpUc27EpZMgaTdlXN/IUSFSIIq7sxh+HIaBHaC4f8zw1FFdRgL1y+A47/Dp+MgK90uYdrEj49CeiKMnQNXPQ2Tl0LaUZgz0DyvqYjDG2DpVAjsCBMXgavHxee41TPPog78BkdjKna/4rQ2769eQ+h771/HXT2g911w7zbzLOzgapjd20x7P3PItjEIISrEqgSllBqilNqjlNqvlHq4hNebKaWilFJblVLblVLDLMdbKKUylVJxlq/3ilzTTSm1w9Lmm0oVzwR2kJsFS28yH24TPgLXOiWfFzYKrv8IjsfBJ2MhM61q47SF+JWw9RPoOwua9TTHQq+FGavAt4V5XvPL0+WbXn9yh5lQ4t0UbvwCPOpf+tzCXpStZ/T9sdwMx175GLh7Xfy6uxcMeNAkqj73wB9fw1uR8N2/IeOkbWMRQpRLqQlKKeUMvAMMBcKASUqpsGKnPQ4s1Vp3ASYCs4u8dkBrHWH5uqPI8XeB6UAby9eQ8r8NG/nhYTgRB2PfBb9Wlz+3/Qi44RPzYfzJGMg8UzUx2sL5FPjmn6aXOPCRv7/WoAXc+hN0uwXWvmbe27kk69s+fcAMf7p7wtSvoJ7/5c9397T0on41U/ptIS8bfnkKGoZBl6mXP7euL1zzf/DPODMEuGUBvBEBPz8JF1JtE48Qolys6UH1APZrrRO01jnAEmB0sXM0UPhrsjdw/HINKqUaA/W11hu01hr4GBhTpshtbdsS2PKh6VG0G27dNW2Hwg0L4dQu89C9OnygaQ3f3GuGJsfNARe3i89x9YCRb8Do2eZ523tXmCG70pw9YRJaQZ5JTj5NrYvJ1s+iYuaa4bpr/g+cnK27pn5jGPGqmUwRNgrWvQlvdDZJuiDfNnEJIcrEmgQVBBwt8nOi5VhRTwM3KqUSgZXAPUVeC7EM/a1SSl1RpM3EUtoEQCk1XSkVq5SKTU5OtiLccji1C76ZZR7mX/lE2a4NvQYmLoakeDM7zNGT1LYlEP8tXPk4BHa4/LldpsDtv5ihzgXDYcM7l15HdCHVDHdeSDXDegFtrY/J3dMMs+3/xUxsqIgLqWaCS6vB0Pqqsl/v2xLGvQ93rocW/cww56fj4fzpisXliLSG9GP2jkKIS7ImQZX0bKj4p9QkYIHWOhgYBnyilHICTgDNLEN//wIWKaXqW9mmOaj1+1rrSK11ZEBAgBXhllFWOnw2FTy8YfwHF880s0abq2DSYkjZBx+NNENojijtKHz/IDTrA73vtu6aRp1gejSEDjGTDpbdfPHsxZzzsGgCpB4wEyKCupY9tu7ToI5vxZ9FrXrJLAG45tmKtRMYZv6fjnoLDq+HOf0rnjwdzU+Pw2sdrOsdC2EH1iSoRKDoWE0wFw/h3QYsBdBabwA8AH+tdbbW+rTl+BbgABBqaTO4lDYrn9awfKYZDrr+Q/AKLH9brQfDpCXmGcxHI+FcJfX2yqugAL6+E3QBjJlt/dAXmEoaExfCVf+B3d/A3EFw6g/zWl42fHYjHNsC130ILQeUL74/e1E/l3990ukDsHmuee4UWPwxaTl1vQlu+8n895o/xAwf1oRqFLu+hg1vAxp++7+a8Z5EjWNNgtoMtFFKhSil3DCTIFYUO+cIMBhAKdUek6CSlVIBlkkWKKVaYiZDJGitTwAZSqleltl7NwHLbfKOymLD2+YD9+r/QPM+FW+v1SCY/BmkHoSPRpRtckFli5kDh9bAtc+Db0jZr1cK+s2Cm1aYHtS8wWa48MvpZpr4qLfMxJGK6DEN6jQo/7Oon58EFw8zfdyWmkSY2Y2tB8PK+02FkepcTSRlHyy/G4K7m57m4XWQEG3vqGwvO8OsccvPtXckopyUtuI3J8u08dcBZ2C+1vo5pdQzQKzWeoVlVt9cwBMzVPeg1vonpdR44BkgD8gHntJaf2NpMxJYANQBvgfu0aUEExkZqWNjbTTMcng9LBgB7YbBhE8uXu9UEYfWwsIJ4B0EAx4qX9vN+pgH97aQvMcMUbUcaHp5FX2vGSdh2S1wxDI0dM2zpvdjC2tegV+fgdt/g+Bu1l93aB0sGAaDHocBD9gmluIKCmDtq6Z0kn+o+XsTEFo596osOedNXcLzSTBjtSk39VZXqN8EbvvZtv8OqkJBAaQfhdP7TOJN2Qcpe+H0flM1BExh4k7XQ+dJ0DjcvvEKAJRSW7TWkaWeZ02CchQ2S1AZp2DOFeDmaZ6vXG6dTnkdXg8Lr4eccv6m7eYFVz8N3W79qzxQeeTnmvpzaUfgro0VG8Ys3u6aV8x6ot4zbdMmmN96X+8EwT1gylLrrikoMFUhMk7BPVvAra7t4ilJQjR8fhvkZcHot83i7epAa/hqBmxfClO/hFZXmuOxH8K3s8xC7dBr7RvjpWSfsySh/ZYEZElGp/eb/w+FPLzNLw/+oeDXGjwbwt4fYM8PUJBrFo53ngThE8xrwi4kQV1Kfp6ZEn5sC0z7tfSZbBWReaZ8w3zZ5+C3Z8wHYbM+ZvjMv3X5Yoh6wQyZTfgYwoqvDnBQq/9nnotM+w2CrOhFbV9qqn+MeQ8iJlV+fGBmvy27BRJjoNddcPUz4OxasTZzM82HbZ0GNgnxIps/gO/+ZYZABzz41/H8XHirm/lwn7Ha8XpRR2Pg4zGQa9lJQDmZ9Xp+bcC/8CvU/FzPv+T4L6Sasldxi0wVGOVshmw7T4K2w0qudCIqjSSoS/n5KVj3uinv03mibQKrDFpD3EIzcy43CwY9Ar3vKdssw8QtprZep+vNmqfqIussvBFuXS8qN9NUgKjnB9OiK9bbLKu8HPPca9O70LSnKYFVv4n11+dcMAnu0FozRHks1kxiGfmmmeJvS8e2mEkeIQNMT6n4f6e4RWYSzYRPzDowR5Gbadbh5WXDtZahVd8QcHEvf5vJe8z73b4UMo6bxNxhHERMNs/lHC1B10CSoEoS/50p4RN5K4x4zXaBVaaMk6b8Tvy30LgzjHrbunH0nAvmuVPuBbOmp45P5cdqS6tfht+ehWlRl5+2XvjM6uZvIeSKS59XmXZ+CSvuMevFxn9w6ZmM2edM+aXD60xSOva7GXZSTub/bfO+pjLJwVXQ/0EY9KhtPiwvpJq/Cygz2aOu78Xn5OfB7F7g5AJ3rivbLM/K9NMTsP5NmPq1mYRkSwX55r913GIzWSov0wwLdp4I4ROtX2guykwSVHFnDpvfxPxawq0/Vuw3MHv4Yzl8dz9cOG1m0/V/8PLDEt8/BJveg5uWm8kR1U3WWfMsqlkvMzOyJOeS4c0uJjFNWly18RWXvMespzu9zyyC7nufef54dJOlh7TWlNEqyDPDS026QIu+ZnF4055/PQfNz4Vv7zN1EjtNMM+4KvJ3taAAFl1viuLe+uPlk/2Oz822K+M/gE7Xlf+etpIYa0YAut5kKptUpqyz5t/YtsXmFwgU3PBpxWemihJJgiouPxeiX4CuN5utF6qjC6lmcWXcQjPePvpt8wFe3IEoU3Ko5x0w9L9VH6etrHoZop41E1madLn49W/vg98/NpM//NtUdXQXyz5nykjt/Bx8mplK8boAnFxNYmje11SnaNrTrPu6FK1Nz/C3/zPX3PBpyb0ea0T/12wWOeI1M3JwOQUF8F5fM5w2M6Z8i9ZtJS/b9Pqyz8FdGypnItOlnDlkZuEW9iZlyM/mJEHVZPt/NaWZ0o+atUODn/yrYndmGrzbx2xlMWP1pSuyVwdZ6fB6ODTrDZOX/P21pHh4t7ep4zfsZfvEVxKtIfYDUy0+qKtJSME9yjezcMfn5rmQT3PzLM63Zdmu3/+L2QG680QY8651H7S7v4XPppg6jLZ+DlYWvz5jkvSNX5SvZFVFFT6Tm/I5tLm66u9fw1mboGQ/qOqo9WDzW2XPGaayweze5sMITCmjjJMw9r3qnZzAPLzuPRP2fm+2Ninq5yfMVPwBF+3+Yl9KmaQ59Usz1NdyYPmnvXe6zgzRXkgxSwXKsmdW2lH4Ypqp6D78Vet7Ae2GQ+MIM/MzL6d8cVfU8a2w9nWzYag9khNAx+ugfhCsq+ShRXFZkqCqK3dPM3x3648mEX06Hj4aBds/g/4PWDc9uzroOcMkqlVFhioPRMG+n6D/v83svZqseR+47Rdwr29KaO36uvRr8rJNzcT8XLO8oCwJUimTWNOOmB2Wq1peDnw906xRuva5qr9/IRc3s3zg0Jryl94SFSYJqrpr1hNmrDFJ6fA686ym//32jsp2PLyh10zYs9L0ogryzXM4n2bQY4a9o6sa/q1NVfnGnU3iWffG5Wvn/fiYmVY+Znb51s+1vsoMS6562SxxqEpr/gdJu8ykCHvPPO12s/n7t+51+8ZRi0mCqglcPcxvvf/caoaEKrpg1NH82Yt6yTwbOLXTbFFfmxZX1vM3dRA7jDVrr769r+TdjrcvMwVze99d/vVMhb2ojONmj7SqcmK7ee4UPtExKlq4e5nh2t3fmAoWospJgqpJfJqZD/Kapo6PpRf1nek9BXc3CytrG1cPGD8f+t1nEsfiG0xpqEJJu81Oyc36mAReES0HmCnwa1419fsqW34uLL/LbFw55IXKv5+1et4Bzm6w4S17R1IrSYIS1UPPGeDuDVlpcM1ztXfqr5OTST4j3zDP4uYPNWWXsjPMOiw3T7N1jC160Vc+borKxsyteFulWfuaWaQ8/NXyT6mvDJ4NTYWJuMWm1qOoUpKgRPVQxweG/89UK2/W097R2F+3W8zU8zOHzNYnS2+C1ATLvmaNbHOPZr3M86h1b1y8SaUtndplhm87XueYC2P73AP5OWbhu6hSkqBE9RE+ofK20qiOWl8Ft/5gSiUd+M2sh2vRz7b3GPQYZKZW3odzfh58fZf5BWToS5Vzj4rya2We523+oHITtbiIJCghqrNGHU3V9/EfQN97bd9+UFdoOxzWv22q89va+jdMCahh/3PsJQN974XsdPj9I3tHUqtIghKiuvNqZBb1VtZzuUGPmg/n9W/btt2keIh+0WwD02GMbdu2taBuZtLIhtn2W8BcC0mCEkJcXqOOZtbkxnfhfIpt2izIh+UzzaSOYa/Yps3K1neWmXq/Y5m9I6k1JEEJIUo38BGzHYWtFq1ueMfsfzXsZfAMsE2bla31YLMj7/o3TWFdUekkQQkhShcQCuE3mCnnGScr1lbKPoh6DtqNgI7jbRNfVVDKPItKjod9P9o7mlpBEpQQwjoDHjQLatdUYEiucGjPxaNsRWwdRYex4N1UishWEUlQQgjr+LaELjfClgWmWnp5bJpjNnEc+l/wCrRpeFXC2dWUkTqyAY5ssnc0NZ4ddyQTQlQ7/R8wu86uftlUs8g5b/btKvqVfdbyfVqx186aD/Y215rhwuqq61SzHcm6N6DZIntHU6NJghJCWM+nqaliEfM+bP0UdP7lz3epY3bD9fA2X6FDTO+pug3tFeVWD3pMN1vAJO+BgLYVa6+gAHZ9aaaxV8deZSWSBCWEKJuBj4CLuymiWph4Cr/ci/5c35xXE/WYDuveNDP6Rr9T/nYupMKX08yGo+ETYdwc28VYA1iVoJRSQ4A3AGdgntb6xWKvNwM+Anws5zystV6plLoaeBFwA3KAB7TWv1muiQYaA5mWZq7RWidV+B0JISpXXV+45ll7R2Ff9fz/eh436HGo37jsbRzbAktvhnOnILCT2dYj51XTQxOAFZMklFLOwDvAUCAMmKSUCit22uPAUq11F2AiMNtyPAUYqbXuBNwMfFLsuila6wjLlyQnIUT10XumGeLcOLv0c4vS2tT1mz/E/HzrD2bYM/c87P7W9nFWY9bM4usB7NdaJ2itc4AlwOhi52igvuV7b+A4gNZ6q9b6uOX4LsBDKVVD+/xCiFrFN8RMO4/90EwCsUbOBfjqDvjuXxDSH2asNmWUmvU2+7ltX1K5MVcz1iSoIKDonNJEy7GingZuVEolAiuBe0poZzywVWudXeTYh0qpOKXUE0qV/NRUKTVdKRWrlIpNTk62IlwhhKgiff4JORkQO7/0c1P2m61Rtn8GAx+Fycv+2vvKycnMbEyIrvhC6BrEmgRVUuLQxX6eBCzQWgcDw4BPlFJ/tq2U6gD8F5hR5JoplqG/KyxfU0u6udb6fa11pNY6MiCgmpREEULUDk0ioOVAU6cwL/vS5/2xAt4faJLPjZ/DwIdMUioqfCLoAqn1V4Q1CSoRaFrk52AsQ3hF3AYsBdBabwA8AH8ApVQw8BVwk9b6QOEFWutjlj8zgEWYoUQhhKhe+s4yEx22lTA8l58LPz4GS6eaclEzVpt9vEri39oM9237rHLjrUasSVCbgTZKqRCllBtmEsSKYuccAQYDKKXaYxJUslLKB/gOeERrva7wZKWUi1KqMIG5AiOAnRV9M0IIUeVaDoRG4RcXkc04CR+Ngg1vQ/fb4R/fm3VklxM+EU7tgJPycQhWJCitdR5wN/AjsBszW2+XUuoZpdQoy2n/BqYppbYBi4FbtNbacl1r4AnLs6Y4pVRDwB34USm1HYgDjgFzbf3mhBCi0ikF/WbB6f2w5ztz7NBaeO8KsxnjuHkw/BXr1oR1HA9OLjJZwkKZPFI9REZG6tjYWHuHIYQQf5efB291hXoBZnv4X/5jZvnd8Ck0bF+2thZPguNb4b5d4ORcOfEW5INysltFD6XUFq11ZGnnSbFYIYSoKGcX6HOP2ePq5yeh/QiYFlX25ARmNl/GCTi4yvZxgtkReHYv+PU/ldO+DUmCEkIIW4iYAm2ugSEvwvUfmVJP5RE6xJSMqqzJEnELIWUvxC1y+I0XJUEJIYQtuNWFKcug150VGzpz9YAOY0zpo+xztosPTO9pzSvg5mlmHh517C1DJEEJIYSj6TzRlD6Kt3Hpo22LIP0ojH4bnN3hj+W2bd/GJEEJIYSjadrLlD4qaW1VeeXlwOpXICgSwsZA68Gwe4VDD/NJghJCCEfj5GTWRB1cBWdP2KbNbYsg/QgMfNgMQYaNhrPH4Pjvtmm/EkiCEkIIR9TZhqWPCp89BXX7q5JF6BBwcoU/vq54+5VEEpQQQjgiv1ZmOG67DWbzbVsMaUdgwMN/TeCo4wOtBpnnUA66HlYSlBBCOKrOE+HUTji5o/xt5OfCmv9Bk67Q5uq/v9Z+lElcJ7ZVLM5KIglKCCEcVYdxpvRRRSZLFPaeBj5y8fT3dsNBOTvsbD5JUEII4ajq+ZnFvzs+N+WJyio/F1a/DE26XNx7ArMfVUh/8xzKAYf5JEEJIYQj6zwRzp00mxmW1bYll+49FQobDakJcGpXhcKsDJKghBDCkYUOAQ/vsk+W+Fvv6ZpLn9duhCkcu7v4Lkr2JwlKCCEcmYs7dBhb9tJH2z+DtMN/n7lXEs8AaN7XIZ9DSYISQghHFz4Rci9YX/qosPfUOAJCry39/LDRkBwPSfEVi9PGJEEJIYSja9YLfJqbGXnW2L4Uzhz6q2pEadqNAJTDDfNJghJCCEenlNknKmEVnD1++XPz8yy9p87m+ZU16jeGpj3hD0lQQgghyqrzRECXXvpo+2dw5mDpz56KCxsNp3bA6QMVCtOWJEEJIUR14NcKgrtffiPDwt5To3BoO7Rs7bcfaf50oMkSkqCEEKK6CL8BknZduvTRjqWm93S5dU+X4tPU1P6TBCWEEKLMOo43FchLKulujVoAAAdFSURBVH1Ukd5TobBRcCLOTLBwAJKghBCiuqjrayl9tMwkpKJ2LDMVIayduVeS9qPMn7u/qVicNiIJSgghqpPON8C5U3Aw+q9j+Xmw+iVo1AnaDit/274hZvafgwzzWZWglFJDlFJ7lFL7lVIPl/B6M6VUlFJqq1Jqu1JqWJHXHrFct0cpda21bQohhChBYemjopMldn5uek9lnblXkrDRkLgZ0hMr1o4NlJqglFLOwDvAUCAMmKSUCit22uPAUq11F2AiMNtybZjl5w7AEGC2UsrZyjaFEEIU5+JutuGI/9aUPsrPg1WW3lO74RVvv/1o86cDDPNZ04PqAezXWidorXOAJcDoYudooL7le2+gcCXZaGCJ1jpba30Q2G9pz5o2hRBClKSzpfTR7m9g5xeQegAGPFTx3hOAf2to2MEhFu1ak6CCgKNFfk60HCvqaeBGpVQisBK4p5RrrWkTAKXUdKVUrFIqNjk52YpwhRCihmvaExq0gLiF5tlTYCdoa4PeU6Gw0XBkA2SctF2b5WBNgiopJRff2WoSsEBrHQwMAz5RSjld5lpr2jQHtX5fax2ptY4MCAiwIlwhhKjhCksfHVoDp/fDgAfByYZz3sJGA9ruw3zWvKNEoGmRn4P5awiv0G3AUgCt9QbAA/C/zLXWtCmEEOJSwm8wfwZ2tBR7taGG7cA/1O6z+axJUJuBNkqpEKWUG2bSQ/HBySPAYAClVHtMgkq2nDdRKeWulAoB2gAxVrYphBDiUvxawZD/wqg3bdt7KhQ2Gg6vg/Mptm/bSqW+K611HnA38COwGzNbb5dS6hmllGVVF/8GpimltgGLgVu0sQvTs/oD+AGYqbXOv1Sbtn5zQghRo/W6A4K6VU7bYaNBF1i/B1UlUFqX+OjHIUVGRurY2Fh7hyGEEDWf1vBWVzMZY+pXNm1aKbVFax1Z2nlSSUIIIcTFlDKljxJWwYVUu4QgCUoIIUTJwkaDzoc9K+1ye0lQ4v/bu5fQOKswjOP/p4kujEXrpdLGpF5wYXVhJeiiIt0oXpDUhWJXdaULBd0pbuxGENHiTlAsVPCC4K0raYuCrqQXiq2GmijRxoZUKWiL2ErzupgvEsI300ws3zmT8/w2M/OFyXl5eWeemfPNMGZm9dZugMuGk31p1wFlZmb1pNZPcPz4Bfz9R+PLO6DMzKy99aMw+w8c/bzxpR1QZmbW3uAIrFyb5Eu7DigzM2tvxYrWNt/EXjhzqtmlG13NzMx6z/pROHcGxnc3uqwDyszMOhu6EwZWN77N54AyM7POVvTBzQ/B+B44+1dzyza2kpmZ9a71o60fSZzY29iSDigzMzu/dRvhkisb3ebrb2wlMzPrXX398OB2uHy4sSUdUGZmtji3bG50OW/xmZlZlhxQZmaWJQeUmZllyQFlZmZZckCZmVmWHFBmZpYlB5SZmWXJAWVmZllyQJmZWZYUEalrWDRJvwE//89/cxXw+wUoZ7lxX+q5L/Xcl/bcm3rz+7IuIq4+3x16KqAuBEn7I2IkdR25cV/quS/13Jf23Jt6S+mLt/jMzCxLDigzM8tSiQH1ZuoCMuW+1HNf6rkv7bk39bruS3HnoMzMrDeU+A7KzMx6gAPKzMyyVExASbpP0lFJE5KeT11PTiRNSjos6ZCk/anrSUXSDkknJB2Zd+wKSXskjVeXq1LWmEKbvmyT9Gs1M4ckPZCyxhQkDUn6UtKYpO8kPVMdL3pmOvSl65kp4hyUpD7gB+AeYArYB2yJiO+TFpYJSZPASEQU/eVCSXcDp4F3IuLW6tgrwMmIeLl6YbMqIp5LWWfT2vRlG3A6Il5NWVtKktYAayLioKSVwAFgM/A4Bc9Mh748SpczU8o7qDuAiYj4KSLOAh8Ao4lrssxExFfAyQWHR4Gd1fWdtB5oRWnTl+JFxHREHKyunwLGgEEKn5kOfelaKQE1CBybd3uKJTZsmQpgt6QDkp5IXUxmromIaWg98IDVievJydOSvq22AIvaxlpI0nXABuAbPDP/WdAX6HJmSgko1Rxb/nubi7cxIm4H7geeqrZ0zDp5A7gRuA2YBl5LW046ki4FPgKejYg/U9eTi5q+dD0zpQTUFDA07/a1wPFEtWQnIo5XlyeAT2htiVrLTLWnPre3fiJxPVmIiJmIOBcRs8BbFDozki6i9ST8bkR8XB0ufmbq+rKUmSkloPYBN0m6XtLFwGPArsQ1ZUHSQHUiE0kDwL3Akc73KsouYGt1fSvwWcJasjH3BFx5mAJnRpKAt4GxiNg+709Fz0y7vixlZor4FB9A9ZHG14E+YEdEvJS4pCxIuoHWuyaAfuC9Unsj6X1gE62fBZgBXgQ+BT4EhoFfgEcioqgPDLTpyyZaWzUBTAJPzp13KYWku4CvgcPAbHX4BVrnW4qdmQ592UKXM1NMQJmZWW8pZYvPzMx6jAPKzMyy5IAyM7MsOaDMzCxLDigzM8uSA8rMzLLkgDIzsyz9C142avLit/kGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "X=np.array(data)\n",
    "y=np.array(label)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.8, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "mlp = MLP([128,512,128,10],activation=[None,'ReLU','ReLU','softmax'], dropout=[0, 0.2, 0.2, 0])\n",
    "losses, accuracies_train, accuracies_val = mlp.optimize(X_train, y_train, learning_rate=0.001, batch_size=1, momentum=0.9, weight_decay=0.0, epochs=25)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_val, label='val')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_relu.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.6924 - acc: 0.7761 - categorical_accuracy: 0.7761\n",
      "Epoch 2/10\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3802 - acc: 0.8643 - categorical_accuracy: 0.8643\n",
      "Epoch 3/10\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3382 - acc: 0.8790 - categorical_accuracy: 0.8790\n",
      "Epoch 4/10\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3134 - acc: 0.8856 - categorical_accuracy: 0.8856\n",
      "Epoch 5/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2941 - acc: 0.8928 - categorical_accuracy: 0.8928\n",
      "Epoch 6/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2786 - acc: 0.8996 - categorical_accuracy: 0.8996\n",
      "Epoch 7/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2652 - acc: 0.9025 - categorical_accuracy: 0.9025\n",
      "Epoch 8/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2534 - acc: 0.9081 - categorical_accuracy: 0.9081\n",
      "Epoch 9/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2440 - acc: 0.9110 - categorical_accuracy: 0.9110\n",
      "Epoch 10/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2352 - acc: 0.9141 - categorical_accuracy: 0.9141\n",
      "Keras model accuracy : 0.8776666666666667\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras import optimizers, metrics, Sequential\n",
    "\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tensorflow.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \n",
    "sess = tensorflow.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "y = label\n",
    "X = data\n",
    "\n",
    "y_dummies = np.array(pd.get_dummies(y))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=0.25, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "with tf.device('GPU'):\n",
    "    sgd = optimizers.sgd(lr=0.01)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=10)\n",
    "\n",
    "#Predict and calculate accuracy\n",
    "yhat_val = model.predict(X_val)\n",
    "\n",
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "\n",
    "print('Keras model accuracy : {}'.format(accuracy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.8818888888888889 \n",
      "Validation Accuracy: 0.8636666666666667 \n",
      "Loss: 0.5236357078156707 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.8909777777777778 \n",
      "Validation Accuracy: 0.8648 \n",
      "Loss: 0.4007253480979453 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.9002 \n",
      "Validation Accuracy: 0.8712 \n",
      "Loss: 0.37435891861420717 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.8969555555555555 \n",
      "Validation Accuracy: 0.8649333333333333 \n",
      "Loss: 0.35990698710379204 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.9106 \n",
      "Validation Accuracy: 0.8716666666666667 \n",
      "Loss: 0.35167849340289126 \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.9182444444444444 \n",
      "Validation Accuracy: 0.8780666666666667 \n",
      "Loss: 0.35096611376963815 \n",
      "\n",
      "Epoch: 6..\n",
      "train Accuracy: 0.9166222222222222 \n",
      "Validation Accuracy: 0.8704 \n",
      "Loss: 0.3399193994632599 \n",
      "\n",
      "Epoch: 7..\n",
      "train Accuracy: 0.9219777777777778 \n",
      "Validation Accuracy: 0.8772 \n",
      "Loss: 0.3484103883231853 \n",
      "\n",
      "Epoch: 8..\n",
      "train Accuracy: 0.9181333333333334 \n",
      "Validation Accuracy: 0.8688666666666667 \n",
      "Loss: 0.3508696647882877 \n",
      "\n",
      "Epoch: 9..\n",
      "train Accuracy: 0.9232222222222223 \n",
      "Validation Accuracy: 0.8726 \n",
      "Loss: 0.3522889730794041 \n",
      "\n",
      "Epoch: 10..\n",
      "train Accuracy: 0.9280222222222222 \n",
      "Validation Accuracy: 0.8781333333333333 \n",
      "Loss: 0.36248040141794 \n",
      "\n",
      "Epoch: 11..\n",
      "train Accuracy: 0.9093777777777777 \n",
      "Validation Accuracy: 0.8602666666666666 \n",
      "Loss: 0.3791673213050742 \n",
      "\n",
      "Epoch: 12..\n",
      "train Accuracy: 0.9263555555555556 \n",
      "Validation Accuracy: 0.8750666666666667 \n",
      "Loss: 0.3765147013873777 \n",
      "\n",
      "Epoch: 13..\n",
      "train Accuracy: 0.9214888888888889 \n",
      "Validation Accuracy: 0.8693333333333333 \n",
      "Loss: 0.39080348520587643 \n",
      "\n",
      "Epoch: 14..\n",
      "train Accuracy: 0.9174888888888889 \n",
      "Validation Accuracy: 0.8689333333333333 \n",
      "Loss: 0.4094709309950197 \n",
      "\n",
      "Epoch: 15..\n",
      "train Accuracy: 0.9119777777777778 \n",
      "Validation Accuracy: 0.8560666666666666 \n",
      "Loss: 0.4177666310297836 \n",
      "\n",
      "Epoch: 16..\n",
      "train Accuracy: 0.9241777777777778 \n",
      "Validation Accuracy: 0.8727333333333334 \n",
      "Loss: 0.4304940879229488 \n",
      "\n",
      "Epoch: 17..\n",
      "train Accuracy: 0.9219333333333334 \n",
      "Validation Accuracy: 0.8714 \n",
      "Loss: 0.4264126737728994 \n",
      "\n",
      "Epoch: 18..\n",
      "train Accuracy: 0.9140888888888888 \n",
      "Validation Accuracy: 0.8692666666666666 \n",
      "Loss: 0.4615705864785923 \n",
      "\n",
      "Epoch: 19..\n",
      "train Accuracy: 0.9200888888888888 \n",
      "Validation Accuracy: 0.8716666666666667 \n",
      "Loss: 0.46095082146103733 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:188: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:188: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,512,128,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, batch_size=1,learning_rate=0.01,epochs=50)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'yhat_train' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-21e7dcd9f6ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-129-77e6a3e7d998>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, batch_size, epochs, verbose)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;31m# Calculate train and Test Accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m         \u001b[0maccuracy_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m         \u001b[0maccuracy_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'yhat_train' referenced before assignment"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.1,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(32,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (32,) and (1,1) not aligned: 32 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-9e969c7d6dab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, epochs, verbose)\u001b[0m\n\u001b[0;32m    314\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion_MSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[1;31m# update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta, output_layer)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_vector\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (32,) and (1,1) not aligned: 32 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,512,128,32,10],activation=[None, 'logistic', 'logistic', 'logistic', 'softmax'], dropout=[0.0, 0.0, 0.0, 0.0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.02,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tensorflow.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \n",
    "sess = tensorflow.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_24 to have shape (10,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-8f69afa4341a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical_accuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#Predict and calculate accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_24 to have shape (10,) but got array with shape (1,)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\dnuho\\anaconda3\\envs\\data\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m(138)\u001b[0;36mstandardize_input_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m    136 \u001b[1;33m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    137 \u001b[1;33m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m--> 138 \u001b[1;33m                            str(data_shape))\n",
      "\u001b[0m\u001b[1;32m    139 \u001b[1;33m    \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    140 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras import optimizers, metrics, Sequential\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "y = label\n",
    "X = data\n",
    "\n",
    "y_dummies = np.array(pd.get_dummies(y))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=0.25, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "with tf.device('GPU'):\n",
    "    sgd = optimizers.sgd(lr=0.1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=20)\n",
    "\n",
    "#Predict and calculate accuracy\n",
    "yhat_val = model.predict(X_val)\n",
    "\n",
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "\n",
    "print('Keras model accuracy : {}'.format(accuracy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosscheck with Keras Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-4c2f6c2ef006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0msgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-117-4c2f6c2ef006>\u001b[0m(1)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m----> 1 \u001b[1;33m\u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      2 \u001b[1;33m    \u001b[0msgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      3 \u001b[1;33m    \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      4 \u001b[1;33m    \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      5 \u001b[1;33m    \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with tf.device('GPU'):\n",
    "    sgd = optimizers.sgd()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict and calculate accuracy\n",
    "yhat_val = model.predict(X_val)\n",
    "\n",
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "\n",
    "print('Keras model accuracy : {}'.format(accuracy_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import logsumexp\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  (a * (1 - a ))\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        #return np.exp(x)/(np.sum(np.exp(x),axis=1)[:,None])\n",
    "        return np.exp(x)/np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    \n",
    "    def __softmax_deriv(self, a):\n",
    "        #a = softmax(x)\n",
    "        return (a * (1 - a))\n",
    "    \n",
    "    def __ReLU(self,x):\n",
    "        return np.vectorize(lambda x:x if x>0 else 0)(x)\n",
    "    \n",
    "    def __ReLU_deriv(self,a):\n",
    "        #a = ReLU()\n",
    "        return np.vectorize(lambda x:1 if x>0 else 0)(a)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__ReLU\n",
    "            self.f_deriv = self.__ReLU_deriv\n",
    "            \n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', dropout=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout=dropout\n",
    "        self.dropout_vector = None\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, mode):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        if (mode=='train' and self.dropout>0):\n",
    "            self.dropout_vector = np.random.binomial(1, 1-self.dropout, size=input.shape)/(1-self.dropout)\n",
    "            lin_output = np.dot(self.dropout_vector*input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):\n",
    "        self.grad_W = (np.atleast_2d(self.dropout_vector*self.input if self.dropout>0 else self.input).T.dot(np.atleast_2d(delta)))\n",
    "        self.grad_b = delta\n",
    "        \n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], batch_size=False, dropout=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.mode = 'train'\n",
    "        self.activation=activation\n",
    "        self.dropout=dropout\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],self.dropout[i]))\n",
    "            \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input=input, mode=self.mode)\n",
    "            input=output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=error**2\n",
    "        # calculate the delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)    \n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    def criterion_CELoss(self,y,y_hat):\n",
    "        error = y * np.log(y_hat)\n",
    "        loss = -np.sum(error)\n",
    "        delta = y_hat-y\n",
    "        return loss,delta\n",
    "        \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            print(layer.W.shape)\n",
    "            print(layer.grad_W.shape)\n",
    "            print(layer.b.shape)\n",
    "            print(layer.grad_b.shape)\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "            \n",
    "    def get_batches(self,X, y, batch_size):\n",
    "        batches = []\n",
    "\n",
    "        X, y = shuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_batch = X[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "            \n",
    "            batches.append((X_batch, y_batch))\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=10, batch_size=False):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        \n",
    "        # Differentiate Stochastic Gradient Descent vs Batch Gradient Descent\n",
    "        if batch_size:\n",
    "            batches = self.get_batches(X, y_dummies, batch_size)\n",
    "            for k in range(epochs):\n",
    "                loss = np.zeros(X.shape[0])\n",
    "                for X,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        loss,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                        \n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                to_return[k] = np.mean(loss)\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss=np.zeros(X.shape[0])\n",
    "                for it in range(X.shape[0]):\n",
    "                    i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X[i])\n",
    "                    print(yhat.shape)\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                to_return[k] = np.mean(loss)\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.test()\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = self.forward(x[i,:])\n",
    "        return output\n",
    "    \n",
    "    def optimize(self, X, y, learning_rate=0.01, test_size=0.25, epochs=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        losses = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            loss=np.zeros(X_train.shape[0])         \n",
    "            \n",
    "            self.test()\n",
    "            yhat_train = self.forward(X_train)\n",
    "            yhat_val = self.forward(X_val)\n",
    "            \n",
    "            # Calculate train and Test Accuracy\n",
    "            accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "            accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "            \n",
    "            self.train()\n",
    "            for it in range(X_train.shape[0]):\n",
    "                i=np.random.randint(X_train.shape[0])\n",
    "                \n",
    "                \n",
    "                # forward pass\n",
    "                y_hat = self.forward(X_train[i])\n",
    "\n",
    "                # backward pass\n",
    "                if self.activation[-1] == 'softmax':\n",
    "                    loss[it],delta = self.criterion_CELoss(y_train[i],y_hat)\n",
    "                else:\n",
    "                    loss[it],delta = self.criterion_MSE(y_train[i],y_hat)\n",
    "                \n",
    "                self.backward(delta)\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "                \n",
    "            self.test()\n",
    "            yhat_train = self.forward(X_train)\n",
    "            yhat_val = self.forward(X_val)\n",
    "            accuracies_val.append(accuracy_train)\n",
    "            accuracies_test.append(accuracy_val)\n",
    "            \n",
    "            if verbose:\n",
    "                print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                      format(e, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "            \n",
    "            losses[e] = np.mean(loss)\n",
    "        return losses, accuracies_val, accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32)\n",
      "(128, 32)\n",
      "(32,)\n",
      "(7, 32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (32,) doesn't match the broadcast shape (7,32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9f0594af6ef9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-a87ebeebcedb>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, learning_rate, epochs, batch_size)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m                     \u001b[1;31m# update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m                 \u001b[0mto_return\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-a87ebeebcedb>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, lr)\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_b\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_W\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_b\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (32,) doesn't match the broadcast shape (7,32)"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,32,10],activation=[None, 'ReLU', 'softmax'], dropout=[0, 0, 0])\n",
    "\n",
    "mlp.fit(data, label, learning_rate=1, batch_size=7, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:175: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\numpy\\lib\\function_base.py:2048: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.0848 \n",
      "Validation Accuracy: 0.08486666666666667 \n",
      "Loss: nan \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,32,10],activation=[None, 'ReLU', 'softmax'], dropout=[0, 0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.1,epochs=10)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.02,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.10084444444444444 \n",
      "Validation Accuracy: 0.09806666666666666 \n",
      "Loss: 0.5865544172118954 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.865 \n",
      "Validation Accuracy: 0.8462666666666666 \n",
      "Loss: 0.3582903284450463 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.8868 \n",
      "Validation Accuracy: 0.8608666666666667 \n",
      "Loss: 0.3042962039784499 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.9027777777777778 \n",
      "Validation Accuracy: 0.8637333333333334 \n",
      "Loss: 0.26360169428583136 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.9176 \n",
      "Validation Accuracy: 0.8737333333333334 \n",
      "Loss: 0.2258711638165271 \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.9186 \n",
      "Validation Accuracy: 0.8690666666666667 \n",
      "Loss: 0.2047567057860974 \n",
      "\n",
      "Epoch: 6..\n",
      "train Accuracy: 0.9353777777777778 \n",
      "Validation Accuracy: 0.8691333333333333 \n",
      "Loss: 0.17605418232840228 \n",
      "\n",
      "Epoch: 7..\n",
      "train Accuracy: 0.9422888888888888 \n",
      "Validation Accuracy: 0.873 \n",
      "Loss: 0.1561336831385689 \n",
      "\n",
      "Epoch: 8..\n",
      "train Accuracy: 0.9543555555555555 \n",
      "Validation Accuracy: 0.8768666666666667 \n",
      "Loss: 0.13689134744454487 \n",
      "\n",
      "Epoch: 9..\n",
      "train Accuracy: 0.9550666666666666 \n",
      "Validation Accuracy: 0.8742666666666666 \n",
      "Loss: 0.12079886400544028 \n",
      "\n",
      "Epoch: 10..\n",
      "train Accuracy: 0.9586444444444444 \n",
      "Validation Accuracy: 0.8714666666666666 \n",
      "Loss: 0.10909576754833403 \n",
      "\n",
      "Epoch: 11..\n",
      "train Accuracy: 0.9439777777777778 \n",
      "Validation Accuracy: 0.8623333333333333 \n",
      "Loss: 0.09118467351906503 \n",
      "\n",
      "Epoch: 12..\n",
      "train Accuracy: 0.9766666666666667 \n",
      "Validation Accuracy: 0.8788666666666667 \n",
      "Loss: 0.07825709485687772 \n",
      "\n",
      "Epoch: 13..\n",
      "train Accuracy: 0.9695333333333334 \n",
      "Validation Accuracy: 0.8717333333333334 \n",
      "Loss: 0.06668788620856476 \n",
      "\n",
      "Epoch: 14..\n",
      "train Accuracy: 0.9786 \n",
      "Validation Accuracy: 0.8713333333333333 \n",
      "Loss: 0.061749810409242205 \n",
      "\n",
      "Epoch: 15..\n",
      "train Accuracy: 0.9784 \n",
      "Validation Accuracy: 0.8766666666666667 \n",
      "Loss: 0.051989393973013505 \n",
      "\n",
      "Epoch: 16..\n",
      "train Accuracy: 0.9856 \n",
      "Validation Accuracy: 0.8752666666666666 \n",
      "Loss: 0.0500660362892502 \n",
      "\n",
      "Epoch: 17..\n",
      "train Accuracy: 0.9854222222222222 \n",
      "Validation Accuracy: 0.8748 \n",
      "Loss: 0.04103112365433603 \n",
      "\n",
      "Epoch: 18..\n",
      "train Accuracy: 0.9888888888888889 \n",
      "Validation Accuracy: 0.8768666666666667 \n",
      "Loss: 0.028987736320235373 \n",
      "\n",
      "Epoch: 19..\n",
      "train Accuracy: 0.9940888888888889 \n",
      "Validation Accuracy: 0.8765333333333334 \n",
      "Loss: 0.024308904610166347 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,512,128,32,10],activation=[None, 'logistic', 'logistic', 'logistic', 'softmax'], dropout=[0.0, 0.0, 0.0, 0.0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.02,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras import optimizers, metrics, Sequential\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "y = label\n",
    "X = data\n",
    "\n",
    "y_dummies = np.array(pd.get_dummies(y))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=0.25, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tensorflow.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \n",
    "sess = tensorflow.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45000/45000 [==============================] - 2s 42us/step - loss: 0.9828 - acc: 0.6622 - categorical_accuracy: 0.6622\n",
      "Epoch 2/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.5128 - acc: 0.8221 - categorical_accuracy: 0.8221\n",
      "Epoch 3/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.4623 - acc: 0.8380 - categorical_accuracy: 0.8380\n",
      "Epoch 4/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.4270 - acc: 0.8482 - categorical_accuracy: 0.8482 0s - loss: 0.4199 - acc:\n",
      "Epoch 5/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.4106 - acc: 0.8544 - categorical_accuracy: 0.8544\n",
      "Epoch 6/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3929 - acc: 0.8596 - categorical_accuracy: 0.8596\n",
      "Epoch 7/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3740 - acc: 0.8663 - categorical_accuracy: 0.8663\n",
      "Epoch 8/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3626 - acc: 0.8705 - categorical_accuracy: 0.8705\n",
      "Epoch 9/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.3487 - acc: 0.8740 - categorical_accuracy: 0.8740 1s - loss: 0.3495 - \n",
      "Epoch 10/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.3452 - acc: 0.8761 - categorical_accuracy: 0.8761\n",
      "Epoch 11/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.3330 - acc: 0.8787 - categorical_accuracy: 0.8787\n",
      "Epoch 12/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3254 - acc: 0.8820 - categorical_accuracy: 0.8820\n",
      "Epoch 13/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3232 - acc: 0.8825 - categorical_accuracy: 0.8825\n",
      "Epoch 14/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3121 - acc: 0.8869 - categorical_accuracy: 0.8869\n",
      "Epoch 15/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3076 - acc: 0.8879 - categorical_accuracy: 0.8879\n",
      "Epoch 16/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3045 - acc: 0.8894 - categorical_accuracy: 0.8894\n",
      "Epoch 17/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.2975 - acc: 0.8918 - categorical_accuracy: 0.8918\n",
      "Epoch 18/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.2938 - acc: 0.8924 - categorical_accuracy: 0.8924\n",
      "Epoch 19/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.2911 - acc: 0.8944 - categorical_accuracy: 0.8944\n",
      "Epoch 20/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.2862 - acc: 0.8959 - categorical_accuracy: 0.8959\n"
     ]
    }
   ],
   "source": [
    "with tf.device('GPU'):\n",
    "    sgd = optimizers.sgd(momentum=0.9)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f974249e5a64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0myhat_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "yhat_val = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "assert len(backend.tensorflow_backend._get_available_gpus()) > 0\n",
    "\n",
    "# confirm PyTorch sees the GPU\n",
    "#from torch import cuda\n",
    "#assert cuda.is_available()\n",
    "#assert cuda.device_count() > 0\n",
    "#print(cuda.get_device_name(cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import logsumexp\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  (a * (1 - a ))\n",
    "    \n",
    "    def __softmax(self, z):\n",
    "        assert len(z.shape) == 2\n",
    "        s = np.max(z, axis=1)\n",
    "        s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "        e_x = np.exp(z - s)\n",
    "        div = np.sum(e_x, axis=1)\n",
    "        div = div[:, np.newaxis] # dito\n",
    "        return e_x / div\n",
    "    \n",
    "    def __softmax_deriv(self, a):\n",
    "        #a = softmax(x)\n",
    "        return (a * (1 - a))\n",
    "    \n",
    "    def __ReLU(self,x):\n",
    "        return np.vectorize(lambda x:x if x>0 else 0)(x)\n",
    "    \n",
    "    def __ReLU_deriv(self,a):\n",
    "        #a = ReLU()\n",
    "        return np.vectorize(lambda x:1 if x>0 else 0)(a)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__ReLU\n",
    "            self.f_deriv = self.__ReLU_deriv\n",
    "            \n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', dropout=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout=dropout\n",
    "        self.dropout_vector = None\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, mode):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        if (mode=='train' and self.dropout>0):\n",
    "            self.dropout_vector = np.random.binomial(1, 1-self.dropout, size=input.shape)/(1-self.dropout)\n",
    "            lin_output = np.dot(self.dropout_vector*input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):\n",
    "        self.grad_W = np.atleast_2d(self.dropout_vector*self.input if self.dropout>0 else self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = np.mean(delta,axis=0)\n",
    "\n",
    "        \n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], batch_size=False, dropout=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.mode = 'train'\n",
    "        self.activation=activation\n",
    "        self.dropout=dropout\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],self.dropout[i]))\n",
    "            \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input=input, mode=self.mode)\n",
    "            input=output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=error**2\n",
    "        # calculate the delta of the output layer\n",
    "        print(activation_deriv(y_hat))\n",
    "        delta=-error*activation_deriv(y_hat)\n",
    "        print('delta',delta.shape)\n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    def criterion_CELoss(self,y,y_hat):\n",
    "        error = y * np.log(y_hat)\n",
    "        loss = -np.sum(error)\n",
    "        delta = y_hat-y\n",
    "        return loss,delta\n",
    "        \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "            \n",
    "    def get_batches(self,X, y, batch_size):\n",
    "        batches = []\n",
    "\n",
    "        X, y = shuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_batch = X[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "            \n",
    "            batches.append((X_batch, y_batch))\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=10, batch_size=False):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.batch_size=batch_size\n",
    "        self.train()\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        \n",
    "        # Differentiate Stochastic Gradient Descent vs Batch Gradient Descent\n",
    "        if batch_size:\n",
    "            batches = self.get_batches(X, y_dummies, batch_size)\n",
    "            for k in range(epochs):\n",
    "                loss = np.zeros(X.shape[0])\n",
    "                for X,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        loss,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                        \n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate/batch_size)\n",
    "                to_return[k] = np.mean(loss)\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss=np.zeros(X.shape[0])\n",
    "                for it in range(X.shape[0]):\n",
    "                    i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X[i])\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                to_return[k] = np.mean(loss)\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.test()\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        output = self.forward(x)\n",
    "        return output\n",
    "    \n",
    "    def optimize(self, X, y, learning_rate=0.01, test_size=0.25, epochs=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        losses = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            loss=np.zeros(X_train.shape[0])         \n",
    "            \n",
    "            self.test()\n",
    "            yhat_train = self.forward(X_train)\n",
    "            yhat_val = self.forward(X_val)\n",
    "            \n",
    "            # Calculate train and Test Accuracy\n",
    "            accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "            accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "            \n",
    "            self.train()\n",
    "            for it in range(X_train.shape[0]):\n",
    "                i=np.random.randint(X_train.shape[0])\n",
    "                \n",
    "                \n",
    "                # forward pass\n",
    "                y_hat = self.forward(X_train[i])\n",
    "\n",
    "                # backward pass\n",
    "                if self.activation[-1] == 'softmax':\n",
    "                    loss[it],delta = self.criterion_CELoss(y_train[i],y_hat)\n",
    "                else:\n",
    "                    loss[it],delta = self.criterion_MSE(y_train[i],y_hat)\n",
    "                \n",
    "                self.backward(delta)\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "                \n",
    "            self.test()\n",
    "            yhat_train = self.forward(X_train)\n",
    "            yhat_val = self.forward(X_val)\n",
    "            accuracies_val.append(accuracy_train)\n",
    "            accuracies_test.append(accuracy_val)\n",
    "            \n",
    "            if verbose:\n",
    "                print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                      format(e, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "            \n",
    "            losses[e] = np.mean(loss)\n",
    "        return losses, accuracies_val, accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "X=np.array(data)\n",
    "y=np.array(label)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.8, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU','ReLU', 'softmax'], dropout=[0, 0, 0])\n",
    "\n",
    "mlp.fit(X_train, y_train, learning_rate=0.01, batch_size=100, epochs=50)\n",
    "predictions = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8034583333333334"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.argmax(predictions,axis=1)==y_val).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00075403, 0.0008682 , 0.00254612, ..., 0.00680553, 0.16737105,\n",
       "        0.04279829],\n",
       "       [0.12927047, 0.16414021, 0.33458677, ..., 0.00966869, 0.04298051,\n",
       "        0.03003678],\n",
       "       [0.10354451, 0.05965707, 0.04696145, ..., 0.21512176, 0.06944888,\n",
       "        0.05644935],\n",
       "       ...,\n",
       "       [0.08890817, 0.01231172, 0.03287138, ..., 0.16082602, 0.45583243,\n",
       "        0.09237466],\n",
       "       [0.0981138 , 0.10013032, 0.0730484 , ..., 0.00644853, 0.10344582,\n",
       "        0.13006442],\n",
       "       [0.03750491, 0.09140053, 0.02441754, ..., 0.2994896 , 0.0482239 ,\n",
       "        0.13411234]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-f649c8e3e8bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-516d300ca621>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, epochs, verbose)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m                 \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[1;31m# backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-516d300ca621>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-516d300ca621>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, mode)\u001b[0m\n\u001b[0;32m    120\u001b[0m         self.output = (\n\u001b[0;32m    121\u001b[0m             \u001b[0mlin_output\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlin_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         )\n\u001b[0;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-516d300ca621>\u001b[0m in \u001b[0;36m__softmax\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# necessary step to do broadcasting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,32,10],activation=[None, 'ReLU', 'softmax'], dropout=[0, 0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.1,epochs=10)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.08288888888888889 \n",
      "Validation Accuracy: 0.09586666666666667 \n",
      "Loss: 0.5693072104850063 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.29984444444444447 \n",
      "Validation Accuracy: 0.2308 \n",
      "Loss: 0.43143106884447724 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.3967333333333333 \n",
      "Validation Accuracy: 0.39653333333333335 \n",
      "Loss: 0.41148782188299593 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.31575555555555557 \n",
      "Validation Accuracy: 0.30546666666666666 \n",
      "Loss: 0.4038022033626358 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.2742222222222222 \n",
      "Validation Accuracy: 0.3324666666666667 \n",
      "Loss: 0.3830421646092189 \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.45944444444444443 \n",
      "Validation Accuracy: 0.37726666666666664 \n",
      "Loss: 0.38414768872433414 \n",
      "\n",
      "Epoch: 6..\n",
      "train Accuracy: 0.3297777777777778 \n",
      "Validation Accuracy: 0.38233333333333336 \n",
      "Loss: 0.3779819147708331 \n",
      "\n",
      "Epoch: 7..\n",
      "train Accuracy: 0.2356888888888889 \n",
      "Validation Accuracy: 0.3218 \n",
      "Loss: 0.3690235986728638 \n",
      "\n",
      "Epoch: 8..\n",
      "train Accuracy: 0.2871111111111111 \n",
      "Validation Accuracy: 0.4760666666666667 \n",
      "Loss: 0.37107293227556915 \n",
      "\n",
      "Epoch: 9..\n",
      "train Accuracy: 0.4762 \n",
      "Validation Accuracy: 0.4464 \n",
      "Loss: 0.36327410338089683 \n",
      "\n",
      "Epoch: 10..\n",
      "train Accuracy: 0.4413111111111111 \n",
      "Validation Accuracy: 0.4441333333333333 \n",
      "Loss: 0.35852222792602917 \n",
      "\n",
      "Epoch: 11..\n",
      "train Accuracy: 0.2136222222222222 \n",
      "Validation Accuracy: 0.21686666666666668 \n",
      "Loss: 0.3484613209308596 \n",
      "\n",
      "Epoch: 12..\n",
      "train Accuracy: 0.38415555555555553 \n",
      "Validation Accuracy: 0.3725333333333333 \n",
      "Loss: 0.35725381287887265 \n",
      "\n",
      "Epoch: 13..\n",
      "train Accuracy: 0.3708888888888889 \n",
      "Validation Accuracy: 0.3318 \n",
      "Loss: 0.34747492715945844 \n",
      "\n",
      "Epoch: 14..\n",
      "train Accuracy: 0.3804888888888889 \n",
      "Validation Accuracy: 0.3498 \n",
      "Loss: 0.3506235426082402 \n",
      "\n",
      "Epoch: 15..\n",
      "train Accuracy: 0.31466666666666665 \n",
      "Validation Accuracy: 0.32953333333333334 \n",
      "Loss: 0.3436369864378808 \n",
      "\n",
      "Epoch: 16..\n",
      "train Accuracy: 0.27595555555555557 \n",
      "Validation Accuracy: 0.2916 \n",
      "Loss: 0.344191868727738 \n",
      "\n",
      "Epoch: 17..\n",
      "train Accuracy: 0.26206666666666667 \n",
      "Validation Accuracy: 0.3340666666666667 \n",
      "Loss: 0.35550668328826546 \n",
      "\n",
      "Epoch: 18..\n",
      "train Accuracy: 0.14195555555555556 \n",
      "Validation Accuracy: 0.36273333333333335 \n",
      "Loss: 0.3389029606236429 \n",
      "\n",
      "Epoch: 19..\n",
      "train Accuracy: 0.1857111111111111 \n",
      "Validation Accuracy: 0.1854 \n",
      "Loss: 0.3480019959235619 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.02,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(32,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (32,) and (1,1) not aligned: 32 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-9e969c7d6dab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, epochs, verbose)\u001b[0m\n\u001b[0;32m    314\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion_MSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[1;31m# update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta, output_layer)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_vector\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (32,) and (1,1) not aligned: 32 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,512,128,32,10],activation=[None, 'logistic', 'logistic', 'logistic', 'softmax'], dropout=[0.0, 0.0, 0.0, 0.0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.02,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tensorflow.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \n",
    "sess = tensorflow.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras import optimizers, metrics, Sequential\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "y = label\n",
    "X = data\n",
    "\n",
    "y_dummies = np.array(pd.get_dummies(y))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=0.25, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45000/45000 [==============================] - 2s 42us/step - loss: 0.9828 - acc: 0.6622 - categorical_accuracy: 0.6622\n",
      "Epoch 2/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.5128 - acc: 0.8221 - categorical_accuracy: 0.8221\n",
      "Epoch 3/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.4623 - acc: 0.8380 - categorical_accuracy: 0.8380\n",
      "Epoch 4/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.4270 - acc: 0.8482 - categorical_accuracy: 0.8482 0s - loss: 0.4199 - acc:\n",
      "Epoch 5/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.4106 - acc: 0.8544 - categorical_accuracy: 0.8544\n",
      "Epoch 6/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3929 - acc: 0.8596 - categorical_accuracy: 0.8596\n",
      "Epoch 7/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3740 - acc: 0.8663 - categorical_accuracy: 0.8663\n",
      "Epoch 8/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3626 - acc: 0.8705 - categorical_accuracy: 0.8705\n",
      "Epoch 9/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.3487 - acc: 0.8740 - categorical_accuracy: 0.8740 1s - loss: 0.3495 - \n",
      "Epoch 10/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.3452 - acc: 0.8761 - categorical_accuracy: 0.8761\n",
      "Epoch 11/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.3330 - acc: 0.8787 - categorical_accuracy: 0.8787\n",
      "Epoch 12/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3254 - acc: 0.8820 - categorical_accuracy: 0.8820\n",
      "Epoch 13/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3232 - acc: 0.8825 - categorical_accuracy: 0.8825\n",
      "Epoch 14/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3121 - acc: 0.8869 - categorical_accuracy: 0.8869\n",
      "Epoch 15/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3076 - acc: 0.8879 - categorical_accuracy: 0.8879\n",
      "Epoch 16/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3045 - acc: 0.8894 - categorical_accuracy: 0.8894\n",
      "Epoch 17/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.2975 - acc: 0.8918 - categorical_accuracy: 0.8918\n",
      "Epoch 18/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.2938 - acc: 0.8924 - categorical_accuracy: 0.8924\n",
      "Epoch 19/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.2911 - acc: 0.8944 - categorical_accuracy: 0.8944\n",
      "Epoch 20/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.2862 - acc: 0.8959 - categorical_accuracy: 0.8959\n"
     ]
    }
   ],
   "source": [
    "with tf.device('GPU'):\n",
    "    sgd = optimizers.sgd(momentum=0.9)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f974249e5a64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0myhat_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "yhat_val = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "assert len(backend.tensorflow_backend._get_available_gpus()) > 0\n",
    "\n",
    "# confirm PyTorch sees the GPU\n",
    "#from torch import cuda\n",
    "#assert cuda.is_available()\n",
    "#assert cuda.device_count() > 0\n",
    "#print(cuda.get_device_name(cuda.current_device()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  (a * (1 - a ))\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        #return np.exp(x)/(np.sum(np.exp(x),axis=1)[:,None])\n",
    "        return (np.exp(x)/(np.sum(np.exp(x))))\n",
    "    \n",
    "    def __softmax_deriv(self, a):\n",
    "        #a = softmax(x)\n",
    "        return (a * (1 - a))\n",
    "    \n",
    "    def __ReLU(self,x):\n",
    "        return np.vectorize(lambda x:x if x>0 else 0)(x)\n",
    "    \n",
    "    def __ReLU_deriv(self,a):\n",
    "        #a = ReLU()\n",
    "        return np.vectorize(lambda x:1 if x>0 else 0)(a)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__ReLU\n",
    "            self.f_deriv = self.__ReLU_deriv\n",
    "            \n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', dropout=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout=dropout\n",
    "        self.dropout_vector = None\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, mode):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        if (mode=='train' and self.dropout>0):\n",
    "            self.dropout_vector = np.random.binomial(1, 1-self.dropout, size=input.shape)/(1-self.dropout)\n",
    "            lin_output = np.dot(self.dropout_vector*input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):\n",
    "        self.grad_W = (np.atleast_2d(self.dropout_vector*self.input if self.dropout>0 else self.input).T.dot(np.atleast_2d(delta)))\n",
    "        self.grad_b = delta\n",
    "        \n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], dropout=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.mode = 'train'\n",
    "        self.activation=activation\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],self.dropout[i]))\n",
    "            \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input=input, mode=self.mode)\n",
    "            input=output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=error**2\n",
    "        # calculate the delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)    \n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    def criterion_CELoss(self,y,y_hat):\n",
    "        error = y*np.log(y_hat)\n",
    "        loss = -np.sum(error)\n",
    "        delta = (y_hat-y)\n",
    "        return loss,delta\n",
    "        \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=10):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            loss=np.zeros(X.shape[0])\n",
    "            for it in range(X.shape[0]):\n",
    "                i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                # forward pass\n",
    "                y_hat = self.forward(X[i])\n",
    "                \n",
    "                # backward pass\n",
    "                if self.activation[-1] == 'softmax':\n",
    "                    loss[it],delta=self.criterion_CELoss(y[i],y_hat)\n",
    "                else:\n",
    "                    loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
    "                \n",
    "                self.backward(delta)\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "            to_return[k] = np.mean(loss)\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.test()\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = self.forward(x[i,:])\n",
    "        return output\n",
    "    \n",
    "    def optimize(self, X, y, learning_rate=0.01, test_size=0.25, epochs=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        losses = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            loss=np.zeros(X_train.shape[0])         \n",
    "            \n",
    "            self.test()\n",
    "            yhat_train = self.forward(X_train)\n",
    "            yhat_val = self.forward(X_val)\n",
    "            \n",
    "            # Calculate train and Test Accuracy\n",
    "            accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "            accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "            \n",
    "            self.train()\n",
    "            for it in range(X_train.shape[0]):\n",
    "                i=np.random.randint(X_train.shape[0])\n",
    "                \n",
    "                \n",
    "                # forward pass\n",
    "                y_hat = self.forward(X_train[i])\n",
    "\n",
    "                # backward pass\n",
    "                if self.activation[-1] == 'softmax':\n",
    "                    loss[it],delta = self.criterion_CELoss(y_train[i],y_hat)\n",
    "                else:\n",
    "                    loss[it],delta = self.criterion_MSE(y_train[i],y_hat)\n",
    "                \n",
    "                self.backward(delta)\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "                \n",
    "            self.test()\n",
    "            yhat_train = self.forward(X_train)\n",
    "            yhat_val = self.forward(X_val)\n",
    "            accuracies_val.append(accuracy_train)\n",
    "            accuracies_test.append(accuracy_val)\n",
    "            \n",
    "            if verbose:\n",
    "                print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                      format(e, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "            \n",
    "            losses[e] = np.mean(loss)\n",
    "        return losses, accuracies_val, accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.10246666666666666 \n",
      "Validation Accuracy: 0.1174 \n",
      "Loss: 0.6678961952755198 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.8626888888888888 \n",
      "Validation Accuracy: 0.8458 \n",
      "Loss: 0.34707613960717637 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.8796888888888889 \n",
      "Validation Accuracy: 0.8606666666666667 \n",
      "Loss: 0.2965338498545959 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.8864666666666666 \n",
      "Validation Accuracy: 0.8628666666666667 \n",
      "Loss: 0.26903911945330916 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.9069111111111111 \n",
      "Validation Accuracy: 0.8658 \n",
      "Loss: 0.24612234831952046 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,512,128,32,10],activation=[None, 'ReLU', 'ReLU', 'ReLU', 'softmax'], dropout=[0, 0, 0, 0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.01,epochs=5)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.10271111111111111 \n",
      "Validation Accuracy: 0.0982 \n",
      "Loss: 0.6172979665369405 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.8249111111111112 \n",
      "Validation Accuracy: 0.8136666666666666 \n",
      "Loss: 0.5552553613926874 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.8004666666666667 \n",
      "Validation Accuracy: 0.7898666666666667 \n",
      "Loss: 0.5357286066458394 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.8010222222222222 \n",
      "Validation Accuracy: 0.7884 \n",
      "Loss: 0.5676060786778625 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.8335555555555556 \n",
      "Validation Accuracy: 0.8195333333333333 \n",
      "Loss: 0.5942903682121251 \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.7999111111111111 \n",
      "Validation Accuracy: 0.7872666666666667 \n",
      "Loss: 0.6980279904846414 \n",
      "\n",
      "Epoch: 6..\n",
      "train Accuracy: 0.7865111111111112 \n",
      "Validation Accuracy: 0.7770666666666667 \n",
      "Loss: 0.7687783121083894 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:172: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:172: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:27: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\numpy\\lib\\function_base.py:2048: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7..\n",
      "train Accuracy: 0.7671555555555556 \n",
      "Validation Accuracy: 0.7549333333333333 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 8..\n",
      "train Accuracy: 0.1005111111111111 \n",
      "Validation Accuracy: 0.09846666666666666 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 9..\n",
      "train Accuracy: 0.1005111111111111 \n",
      "Validation Accuracy: 0.09846666666666666 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 10..\n",
      "train Accuracy: 0.1005111111111111 \n",
      "Validation Accuracy: 0.09846666666666666 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 11..\n",
      "train Accuracy: 0.1005111111111111 \n",
      "Validation Accuracy: 0.09846666666666666 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 12..\n",
      "train Accuracy: 0.1005111111111111 \n",
      "Validation Accuracy: 0.09846666666666666 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 13..\n",
      "train Accuracy: 0.1005111111111111 \n",
      "Validation Accuracy: 0.09846666666666666 \n",
      "Loss: nan \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.02,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.09451111111111112 \n",
      "Validation Accuracy: 0.0938 \n",
      "Loss: 0.6607643513796858 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.8525111111111111 \n",
      "Validation Accuracy: 0.8373333333333334 \n",
      "Loss: 0.3708735144164001 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.8790444444444444 \n",
      "Validation Accuracy: 0.8522666666666666 \n",
      "Loss: 0.3290510188457558 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.8976666666666666 \n",
      "Validation Accuracy: 0.8583333333333333 \n",
      "Loss: 0.2871276297775745 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,512,128,32,10],activation=[None, 'logistic', 'logistic', 'logistic', 'softmax'], dropout=[0.1, 0.1, 0.1, 0.1, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.01,epochs=5)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras import optimizers, metrics, Sequential\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "y = label\n",
    "X = data\n",
    "\n",
    "y_dummies = np.array(pd.get_dummies(y))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=0.25, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tensorflow.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \n",
    "sess = tensorflow.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45000/45000 [==============================] - 1s 27us/step - loss: 1.6435 - acc: 0.5002 - categorical_accuracy: 0.5002\n",
      "Epoch 2/20\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.6294 - acc: 0.8018 - categorical_accuracy: 0.8018\n",
      "Epoch 3/20\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.4744 - acc: 0.8411 - categorical_accuracy: 0.8411\n",
      "Epoch 4/20\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.4227 - acc: 0.8556 - categorical_accuracy: 0.8556\n",
      "Epoch 5/20\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3917 - acc: 0.8647 - categorical_accuracy: 0.8647\n",
      "Epoch 6/20\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3686 - acc: 0.8728 - categorical_accuracy: 0.8728\n",
      "Epoch 7/20\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3492 - acc: 0.8788 - categorical_accuracy: 0.8788\n",
      "Epoch 8/20\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3331 - acc: 0.8841 - categorical_accuracy: 0.8841\n",
      "Epoch 9/20\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3185 - acc: 0.8889 - categorical_accuracy: 0.8889\n",
      "Epoch 10/20\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3054 - acc: 0.8928 - categorical_accuracy: 0.8928\n",
      "Epoch 11/20\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.2935 - acc: 0.8973 - categorical_accuracy: 0.8973\n",
      "Epoch 12/20\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.2822 - acc: 0.9013 - categorical_accuracy: 0.9013\n",
      "Epoch 13/20\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.2720 - acc: 0.9055 - categorical_accuracy: 0.9055\n",
      "Epoch 14/20\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.2618 - acc: 0.9084 - categorical_accuracy: 0.9084\n",
      "Epoch 15/20\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.2523 - acc: 0.9106 - categorical_accuracy: 0.9106\n",
      "Epoch 16/20\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.2439 - acc: 0.9143 - categorical_accuracy: 0.9143\n",
      "Epoch 17/20\n",
      "45000/45000 [==============================] - 1s 22us/step - loss: 0.2348 - acc: 0.9186 - categorical_accuracy: 0.9186\n",
      "Epoch 18/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.2264 - acc: 0.9206 - categorical_accuracy: 0.9206\n",
      "Epoch 19/20\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.2188 - acc: 0.9247 - categorical_accuracy: 0.9247\n",
      "Epoch 20/20\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.2110 - acc: 0.9271 - categorical_accuracy: 0.9271\n"
     ]
    }
   ],
   "source": [
    "with tf.device('GPU'):\n",
    "    sgd = optimizers.sgd()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_val = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8824666666666666"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "assert len(backend.tensorflow_backend._get_available_gpus()) > 0\n",
    "\n",
    "# confirm PyTorch sees the GPU\n",
    "#from torch import cuda\n",
    "#assert cuda.is_available()\n",
    "#assert cuda.device_count() > 0\n",
    "#print(cuda.get_device_name(cuda.current_device()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

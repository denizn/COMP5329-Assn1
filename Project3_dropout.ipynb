{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  (a * (1 - a ))\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        #return np.exp(x)/(np.sum(np.exp(x),axis=1)[:,None])\n",
    "        return (np.exp(x)/(np.sum(np.exp(x))))\n",
    "    \n",
    "    def __softmax_deriv(self, a):\n",
    "        #a = softmax(x)\n",
    "        return (a * (1 - a))\n",
    "    \n",
    "    def __ReLU(self,x):\n",
    "        return np.vectorize(lambda x:x if x>0 else 0)(x)\n",
    "    \n",
    "    def __ReLU_deriv(self,a):\n",
    "        #a = ReLU()\n",
    "        return np.vectorize(lambda x:1 if x>0 else 0)(a)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__ReLU\n",
    "            self.f_deriv = self.__ReLU_deriv\n",
    "            \n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', dropout=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout=dropout\n",
    "        self.dropout_vector = None\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, mode):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        if (mode=='train' and self.dropout>0):\n",
    "            self.dropout_vector = np.random.binomial(1, 1-self.dropout, size=input.shape)/(1-self.dropout)\n",
    "            lin_output = np.dot(self.dropout_vector*input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):\n",
    "        self.grad_W = (np.atleast_2d(self.dropout_vector*self.input if self.dropout>0 else self.input).T.dot(np.atleast_2d(delta)))\n",
    "        self.grad_b = delta\n",
    "        \n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], dropout=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.mode = 'train'\n",
    "        self.activation=activation\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],self.dropout[i]))\n",
    "            \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input=input, mode=self.mode)\n",
    "            input=output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=error**2\n",
    "        # calculate the delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)    \n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    def criterion_CELoss(self,y,y_hat):\n",
    "        error = y*np.log(y_hat)\n",
    "        loss = -np.sum(error)\n",
    "        delta = (y_hat-y)\n",
    "        return loss,delta\n",
    "        \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=10):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            loss=np.zeros(X.shape[0])\n",
    "            for it in range(X.shape[0]):\n",
    "                i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                # forward pass\n",
    "                y_hat = self.forward(X[i])\n",
    "                \n",
    "                # backward pass\n",
    "                if self.activation[-1] == 'softmax':\n",
    "                    loss[it],delta=self.criterion_CELoss(y[i],y_hat)\n",
    "                else:\n",
    "                    loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
    "                \n",
    "                self.backward(delta)\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "            to_return[k] = np.mean(loss)\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.test()\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = self.forward(x[i,:])\n",
    "        return output\n",
    "    \n",
    "    def optimize(self, X, y, learning_rate=0.01, test_size=0.25, epochs=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        losses = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            loss=np.zeros(X_train.shape[0])         \n",
    "            \n",
    "            self.test()\n",
    "            yhat_train = self.forward(X_train)\n",
    "            yhat_val = self.forward(X_val)\n",
    "            \n",
    "            # Calculate train and Test Accuracy\n",
    "            accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "            accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "            \n",
    "            self.train()\n",
    "            for it in range(X_train.shape[0]):\n",
    "                i=np.random.randint(X_train.shape[0])\n",
    "                \n",
    "                \n",
    "                # forward pass\n",
    "                y_hat = self.forward(X_train[i])\n",
    "\n",
    "                # backward pass\n",
    "                if self.activation[-1] == 'softmax':\n",
    "                    loss[it],delta = self.criterion_CELoss(y_train[i],y_hat)\n",
    "                else:\n",
    "                    loss[it],delta = self.criterion_MSE(y_train[i],y_hat)\n",
    "                \n",
    "                self.backward(delta)\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "                \n",
    "            self.test()\n",
    "            yhat_train = self.forward(X_train)\n",
    "            yhat_val = self.forward(X_val)\n",
    "            accuracies_val.append(accuracy_train)\n",
    "            accuracies_test.append(accuracy_val)\n",
    "            \n",
    "            if verbose:\n",
    "                print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                      format(e, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "            \n",
    "            losses[e] = np.mean(loss)\n",
    "        return losses, accuracies_val, accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.10335555555555556 \n",
      "Validation Accuracy: 0.12133333333333333 \n",
      "Loss: 0.5532555242731952 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.8624444444444445 \n",
      "Validation Accuracy: 0.8476 \n",
      "Loss: 0.37749771970350426 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.8786222222222222 \n",
      "Validation Accuracy: 0.8607333333333334 \n",
      "Loss: 0.34724235141318954 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.8777555555555555 \n",
      "Validation Accuracy: 0.8522 \n",
      "Loss: 0.3258745821826974 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.8846888888888889 \n",
      "Validation Accuracy: 0.8623333333333333 \n",
      "Loss: 0.3154830288160891 \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.8840222222222223 \n",
      "Validation Accuracy: 0.8590666666666666 \n",
      "Loss: 0.3015172914055102 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0, 0, 0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.01,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.10875555555555555 \n",
      "Validation Accuracy: 0.10873333333333333 \n",
      "Loss: 0.5505329053184421 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.8651777777777778 \n",
      "Validation Accuracy: 0.8517333333333333 \n",
      "Loss: 0.3441838611341829 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.8810888888888889 \n",
      "Validation Accuracy: 0.8559333333333333 \n",
      "Loss: 0.3115651556755247 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.8911555555555556 \n",
      "Validation Accuracy: 0.8599333333333333 \n",
      "Loss: 0.29806389282704987 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.8976888888888889 \n",
      "Validation Accuracy: 0.8632666666666666 \n",
      "Loss: 0.2883581506613798 \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.9004666666666666 \n",
      "Validation Accuracy: 0.8630666666666666 \n",
      "Loss: 0.26882767596767276 \n",
      "\n",
      "Epoch: 6..\n",
      "train Accuracy: 0.9033333333333333 \n",
      "Validation Accuracy: 0.8668666666666667 \n",
      "Loss: 0.25635955756361506 \n",
      "\n",
      "Epoch: 7..\n",
      "train Accuracy: 0.9083777777777777 \n",
      "Validation Accuracy: 0.8635333333333334 \n",
      "Loss: 0.2525654893459104 \n",
      "\n",
      "Epoch: 8..\n",
      "train Accuracy: 0.9095111111111112 \n",
      "Validation Accuracy: 0.8663333333333333 \n",
      "Loss: 0.23694864158226017 \n",
      "\n",
      "Epoch: 9..\n",
      "train Accuracy: 0.9130888888888888 \n",
      "Validation Accuracy: 0.8678666666666667 \n",
      "Loss: 0.23372472027583874 \n",
      "\n",
      "Epoch: 10..\n",
      "train Accuracy: 0.9151777777777778 \n",
      "Validation Accuracy: 0.8667333333333334 \n",
      "Loss: 0.2378872657168885 \n",
      "\n",
      "Epoch: 11..\n",
      "train Accuracy: 0.9127777777777778 \n",
      "Validation Accuracy: 0.8684 \n",
      "Loss: 0.2278051454584211 \n",
      "\n",
      "Epoch: 12..\n",
      "train Accuracy: 0.9206444444444445 \n",
      "Validation Accuracy: 0.8649333333333333 \n",
      "Loss: 0.21919375675232727 \n",
      "\n",
      "Epoch: 13..\n",
      "train Accuracy: 0.9217333333333333 \n",
      "Validation Accuracy: 0.867 \n",
      "Loss: 0.21454163908212867 \n",
      "\n",
      "Epoch: 14..\n",
      "train Accuracy: 0.9242222222222222 \n",
      "Validation Accuracy: 0.8694 \n",
      "Loss: 0.2118705860475391 \n",
      "\n",
      "Epoch: 15..\n",
      "train Accuracy: 0.9213555555555556 \n",
      "Validation Accuracy: 0.8678 \n",
      "Loss: 0.2089843705648551 \n",
      "\n",
      "Epoch: 16..\n",
      "train Accuracy: 0.9251333333333334 \n",
      "Validation Accuracy: 0.8656 \n",
      "Loss: 0.2093556465082595 \n",
      "\n",
      "Epoch: 17..\n",
      "train Accuracy: 0.9258666666666666 \n",
      "Validation Accuracy: 0.8676 \n",
      "Loss: 0.20231651723972127 \n",
      "\n",
      "Epoch: 18..\n",
      "train Accuracy: 0.9299555555555555 \n",
      "Validation Accuracy: 0.8673333333333333 \n",
      "Loss: 0.19688196311738426 \n",
      "\n",
      "Epoch: 19..\n",
      "train Accuracy: 0.9283111111111111 \n",
      "Validation Accuracy: 0.8669333333333333 \n",
      "Loss: 0.19644967048555637 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'logistic', 'softmax'], dropout=[0.2, 0.2, 0.2, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.01,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

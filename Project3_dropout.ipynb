{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  (a * (1 - a ))\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        #return np.exp(x)/(np.sum(np.exp(x),axis=1)[:,None])\n",
    "        return (np.exp(x)/(np.sum(np.exp(x))))\n",
    "    \n",
    "    def __softmax_deriv(self, a):\n",
    "        #a = softmax(x)\n",
    "        return (a * (1 - a))\n",
    "    \n",
    "    def __ReLU(self,x):\n",
    "        return np.vectorize(lambda x:x if x>0 else 0)(x)\n",
    "    \n",
    "    def __ReLU_deriv(self,a):\n",
    "        #a = ReLU()\n",
    "        return np.vectorize(lambda x:1 if x>0 else 0)(a)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__ReLU\n",
    "            self.f_deriv = self.__ReLU_deriv\n",
    "            \n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', dropout=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout=dropout\n",
    "        self.dropout_vector = None\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, mode):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        if (mode=='train' and self.dropout>0):\n",
    "            self.dropout_vector = np.random.binomial(1, 1-self.dropout, size=input.shape)/(1-self.dropout)\n",
    "            lin_output = np.dot(self.dropout_vector*input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):\n",
    "        self.grad_W = (np.atleast_2d(self.dropout_vector*self.input if self.dropout>0 else self.input).T.dot(np.atleast_2d(delta)))\n",
    "        self.grad_b = delta\n",
    "        \n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], dropout=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.mode = 'train'\n",
    "        self.activation=activation\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],self.dropout[i]))\n",
    "            \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input=input, mode=self.mode)\n",
    "            input=output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=error**2\n",
    "        # calculate the delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)    \n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    def criterion_CELoss(self,y,y_hat):\n",
    "        error = y*np.log(y_hat)\n",
    "        loss = -np.sum(error)\n",
    "        delta = (y_hat-y)\n",
    "        return loss,delta\n",
    "        \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=10):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            loss=np.zeros(X.shape[0])\n",
    "            for it in range(X.shape[0]):\n",
    "                i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                # forward pass\n",
    "                y_hat = self.forward(X[i])\n",
    "                \n",
    "                # backward pass\n",
    "                if self.activation[-1] == 'softmax':\n",
    "                    loss[it],delta=self.criterion_CELoss(y[i],y_hat)\n",
    "                else:\n",
    "                    loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
    "                \n",
    "                self.backward(delta)\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "            to_return[k] = np.mean(loss)\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.test()\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = self.forward(x[i,:])\n",
    "        return output\n",
    "    \n",
    "    def optimize(self, X, y, learning_rate=0.01, test_size=0.25, epochs=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        losses = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            loss=np.zeros(X_train.shape[0])         \n",
    "            \n",
    "            self.test()\n",
    "            yhat_train = self.forward(X_train)\n",
    "            yhat_val = self.forward(X_val)\n",
    "            \n",
    "            # Calculate train and Test Accuracy\n",
    "            accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "            accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "            \n",
    "            for it in range(X_train.shape[0]):\n",
    "                i=np.random.randint(X_train.shape[0])\n",
    "                \n",
    "                self.train()\n",
    "                # forward pass\n",
    "                y_hat = self.forward(X_train[i])\n",
    "\n",
    "                # backward pass\n",
    "                if self.activation[-1] == 'softmax':\n",
    "                    loss[it],delta = self.criterion_CELoss(y_train[i],y_hat)\n",
    "                else:\n",
    "                    loss[it],delta = self.criterion_MSE(y_train[i],y_hat)\n",
    "                \n",
    "                self.backward(delta)\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "                \n",
    "            self.test()\n",
    "            yhat_train = self.forward(X_train)\n",
    "            yhat_val = self.forward(X_val)\n",
    "            accuracies_val.append(accuracy_train)\n",
    "            accuracies_test.append(accuracy_val)\n",
    "            \n",
    "            self.train()\n",
    "            \n",
    "            if verbose:\n",
    "                print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                      format(e, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "            \n",
    "            losses[e] = np.mean(loss)\n",
    "        return losses, accuracies_val, accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.12495555555555556 \n",
      "Validation Accuracy: 0.1254 \n",
      "Loss: 0.5308190834985449 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.8585555555555555 \n",
      "Validation Accuracy: 0.8405333333333334 \n",
      "Loss: 0.3374377932227098 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.8823555555555556 \n",
      "Validation Accuracy: 0.8556 \n",
      "Loss: 0.31021279472605107 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.8904 \n",
      "Validation Accuracy: 0.8619333333333333 \n",
      "Loss: 0.28358820377174765 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.9019555555555555 \n",
      "Validation Accuracy: 0.8639333333333333 \n",
      "Loss: 0.2711784780941892 \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.9079777777777778 \n",
      "Validation Accuracy: 0.8692 \n",
      "Loss: 0.25591111179230713 \n",
      "\n",
      "Epoch: 6..\n",
      "train Accuracy: 0.9088222222222222 \n",
      "Validation Accuracy: 0.8648 \n",
      "Loss: 0.24622581037309219 \n",
      "\n",
      "Epoch: 7..\n",
      "train Accuracy: 0.9147777777777778 \n",
      "Validation Accuracy: 0.8676666666666667 \n",
      "Loss: 0.23603641260914876 \n",
      "\n",
      "Epoch: 8..\n",
      "train Accuracy: 0.9141333333333334 \n",
      "Validation Accuracy: 0.8644 \n",
      "Loss: 0.2289413031067155 \n",
      "\n",
      "Epoch: 9..\n",
      "train Accuracy: 0.9194444444444444 \n",
      "Validation Accuracy: 0.8649333333333333 \n",
      "Loss: 0.22360936419366148 \n",
      "\n",
      "Epoch: 10..\n",
      "train Accuracy: 0.9165111111111112 \n",
      "Validation Accuracy: 0.8650666666666667 \n",
      "Loss: 0.2206572542372118 \n",
      "\n",
      "Epoch: 11..\n",
      "train Accuracy: 0.9213111111111111 \n",
      "Validation Accuracy: 0.8674 \n",
      "Loss: 0.21051203688746092 \n",
      "\n",
      "Epoch: 12..\n",
      "train Accuracy: 0.9191333333333334 \n",
      "Validation Accuracy: 0.8696 \n",
      "Loss: 0.204476072413849 \n",
      "\n",
      "Epoch: 13..\n",
      "train Accuracy: 0.9215333333333333 \n",
      "Validation Accuracy: 0.864 \n",
      "Loss: 0.20521248421710955 \n",
      "\n",
      "Epoch: 14..\n",
      "train Accuracy: 0.9247333333333333 \n",
      "Validation Accuracy: 0.8632666666666666 \n",
      "Loss: 0.19669617646261617 \n",
      "\n",
      "Epoch: 15..\n",
      "train Accuracy: 0.9292222222222222 \n",
      "Validation Accuracy: 0.8655333333333334 \n",
      "Loss: 0.19711516951378705 \n",
      "\n",
      "Epoch: 16..\n",
      "train Accuracy: 0.9248888888888889 \n",
      "Validation Accuracy: 0.8656 \n",
      "Loss: 0.1902472679836751 \n",
      "\n",
      "Epoch: 17..\n",
      "train Accuracy: 0.9296444444444445 \n",
      "Validation Accuracy: 0.8637333333333334 \n",
      "Loss: 0.18654302205471016 \n",
      "\n",
      "Epoch: 18..\n",
      "train Accuracy: 0.9293777777777777 \n",
      "Validation Accuracy: 0.8616666666666667 \n",
      "Loss: 0.19030454515045833 \n",
      "\n",
      "Epoch: 19..\n",
      "train Accuracy: 0.9326888888888889 \n",
      "Validation Accuracy: 0.8645333333333334 \n",
      "Loss: 0.18397369024072405 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'logistic', 'softmax'], dropout=[0.1, 0, 0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.01,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'logistic', 'softmax'], dropout=[0, 0.1, 0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.01,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

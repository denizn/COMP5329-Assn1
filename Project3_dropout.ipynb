{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  (a * (1 - a ))\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        #return np.exp(x)/(np.sum(np.exp(x),axis=1)[:,None])\n",
    "        return (np.exp(x)/(np.sum(np.exp(x))))\n",
    "    \n",
    "    def __softmax_deriv(self, a):\n",
    "        #a = softmax(x)\n",
    "        return (a * (1 - a))\n",
    "    \n",
    "    def __ReLU(self,x):\n",
    "        return np.vectorize(lambda x:x if x>0 else 0)(x)\n",
    "    \n",
    "    def __ReLU_deriv(self,a):\n",
    "        #a = ReLU()\n",
    "        return np.vectorize(lambda x:1 if x>0 else 0)(a)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__ReLU\n",
    "            self.f_deriv = self.__ReLU_deriv\n",
    "            \n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', dropout=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout=dropout\n",
    "        self.dropout_vector = None\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, mode):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        if (mode=='train' and self.dropout>0):\n",
    "            self.dropout_vector = np.random.binomial(1, 1-self.dropout, size=input.shape)/(1-self.dropout)\n",
    "            lin_output = np.dot(self.dropout_vector*input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):\n",
    "        self.grad_W = (np.atleast_2d(self.dropout_vector*self.input if self.dropout>0 else self.input).T.dot(np.atleast_2d(delta)))\n",
    "        self.grad_b = delta\n",
    "        \n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], dropout=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.mode = 'train'\n",
    "        self.activation=activation\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],self.dropout[i]))\n",
    "            \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input=input, mode=self.mode)\n",
    "            input=output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=error**2\n",
    "        # calculate the delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)    \n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    def criterion_CELoss(self,y,y_hat):\n",
    "        error = y*np.log(y_hat)\n",
    "        loss = -np.sum(error)\n",
    "        delta = (y_hat-y)\n",
    "        return loss,delta\n",
    "        \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=10):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            loss=np.zeros(X.shape[0])\n",
    "            for it in range(X.shape[0]):\n",
    "                i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                # forward pass\n",
    "                y_hat = self.forward(X[i])\n",
    "                \n",
    "                # backward pass\n",
    "                if self.activation[-1] == 'softmax':\n",
    "                    loss[it],delta=self.criterion_CELoss(y[i],y_hat)\n",
    "                else:\n",
    "                    loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
    "                \n",
    "                self.backward(delta)\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "            to_return[k] = np.mean(loss)\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.test()\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = self.forward(x[i,:])\n",
    "        return output\n",
    "    \n",
    "    def optimize(self, X, y, learning_rate=0.01, test_size=0.25, epochs=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        losses = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            loss=np.zeros(X_train.shape[0])         \n",
    "            \n",
    "            self.test()\n",
    "            yhat_train = self.forward(X_train)\n",
    "            yhat_val = self.forward(X_val)\n",
    "            \n",
    "            # Calculate train and Test Accuracy\n",
    "            accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "            accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "            \n",
    "            self.train()\n",
    "            for it in range(X_train.shape[0]):\n",
    "                i=np.random.randint(X_train.shape[0])\n",
    "                \n",
    "                \n",
    "                # forward pass\n",
    "                y_hat = self.forward(X_train[i])\n",
    "\n",
    "                # backward pass\n",
    "                if self.activation[-1] == 'softmax':\n",
    "                    loss[it],delta = self.criterion_CELoss(y_train[i],y_hat)\n",
    "                else:\n",
    "                    loss[it],delta = self.criterion_MSE(y_train[i],y_hat)\n",
    "                \n",
    "                self.backward(delta)\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "                \n",
    "            self.test()\n",
    "            yhat_train = self.forward(X_train)\n",
    "            yhat_val = self.forward(X_val)\n",
    "            accuracies_val.append(accuracy_train)\n",
    "            accuracies_test.append(accuracy_val)\n",
    "            \n",
    "            if verbose:\n",
    "                print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                      format(e, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "            \n",
    "            losses[e] = np.mean(loss)\n",
    "        return losses, accuracies_val, accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.10715555555555556 \n",
      "Validation Accuracy: 0.10946666666666667 \n",
      "Loss: 0.516737502994927 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.8626444444444444 \n",
      "Validation Accuracy: 0.8356666666666667 \n",
      "Loss: 0.36576005756277613 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.8643555555555555 \n",
      "Validation Accuracy: 0.8460666666666666 \n",
      "Loss: 0.3395176883995654 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.8826888888888889 \n",
      "Validation Accuracy: 0.855 \n",
      "Loss: 0.321511635780709 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.8842444444444445 \n",
      "Validation Accuracy: 0.8560666666666666 \n",
      "Loss: 0.3025721833415451 \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.8891555555555556 \n",
      "Validation Accuracy: 0.8573333333333333 \n",
      "Loss: 0.29201617236097993 \n",
      "\n",
      "Epoch: 6..\n",
      "train Accuracy: 0.891 \n",
      "Validation Accuracy: 0.8511333333333333 \n",
      "Loss: 0.287170439842203 \n",
      "\n",
      "Epoch: 7..\n",
      "train Accuracy: 0.8973111111111111 \n",
      "Validation Accuracy: 0.8583333333333333 \n",
      "Loss: 0.28100387084776446 \n",
      "\n",
      "Epoch: 8..\n",
      "train Accuracy: 0.8994 \n",
      "Validation Accuracy: 0.858 \n",
      "Loss: 0.2780226923084962 \n",
      "\n",
      "Epoch: 9..\n",
      "train Accuracy: 0.9040888888888889 \n",
      "Validation Accuracy: 0.8589333333333333 \n",
      "Loss: 0.2706324064663193 \n",
      "\n",
      "Epoch: 10..\n",
      "train Accuracy: 0.9046666666666666 \n",
      "Validation Accuracy: 0.8572 \n",
      "Loss: 0.26302542158226677 \n",
      "\n",
      "Epoch: 11..\n",
      "train Accuracy: 0.9079555555555555 \n",
      "Validation Accuracy: 0.8601333333333333 \n",
      "Loss: 0.2727913013948273 \n",
      "\n",
      "Epoch: 12..\n",
      "train Accuracy: 0.9055555555555556 \n",
      "Validation Accuracy: 0.86 \n",
      "Loss: 0.2615350372240783 \n",
      "\n",
      "Epoch: 13..\n",
      "train Accuracy: 0.9025777777777778 \n",
      "Validation Accuracy: 0.8542 \n",
      "Loss: 0.25764373431163395 \n",
      "\n",
      "Epoch: 14..\n",
      "train Accuracy: 0.9105333333333333 \n",
      "Validation Accuracy: 0.8560666666666666 \n",
      "Loss: 0.24785407434267234 \n",
      "\n",
      "Epoch: 15..\n",
      "train Accuracy: 0.9094444444444445 \n",
      "Validation Accuracy: 0.8544 \n",
      "Loss: 0.25177916005622647 \n",
      "\n",
      "Epoch: 16..\n",
      "train Accuracy: 0.9136666666666666 \n",
      "Validation Accuracy: 0.8624666666666667 \n",
      "Loss: 0.2512537945328198 \n",
      "\n",
      "Epoch: 17..\n",
      "train Accuracy: 0.9079555555555555 \n",
      "Validation Accuracy: 0.8546 \n",
      "Loss: 0.2585579774134654 \n",
      "\n",
      "Epoch: 18..\n",
      "train Accuracy: 0.9094222222222222 \n",
      "Validation Accuracy: 0.8585333333333334 \n",
      "Loss: 0.24502815532391595 \n",
      "\n",
      "Epoch: 19..\n",
      "train Accuracy: 0.9131555555555556 \n",
      "Validation Accuracy: 0.8564 \n",
      "Loss: 0.24426372813372768 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0, 0, 0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.01,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.09317777777777778 \n",
      "Validation Accuracy: 0.09433333333333334 \n",
      "Loss: 1.412792924963113 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.2, 0.2, 0.2, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.03,epochs=3)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras import optimizers, metrics, Sequential\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "y = label\n",
    "X = data\n",
    "\n",
    "y_dummies = np.array(pd.get_dummies(y))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=0.25, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "config = tensorflow.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \n",
    "sess = tensorflow.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1179522318103186630\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6671596257\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13570813033383727892\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "45000/45000 [==============================] - 90s 2ms/step - loss: 0.7122 - acc: 0.7556 - categorical_accuracy: 0.7556\n",
      "Epoch 2/3\n",
      "45000/45000 [==============================] - 90s 2ms/step - loss: 0.5508 - acc: 0.8085 - categorical_accuracy: 0.8085\n",
      "Epoch 3/3\n",
      "45000/45000 [==============================] - 91s 2ms/step - loss: 0.5362 - acc: 0.8150 - categorical_accuracy: 0.8150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2723e075a90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = optimizers.SGD(lr=0.03)\n",
    "model = Sequential()\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "model.fit(X_train, y_train, batch_size=1, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_val = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8424"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

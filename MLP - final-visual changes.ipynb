{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "import copy\n",
    "import time\n",
    "%pdb on\n",
    "\n",
    "class StandardScaler(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.std = np.std(X - self.mean, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X - self.mean) / self.std\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  (a * (1 - a ))\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        y = np.atleast_2d(x)\n",
    "        axis = -1\n",
    "        y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "        y = np.exp(y)\n",
    "        summ = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "        out = y / summ\n",
    "        if len(x.shape) == 1: out = out.flatten()    \n",
    "        return out\n",
    "    \n",
    "    def __softmax_deriv(self, a):\n",
    "        \"applies softmax derivative over the given array\"\n",
    "        return a * (1 - a)\n",
    "    \n",
    "    def __ReLU(self,x):\n",
    "        \"\"\"applies relu activation\"\"\"\n",
    "        return x * (x > 0)\n",
    "    \n",
    "    def __ReLU_deriv(self,a):\n",
    "        \"\"\"returns derivative of relu activation\"\"\"\n",
    "        return 1 * (a > 0)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__softmax_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__ReLU\n",
    "            self.f_deriv = self.__ReLU_deriv\n",
    "            \n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', dropout=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout=dropout\n",
    "        self.dropout_vector = None\n",
    "        self.gamma = np.ones((1,n_out))\n",
    "        self.beta = np.ones((1,n_out))\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        self.vel_W = np.zeros(self.W.shape)\n",
    "        self.vel_b = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, mode):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        if mode=='train':\n",
    "  \n",
    "            lin_output = np.dot(input, self.W) + self.b\n",
    "                \n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "                )\n",
    "                \n",
    "            if self.dropout:\n",
    "                self.dropout_vector = np.random.binomial(1, 1-self.dropout, size=self.output.shape[-1])/(1-self.dropout)\n",
    "                self.output = self.output * self.dropout_vector\n",
    "            \n",
    "        else:                \n",
    "            lin_output = np.dot(input, self.W) + self.b\n",
    "            \n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "                )\n",
    "                \n",
    "        self.input=input\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):\n",
    "        \n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "        \n",
    "        if self.dropout: self.dropout_vector*self.grad_W\n",
    "            \n",
    "        self.grad_b = np.sum(delta,axis=0)\n",
    "        \n",
    "        if self.activation_deriv: delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "            \n",
    "        return delta\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], dropout=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.mode = 'train'\n",
    "        self.activation=activation\n",
    "        self.dropout=dropout\n",
    "        self.batch_size = 1\n",
    "        self.weight_decay = 0\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],self.dropout[i]))\n",
    "            \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        sets network mode to train, enables dropout\n",
    "        \"\"\"\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        sets network mode to train, disables dropout\n",
    "        \"\"\"\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        main forward step that triggers consecutive forward steps in each layer\n",
    "        :param input: array of inputs\n",
    "        :returns output: resulting output from all forward passes\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input=input, mode=self.mode)\n",
    "            input=output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        \"\"\"\n",
    "        Criterion that uses Cross Entropy Loss Function \n",
    "        on actual and predicted labels and returns loss and delta\n",
    "        :param y: actual target labels\n",
    "        :param y_hat: predicted target labels\n",
    "        \"\"\"\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=error**2\n",
    "        # calculate the delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)\n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    def criterion_CELoss(self,y,y_hat):\n",
    "        \"\"\"\n",
    "        Criterion that uses Cross Entropy Loss Function \n",
    "        on actual and predicted labels and returns loss and delta\n",
    "        :param y: actual target labels\n",
    "        :param y_hat: predicted target labels\n",
    "        \"\"\"\n",
    "        error = y * np.log(y_hat)\n",
    "        loss = -np.sum(error)\n",
    "        delta = y_hat-y\n",
    "        return loss,delta\n",
    "        \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        \"\"\"\n",
    "        Update step that uses layer gradients and learning rate\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if self.momentum!=0:\n",
    "                layer.vel_W = layer.vel_W * self.momentum + layer.grad_W * self.lr\n",
    "                layer.vel_b = layer.vel_b * self.momentum + layer.grad_b * self.lr\n",
    "                \n",
    "                layer.W -= (layer.vel_W + layer.W * self.weight_decay)\n",
    "                layer.b -= (layer.vel_b + layer.b * self.weight_decay)\n",
    "            else:\n",
    "                layer.W -= (lr * layer.grad_W + layer.W * self.weight_decay)\n",
    "                layer.b -= (lr * layer.grad_b + layer.b * self.weight_decay)\n",
    "            \n",
    "    def get_batches(self,X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Shuffles and splits inputs X,y into batches and returns a list of batches\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param batch_size: Requested size for batches to be returned\n",
    "        \n",
    "        \"\"\"\n",
    "        batches = []\n",
    "\n",
    "        X, y = self.shuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_batch = X[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "            \n",
    "            batches.append((X_batch, y_batch))\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=10, batch_size=1, momentum=0, weight_decay=0):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.batch_size=batch_size\n",
    "        self.momentum = momentum\n",
    "        self.lr = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        epoch_av_loss = np.zeros(epochs)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "        # Differentiate Stochastic Gradient Descent vs Batch Gradient Descent\n",
    "        if batch_size>1:\n",
    "            batches = self.get_batches(X, y_dummies, batch_size)\n",
    "            for k in range(epochs):\n",
    "                losses = []\n",
    "                for X_batch,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X_batch)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        loss,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                    \n",
    "                    losses.append(loss)\n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                epoch_av_loss[k] = np.sum(losses)/X.shape[0]\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss=np.zeros(X.shape[0])\n",
    "                for it in range(X.shape[0]):\n",
    "                    i=np.random.randint(X.shape[0])\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X[i])\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y_dummies[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y_dummies[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                epoch_av_loss[k] = np.sum(loss)/X.shape[0]\n",
    "        return epoch_av_loss\n",
    "    \n",
    "    def shuffle(self, x,y):\n",
    "        \"\"\"\n",
    "        shuffles given input and target variables of same first axis shape\n",
    "        :returns x,y: shuffled input and target\n",
    "        \"\"\"\n",
    "        idxs = [idx for idx in range(x.shape[0])]\n",
    "        np.random.shuffle(idxs)\n",
    "        return x[idxs], y[idxs]\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        predict target variables based on inputs\n",
    "        :returns yhat: an array of predictions\n",
    "        \"\"\"\n",
    "        self.test()\n",
    "        x = np.array(x)\n",
    "        yhat = self.forward(x)\n",
    "        if self.activation[-1]=='softmax':\n",
    "            yhat = np.argmax(yhat,axis=1)\n",
    "        return yhat\n",
    "    \n",
    "    def model_checkpointer(self, X, y, learning_rate=0.001, test_size=0.25, batch_size=1,epochs=10, momentum=0, weight_decay=0, verbose=True):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.best_accuracy = 0\n",
    "        self.best_model = None\n",
    "        self.batch_size=batch_size\n",
    "        self.momentum = momentum\n",
    "        self.lr = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.train()\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        epoch_av_loss = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        if batch_size>1:\n",
    "            batches = self.get_batches(X_train, y_train, batch_size)\n",
    "            for k in range(epochs):\n",
    "                losses = []\n",
    "                \n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                \n",
    "                self.train()\n",
    "                for X_batch,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X_batch)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        loss,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                    \n",
    "                    losses.append(loss)                      \n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                losses[k] = np.sum(loss)/X_train.shape[0]\n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "                if verbose:\n",
    "                    print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                          format(k, accuracy_train, accuracy_val, losses[k]))\n",
    "                    \n",
    "                if accuracy_val > self.best_accuracy:\n",
    "                    self.best_accuracy = accuracy_val\n",
    "                    self.best_model = copy.deepcopy(self)\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss = np.zeros(X_train.shape[0])\n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                self.train()\n",
    "                for it in range(X_train.shape[0]):\n",
    "                    i=np.random.randint(X_train.shape[0])\n",
    "                \n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X_train[i])\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y_train[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y_train[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                \n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "                \n",
    "                if accuracy_val > self.best_accuracy:\n",
    "                    self.best_accuracy = accuracy_val\n",
    "                    self.best_model = copy.deepcopy(self)\n",
    "                \n",
    "                epoch_av_loss[k] = np.sum(loss)/X_train.shape[0]\n",
    "\n",
    "                if verbose:\n",
    "                    print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                          format(k, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "                    \n",
    "        return epoch_av_loss, accuracies_val, accuracies_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIT REQUESTED PARAMETERS ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for these settings : 0.8302916666666667\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('input/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('input/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "X=np.array(data)\n",
    "y=np.array(label)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.8, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "mlp = MLP([128,128,128,128,10],activation=[None,'logistic','logistic','logistic','softmax'], dropout=[0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "mlp.fit(X_train, y_train, learning_rate=0.001, batch_size=100, momentum=0.9, weight_decay=0.000, epochs=10)\n",
    "predictions = mlp.predict(X_val)\n",
    "#predictions.to_csv('output/Predicted_labels.h5')\n",
    "print('Accuracy for these settings : {}'.format((predictions==y_val).mean()))\n",
    "\n",
    "with h5py.File('output/Predicted_labels.h5','w') as hdf:\n",
    "    hdf.create_dataset('labels', data = predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and validation sets, use cross validation to optimize parameters, then refit to whole data, predict test data and save into .h5 file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:238: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:238: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.4071111111111111 \n",
      "Validation Accuracy: 0.406 \n",
      "Loss: 0.0019452359118791805 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.5537777777777778 \n",
      "Validation Accuracy: 0.5316666666666666 \n",
      "Loss: 0.0016308559678410472 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.7143333333333334 \n",
      "Validation Accuracy: 0.696 \n",
      "Loss: 0.001383900740397021 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.7558888888888889 \n",
      "Validation Accuracy: 0.7326666666666667 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.7953333333333333 \n",
      "Validation Accuracy: 0.765 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.8156666666666667 \n",
      "Validation Accuracy: 0.7883333333333333 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 6..\n",
      "train Accuracy: 0.8235555555555556 \n",
      "Validation Accuracy: 0.7926666666666666 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 7..\n",
      "train Accuracy: 0.8447777777777777 \n",
      "Validation Accuracy: 0.806 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 8..\n",
      "train Accuracy: 0.8407777777777777 \n",
      "Validation Accuracy: 0.7943333333333333 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 9..\n",
      "train Accuracy: 0.8573333333333333 \n",
      "Validation Accuracy: 0.8003333333333333 \n",
      "Loss: 0.0007137168334763926 \n",
      "\n",
      "Epoch: 10..\n",
      "train Accuracy: 0.852 \n",
      "Validation Accuracy: 0.795 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 11..\n",
      "train Accuracy: 0.869 \n",
      "Validation Accuracy: 0.8166666666666667 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 12..\n",
      "train Accuracy: 0.8701111111111111 \n",
      "Validation Accuracy: 0.8143333333333334 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 13..\n",
      "train Accuracy: 0.8761111111111111 \n",
      "Validation Accuracy: 0.8183333333333334 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 14..\n",
      "train Accuracy: 0.8778888888888889 \n",
      "Validation Accuracy: 0.817 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 15..\n",
      "train Accuracy: 0.8818888888888889 \n",
      "Validation Accuracy: 0.82 \n",
      "Loss: 0.0007311996392413103 \n",
      "\n",
      "Epoch: 16..\n",
      "train Accuracy: 0.8747777777777778 \n",
      "Validation Accuracy: 0.8033333333333333 \n",
      "Loss: 0.0011012395160567426 \n",
      "\n",
      "Epoch: 17..\n",
      "train Accuracy: 0.8841111111111111 \n",
      "Validation Accuracy: 0.821 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 18..\n",
      "train Accuracy: 0.8855555555555555 \n",
      "Validation Accuracy: 0.8106666666666666 \n",
      "Loss: 0.0012848364649187848 \n",
      "\n",
      "Epoch: 19..\n",
      "train Accuracy: 0.8946666666666667 \n",
      "Validation Accuracy: 0.8213333333333334 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 20..\n",
      "train Accuracy: 0.8938888888888888 \n",
      "Validation Accuracy: 0.827 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 21..\n",
      "train Accuracy: 0.8966666666666666 \n",
      "Validation Accuracy: 0.8256666666666667 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 22..\n",
      "train Accuracy: 0.8881111111111111 \n",
      "Validation Accuracy: 0.8113333333333334 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 23..\n",
      "train Accuracy: 0.8927777777777778 \n",
      "Validation Accuracy: 0.825 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 24..\n",
      "train Accuracy: 0.9013333333333333 \n",
      "Validation Accuracy: 0.8306666666666667 \n",
      "Loss: 0.0016345974230713711 \n",
      "\n",
      "Time taken to train and predict: 3.57 seconds\n",
      "Best accuracy achieved: 0.831 accuracy\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8nGW9///XlX1t9rRp0jSh+76XlrUgSFFZFMSioBwV3HDBI0c8x58ix/NVj+e4HRVFRUUFxIpsVkC0gEhLF1qaNm3SdMvaZt/Xmbl+f9zTNk2ztknuycz7+XjMY2buuTPzyd3p/c593dd9XcZai4iISKAJc7sAERGR/iigREQkICmgREQkICmgREQkICmgREQkICmgREQkICmgREQkICmgREQkICmgREQkIEW49cHp6ek2Ly/PrY8XERGX7Ny5s9ZamzHUeq4FVF5eHjt27HDr40VExCXGmGPDWU9NfCIiEpAUUCIiEpAUUCIiEpBcOwfVn56eHsrLy+ns7HS7lDEVExNDTk4OkZGRbpciIhKwAiqgysvLSUxMJC8vD2OM2+WMCWstdXV1lJeXk5+f73Y5IiIBK6Ca+Do7O0lLSwvacAIwxpCWlhb0R4kiIudrWAFljFlvjCkyxpQYY+7r5/Xpxpi/GWP2GGNeNsbknGtBwRxOJ4XC7ygicr6GDChjTDjwI+BaYD5wqzFmfp/V/gd4xFq7GHgA+MZoFyoiIqFlOEdQq4ESa+1ha2038DhwQ5915gN/8z/e3M/rE0JjYyM//vGPR/xz73jHO2hsbByDikRE3OXx+niztIH/+9tB/uWX2/D67Lh99nA6SWQDZb2elwMX9lnnLeAm4PvAu4FEY0yatbZuVKocJycD6pOf/OQZy71eL+Hh4QP+3KZNm8a6NBGRcWGt5VBNG/8sqeW1klq2HqqjpcsDwIKpk6hp6WJKUsy41DKcgOrvhEnfCP0C8ENjzB3Aq0AF4DnrjYy5C7gLIDc3d9AP/dqz+yisbB5GecM3f+okvnrdggFfv++++zh06BBLly4lMjKShIQEsrKy2L17N4WFhdx4442UlZXR2dnJZz/7We666y7g9LBNra2tXHvttVxyySW8/vrrZGdn8/TTTxMbGzuqv4eITCydPV5qW7uoa+2mrq2L2tZu6lq7aWjv5oL0eC6ZlU5OSpxr9VW3dPJ6SR2vldTyz5JaqpqcTlw5KbG8a0kWF89M56IZ6aTGR41rXcMJqHJgWq/nOUBl7xWstZXAewCMMQnATdbapr5vZK19CHgIYOXKleN3nDhM3/zmN9m7dy+7d+/m5Zdf5p3vfCd79+491R384YcfJjU1lY6ODlatWsVNN91EWlraGe9x8OBBHnvsMX72s59xyy238Mc//pHbbrvNjV9HRIahuqWTV4tr2Xq4jh6vj8jwMCLDw4gKN859RFiv+z7L/Ot6fD7q27r7BFAXdW3d1LZ00dbt7fezI8MNPV5nV5iXFsfFM9O5ZGY6a2ekkRw3dmHQ2uVh25E6XjtYxz9Laik60QJAclwkF89IP1VHbpp7oQnDC6jtwCxjTD7OkdEG4P29VzDGpAP11lof8CXg4fMtbLAjnfGyevXqM65V+sEPfsCf/vQnAMrKyjh48OBZAZWfn8/SpUsBWLFiBUePHh23ekVkaD1eH7tKG3m5qJpXimvY52+pSY2PYlJMBD1eS5fHR4+39214f0+HhxlS46NIi48iPSGaaalxpCdEk5YQRXq8c5+WEE1afBRpCVHERoZTUt166sjl6d2V/O6NUoyBRdlJp4JixfQUYiIHPs0wkObOHo7WtnGkto2jte0crWvjcE0r+yqb8fgs0RFhrM5P5d3Ls7lkZjrzsyYRFhY4vYyHDChrrccYczfwAhAOPGyt3WeMeQDYYa19BlgHfMMYY3Ga+D41hjWPm/j4+FOPX375ZV566SW2bNlCXFwc69at6/dapujo6FOPw8PD6ejoGJdaRWRgVU0dvFJUwyvFNbx2sJaWLg/hYYYVuSnce80cLp+dMejO2VpLj9fS7fXR4w+vbn9w9Xh9hBlIi48mKTZyxDv4WZMTmTU5kX+5OJ8er4895Y2njmx+9uphHnz5ENERYazKSz0VWPOnTiLc/zltXR6O1p0OICeM2jha10Zta/cZnzU1KYa89HjuuuwCLpmZzvJzDL7xMqyRJKy1m4BNfZZ9pdfjjcDG0S1t/CUmJtLS0tLva01NTaSkpBAXF8eBAwfYunXrOFcnIsPV5fGy82gDrxTX8HJRzakmrKykGN65OIt1czK4aGY6k2KGN9yYMYaoCENURBhED73+uYoMD2PF9FRWTE/ls1fNoq3Lw7Yj9aeOsL71/AG+hdMUNyMjgbL6dqpbus54j8mToslLi+eqeZPJS48nLy2e/PR4pqfFBXQY9SeghjpyW1paGhdffDELFy4kNjaWyZMnn3pt/fr1/OQnP2Hx4sXMmTOHNWvWuFipSGiw1mlua+/20tbloaPHf9/tpa3bS3u3h/Zur3Pr8tDe4+XgiVZeP1RLe7eXyHDD6vxUbloxl8tnZzJ7csKEulA+PjqCK+ZmcsXcTMA5X7blUB2vHazlWH07l83OIL9XCOWlxxEXFTy7dWOtO30VVq5caftOWLh//37mzZvnSj3jLZR+V5HhaO7s4bWDtWw+UM2Ww3U0tvfQ3u1hJJfdRIWHMSUphstmp7NudiZrZ6QRHx08O+xgYYzZaa1dOdR6+pcTETp7vJQ3dFDe0O6/76Ci0Xle29rF7MxEluUmszw3hcXTkkkYhZ2+tZbiE61sLqpm84Fqdh5rwOOzJMVGcsnMdCZPiiE+OpzYqHDioyJO3cdFhftvEcRF93ocFU5keEANLyrnSQElEuQ8Xh+tXR5qWrpOh1Bjx+kgamg/62R6ZLhhanIsOSmxLM5Jpuh4C387UA1AmIHZkxNZlpvC8txkluWmcEF6/LA6B7R3e3i9pI7NRdW8XFRDRaPTiWhe1iTuuuwCrpybydJpyUQoaAQFlMiEYK2lvKGDkupWmjt7aOn00NrlodV/39zZc+rxyeUt/vuOnrOvwYkKDyM7xQmg+fMnk50cS05KHDkpzn1GYvSpXmInNbX3sLu8kTePNbCrrJHn9lTy2LZSAJJiI1k6zTnCWj49mSXTkk91QDha2+YcJRXVsPVwHd0eH/FR4VwyK51PXzmTdXMyx21kAplYFFAiAaazx0tJdSuFlc0UVjm3/VXNtHSeNTgLYQYSoiNIjIkkMSaChOgIUuOjyE2NO/U8MSaShOgI0hKiToVQRkL0iLtDJ8VFcvnsDC6fnQGAz2c5XNvKm8ca2VXWwJvHGvne34qxFoyBWZkJ9HgtR2rbAJiREc8H10znirmZrMxLITpiYvUok/GngBJxUX1bN4WVTgAVVjVTWNnMoZpWPP6eAXFR4cydksgNS6cyPyuJ2ZMTSImPIjE6goSYCGIjw13rlRYWZpiZmcjMzERuWeUMNtPS2cNbZU3sKm3gzdIGjDHccVEeV8zJdH1UApl4FFAi56ihrZvWLg9dHi+dPb5T9509/Tz3eOnq8Z26L61vp7CymePNpy/2njIphvlTJ3H1/MnMy5rE/KmTmJ4aF1BX9g8lMSaSS2alc8msdLdLkSCggDoPCQkJtLa2ul2GjLO9FU18+4UiXimuGfHPRkWEERMRRlZSLGtnpDHfH0TzsiaN+0CcIoFOASUyTIdrWvnfvxbz5z1VJMdF8tm3zSI7OZboyDBiIsOdW0QY0ZHhxESGERPhX+Z/PSo8bEIdDYm4LXAD6i/3wfGC0X3PKYvg2m8O+PIXv/hFpk+ffmo+qPvvvx9jDK+++ioNDQ309PTw9a9/nRtumJDzMco5qmrq4PsvHeQPO8uJjgjj01fO5M7LLhj2MDkicm4CN6BcsGHDBj73uc+dCqgnnniC559/nnvuuYdJkyZRW1vLmjVruP766yfUcClyburbuvnx5hIe2XoMLNy+ZjqfumImGYljOBibiJwSuAE1yJHOWFm2bBnV1dVUVlZSU1NDSkoKWVlZ3HPPPbz66quEhYVRUVHBiRMnmDJlyrjXJ+OjtcvDz/9xmJ//4wjt3R7eszyHz101y9UJ5URCUeAGlEtuvvlmNm7cyPHjx9mwYQO/+93vqKmpYefOnURGRpKXl9fvNBvijpbOHh5+7Sj/OFhDbmqcM3VBZgKzJicwLWVkPeA6e7z87o1SfrS5hPq2bq5ZMJkvvH0OsyYnjuFvICIDUUD1sWHDBu68805qa2t55ZVXeOKJJ8jMzCQyMpLNmzdz7Ngxt0sUnCFzHtlyjJ+8cojG9h4W5yTx+qE6ntxVcWqdmMgwZmQk+APLCa7ZkxOZlhp3xigJHq+PJ9+s4HsvFVPZ1MnFM9O495q5LJ2W7MavJiJ+Cqg+FixYQEtLC9nZ2WRlZfGBD3yA6667jpUrV7J06VLmzp3rdokhrbPHy6NvlPLjl0uobe1m3ZwMPn/1bBbnOGHS3NnDwROtlFS3cPBEK8XVrWw7Us9TuytPvUdUhBNcsycnMD01jj8XVHGopo0lOUl8+71LuHimruERCQQKqH4UFJzuPZiens6WLVv6XU/XQI2fbo+PJ3aU8cO/l3C8uZOLZqTx09tns2J66hnrTYqJZMX0FFZMTzljeUtnDyXVrRysbuXgiRYOVrey42gDT++uZFZmAj+5bQXXLJiszi8iAUQBJQHN4/Xx5K4KfvC3g5Q3dLByegrfed8SLpoxsqOcxJhIluWmsCz3zODq6PYSExmmYBIJQAooGVVen6WmpYu0hKjzmpvH67M8t6eS7710kCO1bSzOSeLrNy7k8tkZoxomsVEasFQkUAVcQFlrg/6vWbdmMR5rRcdbuHfjW+wpbyLMQFZS7KkpHXL6TOeQlRzTb4BZa3lh33G+89diik+0MndKIg/dvoKr56v5TSTUBFRAxcTEUFdXR1paWtDujKy11NXVERMTPPPf9Hh9PPjyIf7v7weZFBPJfdfOpa3LQ4V/Qryth+qoau6kdy6HGZg8KeZUYOWkxJIaH8XGneXsq2xmRkY8P3z/Mt6xMEvDA4mEqIAKqJycHMrLy6mpGfkgnBNJTEwMOTk5bpcxKvZWNHHvxj3sr2rmuiVTuf+6+aQlnD3SQrfHx/Gmzj6zuTrTi287Us/TuzvwWchNjeM7tyzhhqXZZ02YJyKhJaACKjIykvz8fLfLkGHo8nj54d9LePDlQ6TER/HT21dwzYKBR9eIiggjNy1uwDmBerw+qlu6yEyMPq9zVyISPAIqoGRieKuskXs3vkXxiVbeszybr7xrPslx5zdVRGR4GNnJsaNUoYgEAwWUDFtnj5fvvlTMz149TGZiDL+8YxVXzM10uywRCVIKKBmWncfquXfjHg7XtLFh1TT+/Z3zNN2ESDCzFhpLoboQTuxz7muK4c6/Q8T4TK6pgJJBdXR7+fYLRfzy9SNMTYrlNx9ZzaWzMtwuSyRw9HRCey1ExkFkLETEwETrhdzRACcKT4fRiX1QvR+6W06vk5QLk+dDZxMkjM8+QAElA9p6uI4v/nEPx+rauX3NdL547VwSovWVkRDl80HDEf9OvBCq9zn39YfA+k6vZ8JOh1VknHOLihvgcTzMvAryLhm/UKstgYodp4+KThRCy+mxKolJhskLYMkGJ5AyF0DmPIiZND719aK9jZyhqaOH7UfqeWHfcf6ws5zc1Dgeu3MNa2ekuV2ayPhprTkdQCfvaw5AT7t/BQOp+ZA5Hxa+ByZNBU8XdLdBT4ez3qnHbdDd7jxuPe7cd7c7y7ta4Z/fgymLYe3dsODdY9N85vVA0SbY9hAc/YezLDwKMuZA/mWng2jyfEjMCpgjQAVUiKtv62bbkTreOFLPG4fr2X+8GWshKjyMD1+czxeumU1clL4mMo6626H4eWfnnzFn7HeWPh+Ub4MDf4aqt5yjirZe12LGpTs77uUfcu4nL4CMuc7Rz/nq6YA9v4ctP4Y/3QUvfRVW3wUr7oC41CF/fEhtdfDmr2D7w9Bc7jTTXfU1mHMtpM6A8MD+v23cGnZn5cqVdseOHa58diirbunkjcP1vHGkjm1H6ik+4YzIHhMZxvLcFC7MT+PCC1JZOi2ZmEiNUxcwfD5oPeH8BR4edXZzUdg5/Ft1t/vfs9q5b6s+/bj3fXcrLHk/XHLP2J578Hlh96Ow+b+gpcpZljYL5l3n3KYuG72w8vmgbCvsewr2P+N8XniUEz4njyQy/WGUMA49VX0+OPR32PJDOLzZ+Tdd+n5Y80lImzHy96vcBW88BHv/CN4uyL8cLvwYzF5/bt+VUWaM2WmtXTnkegqo4FbZ2MEbR+p443A9247Uc7i2DYD4qHBW5KVyYX4qay5IZVF2MlERukB22FqOQ+HTznmG2BSITXX+4o1NcW4RZ4+mMSif19lJNpY5PacaS6GptNfjcvB2D/zz4dGDnOfw33u6+gRPSz9vZCA+HRImOzvmhMng6XR+14hYWPNxuOjTzu84WqyFkr/BX7/iNKdlr4R190HjMSh8Bo6+BtYLSdNOh9W0C0e+o/V5oXQrFD7lvG/rcWe7zboa5t8Is69x5TzLWU7sc46oCp4Ab49ztLP2UzD94sED2tPt/Dtt+ymUb4fIeFh6K6y6EzIDax47BVSIK6tv58tP7eWVYqepYlJMBKvzU1mdn8qF+WksmDqJCI3YMHLWQsEfYNO90Nk48HqR8f7ASnbCKzalV4ClOucn+gaQz3Pme8RnQnKu/zbNuU/McoKqp+PM8x097f7zGv08PrlueBQkTnGCJz7zdAD1DqO4tP6bfWoPwsvfcP4ij05yQmrNxyE68fy2Z+VuJ5iOvAIp+XDV/TD/hjN3xO31UPQX2P+sc5Th7YL4DJj7Ties8i4b+LyNzwvHXndCaf+zTjhHxJwZSuf7O4yVlhOw/efOraMespacPk8V3usSj+Yq2PlL2PFL5yg49QKnmXDp+yEmyb36B6GAClE9Xh8Pv3aE775UTLgxfPKKmaybk8HcKZM0tt35aq2G5+6BA89Bzip41/ecv7g7GpydaEeDsyNpbzj9uO9rHQ2ne3wlTDkzfE7eknIhKcc58gk0x/fC5v8HRX92wuySe2DVR50jyZFoLIW/f905/xKb6hwxrfiXoTsIdLXAwRedsCl+0eloEJ0Ec9bDvOthxpVOEJe+7m++e9bZaUfEOqG04EaYdQ1EJ5z7NhhvPR3w1uOw9cdQWwyJU+HCu2Dqctj5K6eJ0ueBWW+H1R9ztkFYYP/xqYAKQW+VNXLfkwXsr2rm6vmTeeCGBWQlafigUbH3SfjzvzpHIlf+h/OX7Lm05ft80NXk7DAjJ/CI9uU7YfPXnSOahClw2RecTgRDBUxHA/zjO/DGT52jpDWfhEs+d25/6fd0Oudr9j/rdHDobDzdrHnyuqRZb/eH0ttHp1ODm3w+KHnJOU915BVnWXQSLLsNVn3k3M5VuUQBFUJauzz874tF/Pr1o2QkRvO16xeyfuHAA7fKCLTVwZ8/7zQRTV0ONz4YcO35rjr6T/j7f0LpFufIb90XYfGGs5sJPV2w7Wfw6redCz2Xvh+u+A9Iyh6dOrw9cOyfTlh1NjvNf7OunvihNJDjBVBT5HR6mEhHg34KqBDxUuEJvvL0XqqaO7ntwuncu36Oe0MQdbU652dqi50dRO5FAd/UMKj9zzpNeh2NThPUxZ8L+G65rrAWDv3NabKr3AVpM2Hdl2DBe5zX9z0Jf/ua06w3421w9QMwZaG7NYurRjWgjDHrge8D4cDPrbXf7PN6LvBrINm/zn3W2k2DvacC6vxUN3dy/7P72FRwnDmTE/l/71nEiumj2LNqRMUcgB2/cNrJu5ohLMJpE0+aBove61yRnjHHndrORXs9/OWLTi+qKYvh3T9xuhvL4Kx1mto2/5dzLVHmAqfJr3IXTFnkBNOMK92uUgLAqAWUMSYcKAauBsqB7cCt1trCXus8BOyy1j5ojJkPbLLW5g32vgqoc+PzWR7dVsq3/nKALq+Pz75tFndeesH4dxH39jidBbb/wrkyPTzK6V206qPOzvzAJtjzuHOOwvoga6kTVAtvGv3rSnw+51qd0egiXPQ8PPsZaK+Dy/4NLv38mT2mZGg+n3PU9PI3nR6HV/w7LLplYh9Ny6gabkANp71iNVBirT3sf+PHgRuAwl7rWODk3iEJqERGXfGJFr70ZAE7jzVw0Yw0/uvdi8hPH+c29qYKePPXsPPXznUkyblO1+BltzvXz5y0+L3OreWE0zV5z+Pw/H3wwn84f0Uv2QBz3jHynmo9Hc5f58cLTt9O7HMCKj7DucI/Y65znujk4951DaSjEZ7/Erz1qPOX/wf+4HTrlZELC4NFNzs3kfMwnCOom4H11tqP+p/fDlxorb271zpZwItAChAPXGWt3dnPe90F3AWQm5u74tixY6P1ewS1zh4vP9pcwk9eOURCdARffud83rM8GzNe42VZ6/Qa2v5z58jI+pwT0Ks+6gx0OdzebNUHnKDa8wdn2JWoRJh/PSy+BfIuPft9Wqvh+B6na/PJMKo7eLqbdlSicy5jyiLn+qC6Q854aTVFZ16EGpfeJ7TmQMY8J7iMgYMvwTOfdq6RufTzzpHTOE0nIBKKRrOJ773ANX0CarW19tO91vm8/73+1xizFvgFsNDa3kP8nklNfMNT3dLJrQ9t5VBNG+9Zns2X3zmf1Phx2nl2NMJbjznNeHUHnetVlt/uXK+Smn/u7+vzOT2u9jzuXNHf1exc27HoJjDh/qOivU5gnJQ0zQmiKYtgsj+Ukqf332xkLTRXnA6r6v3Ofc0B57NOik11jgCrdjvBdeODkL383H8vERmW0WziKwem9Xqew9lNeB8B1gNYa7cYY2KAdKB6eOVKf7o8Xj7+m51UNnbyyIdXc9nsMRoHzVrn+pT6I850Ag1HnB36/ufA0+FclPrunzpX3o/GtTthYZB/qXN7x/84owTs+T1sfRAwzpHOjLf1CqQFIxs40xjnQtekHOcIr/fv2VJ1ZnDVlcClX4DL7p3Y1yWJBKHhBNR2YJYxJh+oADYA7++zTinwNuBXxph5QAxQg5wzay3/31N7ebO0kR9/YPn5h5PXA01l0HDUH0JH/YF01Ln1PrIAZyicRTc7zXhTl57fZw8mMtaZrmDhe5xRAsKjx655zRhnWoRJU9WbTGQCGDKgrLUeY8zdwAs4XcgfttbuM8Y8AOyw1j4D/CvwM2PMPTgdJu6wbl1gFSR+9fpRnthRzmeunMk7FmWd25sUbIRdv3UCqbHMGXDzpPAop3krJd8ZeDM1H1LynOcp0925wDFQx0QTEVfoQt0A9NrBWj70y228bW4mP7ltBWEjHUOvsxk2fcFpNkubBVmLe4VPnhNGiVkBMey+iISe0TwHJePoaG0bn3r0TWZkxPOd9y0deThV7ISNH3GmKlj3784YaQoiEZmAFFABpKWzhzsf2YEx8PMPriIhegT/PD4fvP4DZ1y0xCy4YxNMXzt2xYqIjDEFVIDw+Sz3/H43h2vb+M2HV5ObNoILWFuOw58+BodfdubSue77ozuhnIiICxRQAeI7fy3mpf3V3H/dfC6aOYyRD04qfgGe+oQzQd11P4DlHxy9abFFRFykgAoAz+2p5IebS3jfyml86KK84f2Qpwv++lV440GYvAhu/sXEGpBVRGQICiiX7a1o4gt/eIsV01N44MYFwxu+qKbI6QhxogAu/IQzFp4uMhWRIKOAclFtaxd3PbKDlLgofnLbCqIjhuhtZy28+YgzFURUHLz/CZh9zfgUKyIyzhRQLun2+PjEb3dS19bNxo9fREZi9OA/0NEAz34WCp+GC9Y5Qw8latZcEQleCigXWGv56jP72H60ge9vWMqinKTBf6B0K/zxo844cld9DS76jObWEZGgp4BywW+3HuOxbaV8Yt0MbliaPfjK+56CjR92hiX68IuQs2J8ihQRcZkCapxtOVTH154t5Mq5mXzh7UP0ujsZTjmrnAn0RmPGWBGRCUIBNY7K6tv55O92Mj0tju9tWEr4YMMYFT7tD6eVcNtGDaQqIiFHJzLGSVuXhzsf2YHXZ/n5h1YxKSZy4JX3P+uEU/YK+IDCSURCk46gxoG1lns3vkXxiRZ+9S+ryU8fZCqL/c/CH+6Aqcvhtj+qWU9EQpaOoMbBL147wqaC4/zb+rmDTzy4/zl/OC1TOIlIyFNAjbFtR+r5xl8OcM2CyXzssgsGXvHAn+EPH4KspQonEREUUGOquqWTTz36JrmpcXz7vUsGHsbowCZ4wh9Otz8JMUNcFyUiEgJ0DmqM9Hh93P3oLlo6e/jNR1YP3Cmi6C/wxAedWW8VTiIipyigxsi3Xyhi25F6vve+pcydMkBzXdFf4Pe3w5RFcJvCSUSkNzXxjYG/FFTx0KuHuX3NdG5cNsBIEUXPnw6n2/8EscnjW6SISIBTQI2yQzWt3LtxD0umJfPld83rf6XiF+CJ22HKQoWTiMgAFFCjqL3bwyd+u5PIcMODH1je//QZxS/C72+DzPkKJxGRQegc1Cix1vKlJws4WN3KIx9ezdTk2LNXKn4Rfv8ByJwHH3wKYlPGv1ARkQlCR1Cj5Ddbj/H07kr+9erZXDqrn4txS15ywiljLtyucBIRGYoCahS8WdrAfz7njFD+yXUzz16h4k2nQ0T6HPjg0xCXOv5FiohMMAqo81TX2sWnfvcmU5Ji+O4tSwnrO0J5w1F49H0Ql+6MSq5wEhEZFp2DOg9en+Uzj++irq2bJz9xEUlxfS7Gba+H394M3i644zlN0S4iMgIKqPPwnb8W8c+SOv77psUszO5zkW1PJzz+fmg85pxzyhhickIRETmDAuocvVR4gh9tPsSGVdO4ZdW0M1/0+eCpj0PpFrjpF5B3sTtFiohMYDoHdQ5K69q554ndLMyexP3XLzh7hZe+Avv+BFc/AItuHv8CRUSCgAJqhDp7vHz8tzsJM4YHP7CCmMg+F+O+8RC8/n+w6k646DPuFCkiEgTUxDcC1lq+/NReCqua+eUdq5iWGnfmCvufg7/8G8x5B1z7LRhoeg0RERmSjqBGoOhECxt3lvPJdTNUV2F2AAAS3klEQVS4Ym7mmS+WbYc/fgSylzvnncL6GeZIRESGTQE1ArtLGwG4ZWWfThF1h+Cx9zndyG/9PUTF9fPTIiIyEgqoEdhT0URiTATT03oFUFst/O5msNaZ0ymhn2GORERkxHQOagT2VjSxKDvp9NTt3e3w2AZoroQPPQtpM9wtUEQkiAzrCMoYs94YU2SMKTHG3NfP6981xuz234qNMY2jX6q7uj0+DlS1sCjHf0GuzwtP3gnlO+Cmn8O01e4WKCISZIY8gjLGhAM/Aq4GyoHtxphnrLWFJ9ex1t7Ta/1PA8vGoFZXFZ9oodvrY1F2ktOc9/yX4MBzsP5bMO86t8sTEQk6wzmCWg2UWGsPW2u7gceBGwZZ/1bgsdEoLpAUVDQBOAG15Yew7aew9m5Y83GXKxMRCU7DCahsoKzX83L/srMYY6YD+cDfB3j9LmPMDmPMjpqampHW6qqCiiYmxUSQW/UivPhlmH8DXP2fbpclIhK0hhNQ/V1tagdYdwOw0Vrr7e9Fa+1D1tqV1tqVGRkTq7dbQXkTl2Z5MH/6GExbA+9+CMLUCVJEZKwMZw9bDvS+8CcHqBxg3Q0EYfNet8dH0fEWroo/4kydsf4bEBnjdlkiIkFtOAG1HZhljMk3xkThhNAzfVcyxswBUoAto1ui+051kAg7CmGRMLmfAWJFRGRUDRlQ1loPcDfwArAfeMJau88Y84Ax5vpeq94KPG6tHaj5b8LaU+50kMjuLIbMeRAR7XJFIiLBb1gX6lprNwGb+iz7Sp/n949eWYGloKKJpJgIYmr3OgPBiojImNNZ/mHYW9HE5VO6Me11kLXE7XJEREKCAmoIXR4vB443c3miv19I1lJ3CxIRCREKqCEUH2+lx2tZHH4UTJg6SIiIjBMF1BBOjiCR01kM6XM0lYaIyDhRQA2hoKKRpNhIp4PEVDXviYiMFwXUEAoqmrh0igfTelwdJERExpECahBdHi9Fx1u4fFKVs0ABJSIybhRQgyg63kKP17Ik/KizYMoiV+sREQklCqhBnNFBIm0mRCe6XJGISOhQQA1ib0UTyXGRxNbtVfOeiMg4U0ANYk95E2ungGkq1wW6IiLjTAE1gC6Pl+ITLaxTBwkREVcooAZwsoPE0ohjzoKsxe4WJCISYhRQAzg5xca0zmJIng6xKS5XJCISWhRQA1AHCRERdymgBlBQ0cSFWeGYhiMKKBERFyig+tHZ44wgsW7ScWeBxuATERl3Cqh+FB1vweOzLIv0d5CYoiMoEZHxpoDqxx7/CBK5XcUwKRsSMlyuSEQk9Cig+rG3vImUuEhi6/bp/JOIiEsUUP0oqGhi5dRoTO1BBZSIiEsUUH109jgjSFyRdBywCigREZcooPo44O8gsfRkBwmNwSci4goFVB8np9iY3l0C8ZmQOMXlikREQpMCqo+C8kZS46OIqy1wmveMcbskEZGQpIDqo6CimWVZMZiaIp1/EhFxkQKql84eLwdPtHBFcg1YrwJKRMRFCqhe9lc1nzmChAJKRMQ1Cqhe9vo7SOT1lDjTayTnulyRiEjoUkD1UlDRRFp8FHEnp9hQBwkREdcooHrZU97EkqlxmOpCNe+JiLhMAeXX2ePlYHUr61LqwNutgBIRcZkCym9/VTNen2V5lEaQEBEJBAoov5MjSOR3l0BUIqTku1yRiEhoU0D5FZT7O0jU+6fYCNOmERFxk/bCfgUVTSzJTsAc36vzTyIiAWBYAWWMWW+MKTLGlBhj7htgnVuMMYXGmH3GmEdHt8yxdbKDxGUpDeDpUECJiASAiKFWMMaEAz8CrgbKge3GmGestYW91pkFfAm42FrbYIzJHKuCx0Khv4PEiqgyZ4ECSkTEdcM5gloNlFhrD1tru4HHgRv6rHMn8CNrbQOAtbZ6dMscWydHkLjAUwKRcZA+y+WKRERkOAGVDZT1el7uX9bbbGC2Meafxpitxpj1/b2RMeYuY8wOY8yOmpqac6t4DOwpbyI9wT+CxJRFEBbudkkiIiFvOAHV33g/ts/zCGAWsA64Ffi5MSb5rB+y9iFr7Upr7cqMjIyR1jpm9lY0sXhqIuZ4gZr3REQCxHACqhyY1ut5DlDZzzpPW2t7rLVHgCKcwAp4Hd1OB4lL05qhu1UBJSISIIYTUNuBWcaYfGNMFLABeKbPOk8BVwAYY9JxmvwOj2ahY+V0B4lSZ4ECSkQkIAwZUNZaD3A38AKwH3jCWrvPGPOAMeZ6/2ovAHXGmEJgM3CvtbZurIoeTSc7SMzwHILwKMiY63JFIiICw+hmDmCt3QRs6rPsK70eW+Dz/tuEUlDRRHpCNHH1e2HyAgiPdLskERFBI0lQUN7EoqmJmKq3NECsiEgACemAcjpItHBxRjt0Nun8k4hIAAnpgCqsasZnYZU6SIiIBJyQDqiC8kYAZnhKICwCMue7XJGIiJwU2gFV0UxGYjTx9fsgYx5ExrhdkoiI+IV0QO2taGLR1En+DhJq3hMRCSQhG1Dt3R4OVrewJqML2mthqnrwiYgEkpANqP3+DhKrozXFhohIIArZgNpT7owgMdNbAibMuUhXREQCRsgGVEFFExmJ0STU74P02RAV73ZJIiLSS8gG1N6KJhZnJ4E6SIiIBKSQDKj2bg8l1a2szuiBlioFlIhIAArJgCqs9HeQiCl3FmgMPhGRgBOSAVXgn2JjlrfEWTBlkYvViIhIf0IzoMqbyDzZQSJ1BsRMcrskERHpIyQDand5I4tzkqFqj84/iYgEqJALqMb2bg7XtLF2KtBUqoASEQlQIRdQu8qcEczXxlY4CxRQIiIBKfQCqrSRMAMzvYecBQooEZGAFIIB1cDcKZOIqt4DybkQl+p2SSIi0o+QCiifz7K7rJFluckaQUJEJMCFVEAdqmmlpdPDqqwIqD+sgBIRCWAhFVC7Sp0OEqs0goSISMALrYAqayApNpKs9iJngY6gREQCVmgFVKlz/ims6i1InAoJmW6XJCIiAwiZgGrt8lB0ooVlOclQugVyVrpdkoiIDCJkAuqtskashbWpzdBUBvmXuV2SiIgMImQCaldpAwALu99yFiigREQCWggFVCMzMxOIq3gdEiY707yLiEjAComAstayq6yR5dOS4MirztGTMW6XJSIigwiJgDpW1059WzeXpzVCWzXkXep2SSIiMoSQCKhdZc75pxW+Pc4CnX8SEQl4oRFQpY3ER4UzuXYbJOVCSp7bJYmIyBBCJqCW5kzCHHsN8i/V+ScRkQkg6AOqo9vL/qpm3p5eBx0Nat4TEZkggj6gCiqa8Pgsa8P2OQvUQUJEZEIYVkAZY9YbY4qMMSXGmPv6ef0OY0yNMWa3//bR0S/13Jy8QDeveSekzoCkbJcrEhGR4YgYagVjTDjwI+BqoBzYbox5xlpb2GfV31tr7x6DGs/LrtJGLkiNJqp8Cyy62e1yRERkmIZzBLUaKLHWHrbWdgOPAzeMbVmjw1rLm6UNvCujGrpbnA4SIiIyIQwnoLKBsl7Py/3L+rrJGLPHGLPRGDOtvzcyxtxljNlhjNlRU1NzDuWOTGVTJ9UtXayL2u8s0PknEZEJYzgB1V+fbNvn+bNAnrV2MfAS8Ov+3sha+5C1dqW1dmVGRsbIKj0HJ88/zWrfDRnzNP+TiMgEMpyAKgd6HxHlAJW9V7DW1llru/xPfwasGJ3yzs+u0kYSInwknNiu7uUiIhPMcAJqOzDLGJNvjIkCNgDP9F7BGJPV6+n1wP7RK/Hc7Spt4N2ZVRhPhwJKRGSCGbIXn7XWY4y5G3gBCAcettbuM8Y8AOyw1j4DfMYYcz3gAeqBO8aw5mHp8njZW9nM56YXAwbyLna7JBERGYEhAwrAWrsJ2NRn2Vd6Pf4S8KXRLe38FFY20+3xsbBrN2QthtgUt0sSEZERCNqRJHaVNhJNNykNb6n3nojIBBS8AVXWyNsTj2K83ZB/udvliIjICAVvQJU28M6Eg2DCYfpat8sREZERCsqAqm7ppLyhg2W+AsheDtGJbpckIiIjFJQBtau0kXg6yGzep+7lIiITVNAG1JqIYoz1qoOEiMgEFaQB1cC7EksgPAqmXeh2OSIicg6CLqA8Xh97ypu4kL2Qsxqi4twuSUREzkHQBdSB4y1E9jSR1VGs6TVERCawoAuoXWWNrAnbj8Gqg4SIyAQWfAFV2sAV0UXYiFjIXul2OSIico6CLqB2lzZyacR+TO4aiIhyuxwRETlHQRVQDW3dNNVWktNzRM17IiITXFAF1G7/+SdAASUiMsEFVUDtKm3g4vB92KgEyFrqdjkiInIegiugyhq5LPIAJu8SCB/WVFciIhKggiagfD5LVekhcnwVGt5IRCQIBE1AldS0sqhnj/NE559ERCa8oAmoXaUNrA0rxBudDJMXul2OiIicp+AJqGMNXBKxj7ALLoWwoPm1RERCVtD0JKg6VsRUajW9u4hIkAiKQ43mzh6m1G9znqiDhIhIUAiKgNpT1sTasH10x6RDxhy3yxERkVEQFAG161g9a8MKMfmXgjFulyMiIqMgKM5BVR0uYLJphJnr3C5FRERGyYQ/grLWklD1uvNE1z+JiASNCR9QR+vaWeLZQ1vMFEjJd7scEREZJRM+oHYdq2NtWCGeXJ1/EhEJJhP+HFRl8ZukmlZ88650uxQRERlFE/4IKqrsNQDCLtD5JxGRYDKhA6q928MFrTtpiJkGSTlulyMiIqNoQgdUQWkdq81+2qeudbsUEREZZRM6oMr3v8Ek00HS/KvcLkVEREbZhA4ojrwKQMLcK1wuRERERtuEDShrLVMbtnM8Kg8SMt0uR0RERtmEDaiKuiaW+PbTOGWN26WIiMgYGFZAGWPWG2OKjDElxpj7BlnvZmOMNcasHL0S+9d6eBtxpovYWWreExEJRkNeqGuMCQd+BFwNlAPbjTHPWGsL+6yXCHwGeGMsCu1r7vxlEP59cuddMx4fJyIi42w4R1CrgRJr7WFrbTfwOHBDP+v9J/DfQOco1jewhAxYcQcmLmVcPk5ERMbXcAIqGyjr9bzcv+wUY8wyYJq19rnB3sgYc5cxZocxZkdNTc2IixURkdAxnIDqbwRWe+pFY8KA7wL/OtQbWWsfstautNauzMjIGH6VIiIScoYTUOXAtF7Pc4DKXs8TgYXAy8aYo8Aa4Jnx6CghIiLBazgBtR2YZYzJN8ZEARuAZ06+aK1tstamW2vzrLV5wFbgemvtjjGpWEREQsKQAWWt9QB3Ay8A+4EnrLX7jDEPGGOuH+sCRUQkNA1rPihr7SZgU59lXxlg3XXnX5aIiIS6CTuShIiIBDcFlIiIBCQFlIiIBCRjrR16rbH4YGNqgGOj8FbpQO0ovE+w0vYZmrbR0LSNBqftM7Te22i6tXbIi2FdC6jRYozZYa3VNVcD0PYZmrbR0LSNBqftM7Rz2UZq4hMRkYCkgBIRkYAUDAH1kNsFBDhtn6FpGw1N22hw2j5DG/E2mvDnoEREJDgFwxGUiIgEIQWUiIgEpAkbUMaY9caYImNMiTHmPrfrCUTGmKPGmAJjzG5jjEaXB4wxDxtjqo0xe3stSzXG/NUYc9B/H7LTNA+wfe43xlT4v0e7jTHvcLNGtxljphljNhtj9htj9hljPutfru8Rg26fEX+PJuQ5KGNMOFAMXI0zX9V24FZrbaGrhQUY//xcK621uoDQzxhzGdAKPGKtXehf9t9AvbX2m/4/dlKstV90s063DLB97gdarbX/42ZtgcIYkwVkWWvfNMYkAjuBG4E70PdosO1zCyP8Hk3UI6jVQIm19rC1tht4HLjB5ZpkArDWvgrU91l8A/Br/+Nf4/xnCkkDbB/pxVpbZa190/+4BWcaomz0PQIG3T4jNlEDKhso6/W8nHPcAEHOAi8aY3YaY+5yu5gANtlaWwXOfy4g0+V6AtHdxpg9/ibAkGy66o8xJg9YBryBvkdn6bN9YITfo4kaUKafZROvrXLsXWytXQ5cC3zK33wjMlIPAjOApUAV8L/ulhMYjDEJwB+Bz1lrm92uJ9D0s31G/D2aqAFVDkzr9TwHqHSploBlra3031cDf8JpGpWznfC3m59sP692uZ6AYq09Ya31Wmt9wM/Q9whjTCTOzvd31ton/Yv1PfLrb/ucy/doogbUdmCWMSbfGBMFbACecbmmgGKMifefoMQYEw+8Hdg7+E+FrGeAD/kffwh42sVaAs7Jna7fuwnx75ExxgC/APZba7/T6yV9jxh4+5zL92hC9uID8HdR/B4QDjxsrf0vl0sKKMaYC3COmgAigEe1jcAY8xiwDmfo/xPAV4GngCeAXKAUeK+1NiQ7CgywfdbhNMtY4CjwsZPnWkKRMeYS4B9AAeDzL/53nPMsIf89GmT73MoIv0cTNqBERCS4TdQmPhERCXIKKBERCUgKKBERCUgKKBERCUgKKBERCUgKKBERCUgKKBERCUj/P9j2SDVX9Z5mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('input/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('input/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "X=np.array(data)\n",
    "y=np.array(label)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.8, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "mlp = MLP([128, 64, 32, 10],activation=[None,'logistic', 'logistic', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "start = time.time()\n",
    "losses, accuracies_train, accuracies_val = mlp.model_checkpointer(X_train, y_train, learning_rate=0.001, batch_size=32, momentum=0.9, weight_decay=0.0, epochs=25)\n",
    "end = time.time()\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_val, label='val')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_logistic.png')\n",
    "\n",
    "print('Time taken to train and predict: {:.2f} seconds'.format(end-start))\n",
    "print('Best accuracy achieved: {:.3f} accuracy'.format(mlp.best_accuracy))\n",
    "\n",
    "with h5py.File('input/test_128.h5','r') as H:\n",
    "    test_data = np.copy(H['data'])\n",
    "    \n",
    "scaler = StandardScaler().fit(data)\n",
    "scaled_test_data = scaler.transform(test_data)\n",
    "test_predictions = mlp.predict(scaled_test_data)\n",
    "\n",
    "with h5py.File('output/Predicted_labels.h5','w') as hdf:\n",
    "    hdf.create_dataset('labels', data = test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu Architecture [128,64,32,10] Dropout = 0.1, Batch_Size =16, LR=0.001, momentum=25, epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:238: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:238: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.6464444444444445 \n",
      "Validation Accuracy: 0.626 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.7947777777777778 \n",
      "Validation Accuracy: 0.7726666666666666 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.836 \n",
      "Validation Accuracy: 0.8083333333333333 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.8495555555555555 \n",
      "Validation Accuracy: 0.8153333333333334 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.8656666666666667 \n",
      "Validation Accuracy: 0.8193333333333334 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.8682222222222222 \n",
      "Validation Accuracy: 0.8223333333333334 \n",
      "Loss: nan \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('input/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('input/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "X=np.array(data)\n",
    "y=np.array(label)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.8, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "mlp = MLP([128, 64, 32, 10],activation=[None,'ReLU', 'ReLU', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "losses, accuracies_train, accuracies_val = mlp.model_checkpointer(X_train, y_train, learning_rate=0.0001 , batch_size=16, momentum=0.9, weight_decay=0.0, epochs=25)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Time taken to train and predict: {:.2f} seconds'.format(end-start))\n",
    "print('Best accuracy achieved: {:.3f} accuracy'.format(mlp.best_accuracy))\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_val, label='val')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_logistic.png')\n",
    "\n",
    "# READ TEST DATA\n",
    "with h5py.File('input/test_128.h5','r') as H:\n",
    "    test_data = np.copy(H['data'])\n",
    "    \n",
    "scaler = StandardScaler().fit(data)\n",
    "scaled_test_data = scaler.transform(test_data)\n",
    "test_predictions = mlp.best_model.predict(scaled_test_data)\n",
    "\n",
    "# WRITE PREDICTIONS\n",
    "with h5py.File('output/Predicted_labels.h5','w') as hdf:\n",
    "    hdf.create_dataset('labels', data = test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## %87.3 Accuracy Relu Architecture [128,64,32,10] Dropout = 0.0, Batch_Size =16, LR=0.001, epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "with h5py.File('input/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('input/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "mlp = MLP([128, 64, 32, 10],activation=[None, 'ReLU', 'ReLU','softmax'], dropout=[0.0, 0.0, 0.0, 0])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.model_checkpointer(data, label, batch_size=16, momentum=0.9, learning_rate=0.0001,epochs=25)\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken to train and predict: {:.2f} seconds'.format(end-start))\n",
    "print('Best accuracy achieved: {:.3f} accuracy'.format(mlp.best_accuracy))\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='validation')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_relu_dropout0.1.png')\n",
    "\n",
    "# READ TEST DATA\n",
    "with h5py.File('input/test_128.h5','r') as H:\n",
    "    test_data = np.copy(H['data'])\n",
    "    \n",
    "scaler = StandardScaler().fit(data)\n",
    "scaled_test_data = scaler.transform(test_data)\n",
    "test_predictions = mlp.best_model.predict(scaled_test_data)\n",
    "\n",
    "# WRITE PREDICTIONS\n",
    "with h5py.File('output/Predicted_labels.h5','w') as hdf:\n",
    "    hdf.create_dataset('labels', data = test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## %89 Accuracy Relu Architecture [128,64,32,10] Dropout = 0.1, Batch_Size =32, LR=0.001, epochs=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with h5py.File('input/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('input/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "mlp = MLP([128, 256, 64, 10],activation=[None, 'ReLU', 'ReLU','softmax'], dropout=[0.1, 0.1, 0.0, 0])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.model_checkpointer(data, label, batch_size=32, momentum=0.9, learning_rate=0.0001,epochs=80)\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken to train and predict: {:.2f} seconds'.format(end-start))\n",
    "print('Best accuracy achieved: {:.3f} accuracy'.format(mlp.best_accuracy))\n",
    "\n",
    "ax.plot(accuracies_train, label='train')\n",
    "ax.plot(accuracies_test, label='validation')\n",
    "ax.tight_layout()\n",
    "ax.legend()\n",
    "plt.set_xlabel('Epochs')\n",
    "plt.set_ylabel('Accuracy')\n",
    "ax.savefig('accuracy_relu_dropout0.1.png')\n",
    "\n",
    "# READ TEST DATA\n",
    "with h5py.File('input/test_128.h5','r') as H:\n",
    "    test_data = np.copy(H['data'])\n",
    "    \n",
    "scaler = StandardScaler().fit(data)\n",
    "scaled_test_data = scaler.transform(test_data)\n",
    "test_predictions = mlp.best_model.predict(scaled_test_data)\n",
    "\n",
    "# WRITE PREDICTIONS\n",
    "with h5py.File('output/Predicted_labels.h5','w') as hdf:\n",
    "    hdf.create_dataset('labels', data = test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with h5py.File('input/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('input/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "mlp = MLP([128, 256, 64, 10],activation=[None, 'ReLU', 'ReLU','softmax'], dropout=[0.2, 0.2, 0.0, 0])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.model_checkpointer(data, label, batch_size=32, momentum=0.9, learning_rate=0.0001,epochs=80)\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken to train and predict: {:.2f} seconds'.format(end-start))\n",
    "print('Best accuracy achieved: {:.3f} accuracy'.format(mlp.best_accuracy))\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='validation')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.set_xlabel('Epochs')\n",
    "plt.set_ylabel('Accuracy')\n",
    "plt.savefig('accuracy_relu_dropout0.1.png')\n",
    "\n",
    "# READ TEST DATA\n",
    "with h5py.File('input/test_128.h5','r') as H:\n",
    "    test_data = np.copy(H['data'])\n",
    "    \n",
    "scaler = StandardScaler().fit(data)\n",
    "scaled_test_data = scaler.transform(test_data)\n",
    "test_predictions = mlp.best_model.predict(scaled_test_data)\n",
    "\n",
    "# WRITE PREDICTIONS\n",
    "with h5py.File('output/Predicted_labels.h5','w') as hdf:\n",
    "    hdf.create_dataset('labels', data = test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with h5py.File('input/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('input/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "mlp = MLP([128, 256, 64, 10],activation=[None, 'ReLU', 'ReLU','softmax'], dropout=[0.3, 0.3, 0.0, 0])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.model_checkpointer(data, label, batch_size=32, momentum=0.9, learning_rate=0.0001,epochs=80)\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken to train and predict: {:.2f} seconds'.format(end-start))\n",
    "print('Best accuracy achieved: {:.3f} accuracy'.format(mlp.best_accuracy))\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='validation')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.set_xlabel('Epochs')\n",
    "plt.set_ylabel('Accuracy')\n",
    "plt.savefig('accuracy_relu_dropout0.1.png')\n",
    "\n",
    "# READ TEST DATA\n",
    "with h5py.File('input/test_128.h5','r') as H:\n",
    "    test_data = np.copy(H['data'])\n",
    "    \n",
    "scaler = StandardScaler().fit(data)\n",
    "scaled_test_data = scaler.transform(test_data)\n",
    "test_predictions = mlp.best_model.predict(scaled_test_data)\n",
    "\n",
    "# WRITE PREDICTIONS\n",
    "with h5py.File('output/Predicted_labels.h5','w') as hdf:\n",
    "    hdf.create_dataset('labels', data = test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

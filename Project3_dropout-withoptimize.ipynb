{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import logsumexp\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  (a * (1 - a ))\n",
    "    \n",
    "    def __softmax(self, z):\n",
    "        z=np.atleast_2d(z)\n",
    "        s = np.max(z, axis=1)\n",
    "        s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "        e_x = np.exp(z - s)\n",
    "        div = np.sum(e_x, axis=1)\n",
    "        div = div[:, np.newaxis] # dito\n",
    "        return e_x / div\n",
    "    \n",
    "    def __softmax_deriv(self, a):\n",
    "        #a = softmax(x)\n",
    "        return (a * (1 - a))\n",
    "    \n",
    "    def __ReLU(self,x):\n",
    "        return np.vectorize(lambda x:x if x>0 else 0)(x)\n",
    "    \n",
    "    def __ReLU_deriv(self,a):\n",
    "        #a = ReLU()\n",
    "        return np.vectorize(lambda x:1 if x>0 else 0)(a)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__ReLU\n",
    "            self.f_deriv = self.__ReLU_deriv\n",
    "            \n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', dropout=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout=dropout\n",
    "        self.dropout_vector = None\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, mode):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        if (mode=='train' and self.dropout>0):\n",
    "            self.dropout_vector = np.random.binomial(1, 1-self.dropout, size=input.shape)/(1-self.dropout)\n",
    "            lin_output = np.dot(self.dropout_vector*input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):\n",
    "        self.grad_W = np.atleast_2d(self.dropout_vector*self.input if self.dropout>0 else self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = np.sum(delta,axis=0)\n",
    "        \n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], batch_size=False, dropout=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.mode = 'train'\n",
    "        self.activation=activation\n",
    "        self.dropout=dropout\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],self.dropout[i]))\n",
    "            \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input=input, mode=self.mode)\n",
    "            input=output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=error**2\n",
    "        # calculate the delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)\n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    def criterion_CELoss(self,y,y_hat):\n",
    "        error = y * np.log(y_hat)\n",
    "        loss = -np.sum(error)\n",
    "        delta = y_hat-y\n",
    "        return loss,delta\n",
    "        \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "            \n",
    "    def get_batches(self,X, y, batch_size):\n",
    "        batches = []\n",
    "\n",
    "        X, y = shuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_batch = X[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "            \n",
    "            batches.append((X_batch, y_batch))\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=10, batch_size=False):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.batch_size=batch_size\n",
    "        self.train()\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        \n",
    "        # Differentiate Stochastic Gradient Descent vs Batch Gradient Descent\n",
    "        if batch_size:\n",
    "            batches = self.get_batches(X, y_dummies, batch_size)\n",
    "            for k in range(epochs):\n",
    "                loss = np.zeros(X.shape[0])\n",
    "                for X,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        loss,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                        \n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate/batch_size)\n",
    "                to_return[k] = np.mean(loss)\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss=np.zeros(X.shape[0])\n",
    "                for it in range(X.shape[0]):\n",
    "                    i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X[i])\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                to_return[k] = np.mean(loss)\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.test()\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        output = self.forward(x)\n",
    "        return output\n",
    "    \n",
    "    def optimize(self, X, y, learning_rate=0.01, test_size=0.25, batch_size=False,epochs=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        losses = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        if batch_size:\n",
    "            batches = self.get_batches(X_train, y_train, batch_size)\n",
    "            for k in range(epochs):\n",
    "                loss = []\n",
    "                for X,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        l,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        l,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                    \n",
    "                    loss.append(l)\n",
    "                    print('l',l.shape)\n",
    "                        \n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate/batch_size)\n",
    "                losses[k] = np.sum(loss)/X_train.shape[1]\n",
    "                print(X_train.shape)\n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "                if verbose:\n",
    "                    print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                          format(k, accuracy_train, accuracy_val, losses[k]))\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss=np.zeros(X.shape[1])\n",
    "                for it in range(X.shape[0]):\n",
    "                    i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X[i])\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                losses[k] = np.mean(loss)\n",
    "                \n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "\n",
    "                if verbose:\n",
    "                    print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                          format(k, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "            \n",
    "                losses[k] = np.mean(loss)\n",
    "        return losses, accuracies_val, accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "X=np.array(data)\n",
    "y=np.array(label)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.8, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "    \n",
    "mlp = MLP([128,128,32,10],activation=[None, 'ReLU','ReLU', 'softmax'], dropout=[0.0, 0.0, 0.0])\n",
    "\n",
    "mlp.fit(X_train, y_train, learning_rate=0.1, batch_size=100, epochs=10)\n",
    "predictions = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for these settings : 0.8322708333333333\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for these settings : {}'.format((np.argmax(predictions,axis=1)==y_val).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 128)\n",
      "Epoch: 0..\n",
      "train Accuracy: 0.461 \n",
      "Validation Accuracy: 0.36673333333333336 \n",
      "Loss: 742.3128880528795 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.7164222222222222 \n",
      "Validation Accuracy: 0.6717333333333333 \n",
      "Loss: 430.5848493376004 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.7827555555555555 \n",
      "Validation Accuracy: 0.7516666666666667 \n",
      "Loss: 256.4436042955396 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.8097555555555556 \n",
      "Validation Accuracy: 0.7858 \n",
      "Loss: 201.4042224241642 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.825 \n",
      "Validation Accuracy: 0.8050666666666667 \n",
      "Loss: 177.87596620452908 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.8364222222222222 \n",
      "Validation Accuracy: 0.8143333333333334 \n",
      "Loss: 163.67857203162592 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 6..\n",
      "train Accuracy: 0.8435777777777778 \n",
      "Validation Accuracy: 0.8221333333333334 \n",
      "Loss: 154.87251738849116 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 7..\n",
      "train Accuracy: 0.8483777777777778 \n",
      "Validation Accuracy: 0.8249333333333333 \n",
      "Loss: 148.0129210430468 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 8..\n",
      "train Accuracy: 0.8529777777777777 \n",
      "Validation Accuracy: 0.8285333333333333 \n",
      "Loss: 142.73001175841705 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 9..\n",
      "train Accuracy: 0.8567777777777777 \n",
      "Validation Accuracy: 0.8324 \n",
      "Loss: 138.29894917806865 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 10..\n",
      "train Accuracy: 0.8592444444444445 \n",
      "Validation Accuracy: 0.8363333333333334 \n",
      "Loss: 135.11618133278827 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 11..\n",
      "train Accuracy: 0.862 \n",
      "Validation Accuracy: 0.8401333333333333 \n",
      "Loss: 131.7433645930229 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 12..\n",
      "train Accuracy: 0.8637333333333334 \n",
      "Validation Accuracy: 0.8416 \n",
      "Loss: 128.43442859811563 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 13..\n",
      "train Accuracy: 0.866 \n",
      "Validation Accuracy: 0.8440666666666666 \n",
      "Loss: 126.46510900447039 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 14..\n",
      "train Accuracy: 0.8682222222222222 \n",
      "Validation Accuracy: 0.8457333333333333 \n",
      "Loss: 124.3726702030681 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 15..\n",
      "train Accuracy: 0.8703111111111111 \n",
      "Validation Accuracy: 0.8471333333333333 \n",
      "Loss: 122.14707065908722 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 16..\n",
      "train Accuracy: 0.8717777777777778 \n",
      "Validation Accuracy: 0.8473333333333334 \n",
      "Loss: 119.76357372809996 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 17..\n",
      "train Accuracy: 0.8732444444444445 \n",
      "Validation Accuracy: 0.8482 \n",
      "Loss: 118.05179794571949 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 18..\n",
      "train Accuracy: 0.8746666666666667 \n",
      "Validation Accuracy: 0.8488666666666667 \n",
      "Loss: 116.25220671656582 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 19..\n",
      "train Accuracy: 0.8775333333333334 \n",
      "Validation Accuracy: 0.8496 \n",
      "Loss: 114.45238689560674 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 20..\n",
      "train Accuracy: 0.8795333333333333 \n",
      "Validation Accuracy: 0.849 \n",
      "Loss: 112.95675643036397 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 21..\n",
      "train Accuracy: 0.8794 \n",
      "Validation Accuracy: 0.8505333333333334 \n",
      "Loss: 111.62803502877185 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 22..\n",
      "train Accuracy: 0.8812666666666666 \n",
      "Validation Accuracy: 0.8516 \n",
      "Loss: 110.99694127563164 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 23..\n",
      "train Accuracy: 0.8816888888888889 \n",
      "Validation Accuracy: 0.85 \n",
      "Loss: 109.604892474029 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 24..\n",
      "train Accuracy: 0.8833333333333333 \n",
      "Validation Accuracy: 0.8502666666666666 \n",
      "Loss: 108.28291629390911 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 25..\n",
      "train Accuracy: 0.884 \n",
      "Validation Accuracy: 0.8507333333333333 \n",
      "Loss: 107.94644965450233 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 26..\n",
      "train Accuracy: 0.8850666666666667 \n",
      "Validation Accuracy: 0.8533333333333334 \n",
      "Loss: 106.65395878586233 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 27..\n",
      "train Accuracy: 0.8861777777777777 \n",
      "Validation Accuracy: 0.8549333333333333 \n",
      "Loss: 105.59563321862586 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 28..\n",
      "train Accuracy: 0.8868444444444444 \n",
      "Validation Accuracy: 0.8561333333333333 \n",
      "Loss: 104.45575065229727 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 29..\n",
      "train Accuracy: 0.8883111111111112 \n",
      "Validation Accuracy: 0.8554 \n",
      "Loss: 102.89926739159161 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 30..\n",
      "train Accuracy: 0.8875555555555555 \n",
      "Validation Accuracy: 0.8537333333333333 \n",
      "Loss: 101.97408772414809 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 31..\n",
      "train Accuracy: 0.8882666666666666 \n",
      "Validation Accuracy: 0.8558 \n",
      "Loss: 101.73483314262322 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 32..\n",
      "train Accuracy: 0.8889111111111111 \n",
      "Validation Accuracy: 0.8523333333333334 \n",
      "Loss: 101.36512854297976 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 33..\n",
      "train Accuracy: 0.8898888888888888 \n",
      "Validation Accuracy: 0.8526666666666667 \n",
      "Loss: 99.92355272290486 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 34..\n",
      "train Accuracy: 0.8904 \n",
      "Validation Accuracy: 0.8528666666666667 \n",
      "Loss: 99.0249993023448 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 35..\n",
      "train Accuracy: 0.8910666666666667 \n",
      "Validation Accuracy: 0.8532666666666666 \n",
      "Loss: 98.27979078752544 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 36..\n",
      "train Accuracy: 0.8914444444444445 \n",
      "Validation Accuracy: 0.8527333333333333 \n",
      "Loss: 97.14970845815044 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 37..\n",
      "train Accuracy: 0.8926888888888889 \n",
      "Validation Accuracy: 0.8536666666666667 \n",
      "Loss: 96.41441092831334 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 38..\n",
      "train Accuracy: 0.8931777777777777 \n",
      "Validation Accuracy: 0.8536 \n",
      "Loss: 95.55607394279089 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 39..\n",
      "train Accuracy: 0.8940222222222223 \n",
      "Validation Accuracy: 0.8548 \n",
      "Loss: 94.88200742696179 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 40..\n",
      "train Accuracy: 0.8941333333333333 \n",
      "Validation Accuracy: 0.8539333333333333 \n",
      "Loss: 94.2410104986528 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 41..\n",
      "train Accuracy: 0.8948222222222222 \n",
      "Validation Accuracy: 0.8537333333333333 \n",
      "Loss: 93.49872805472619 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 42..\n",
      "train Accuracy: 0.8948444444444444 \n",
      "Validation Accuracy: 0.8541333333333333 \n",
      "Loss: 92.80120062847946 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 43..\n",
      "train Accuracy: 0.8954444444444445 \n",
      "Validation Accuracy: 0.8541333333333333 \n",
      "Loss: 92.2560059341095 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 44..\n",
      "train Accuracy: 0.8943777777777778 \n",
      "Validation Accuracy: 0.8530666666666666 \n",
      "Loss: 91.71085063905775 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 45..\n",
      "train Accuracy: 0.8949333333333334 \n",
      "Validation Accuracy: 0.8542 \n",
      "Loss: 90.84491769858852 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 46..\n",
      "train Accuracy: 0.8951555555555556 \n",
      "Validation Accuracy: 0.8533333333333334 \n",
      "Loss: 90.47459779080093 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 47..\n",
      "train Accuracy: 0.8960888888888889 \n",
      "Validation Accuracy: 0.8526666666666667 \n",
      "Loss: 89.74761931106367 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 48..\n",
      "train Accuracy: 0.8967111111111111 \n",
      "Validation Accuracy: 0.853 \n",
      "Loss: 89.33592433200967 \n",
      "\n",
      "(45000, 128)\n",
      "Epoch: 49..\n",
      "train Accuracy: 0.8962222222222223 \n",
      "Validation Accuracy: 0.8532 \n",
      "Loss: 88.55848487864736 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,512,128,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.3, 0.3, 0.3, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, batch_size=1000,learning_rate=0.2,epochs=50)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'yhat_train' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-21e7dcd9f6ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-129-77e6a3e7d998>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, batch_size, epochs, verbose)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;31m# Calculate train and Test Accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m         \u001b[0maccuracy_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m         \u001b[0maccuracy_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'yhat_train' referenced before assignment"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.1,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(32,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (32,) and (1,1) not aligned: 32 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-9e969c7d6dab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, epochs, verbose)\u001b[0m\n\u001b[0;32m    314\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion_MSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[1;31m# update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta, output_layer)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_vector\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (32,) and (1,1) not aligned: 32 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,512,128,32,10],activation=[None, 'logistic', 'logistic', 'logistic', 'softmax'], dropout=[0.0, 0.0, 0.0, 0.0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.02,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tensorflow.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \n",
    "sess = tensorflow.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras import optimizers, metrics, Sequential\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "y = label\n",
    "X = data\n",
    "\n",
    "y_dummies = np.array(pd.get_dummies(y))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=0.25, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosscheck with Keras Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45000/45000 [==============================] - 2s 42us/step - loss: 0.9828 - acc: 0.6622 - categorical_accuracy: 0.6622\n",
      "Epoch 2/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.5128 - acc: 0.8221 - categorical_accuracy: 0.8221\n",
      "Epoch 3/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.4623 - acc: 0.8380 - categorical_accuracy: 0.8380\n",
      "Epoch 4/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.4270 - acc: 0.8482 - categorical_accuracy: 0.8482 0s - loss: 0.4199 - acc:\n",
      "Epoch 5/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.4106 - acc: 0.8544 - categorical_accuracy: 0.8544\n",
      "Epoch 6/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3929 - acc: 0.8596 - categorical_accuracy: 0.8596\n",
      "Epoch 7/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3740 - acc: 0.8663 - categorical_accuracy: 0.8663\n",
      "Epoch 8/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3626 - acc: 0.8705 - categorical_accuracy: 0.8705\n",
      "Epoch 9/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.3487 - acc: 0.8740 - categorical_accuracy: 0.8740 1s - loss: 0.3495 - \n",
      "Epoch 10/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.3452 - acc: 0.8761 - categorical_accuracy: 0.8761\n",
      "Epoch 11/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.3330 - acc: 0.8787 - categorical_accuracy: 0.8787\n",
      "Epoch 12/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3254 - acc: 0.8820 - categorical_accuracy: 0.8820\n",
      "Epoch 13/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3232 - acc: 0.8825 - categorical_accuracy: 0.8825\n",
      "Epoch 14/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3121 - acc: 0.8869 - categorical_accuracy: 0.8869\n",
      "Epoch 15/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3076 - acc: 0.8879 - categorical_accuracy: 0.8879\n",
      "Epoch 16/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3045 - acc: 0.8894 - categorical_accuracy: 0.8894\n",
      "Epoch 17/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.2975 - acc: 0.8918 - categorical_accuracy: 0.8918\n",
      "Epoch 18/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.2938 - acc: 0.8924 - categorical_accuracy: 0.8924\n",
      "Epoch 19/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.2911 - acc: 0.8944 - categorical_accuracy: 0.8944\n",
      "Epoch 20/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.2862 - acc: 0.8959 - categorical_accuracy: 0.8959\n"
     ]
    }
   ],
   "source": [
    "with tf.device('GPU'):\n",
    "    sgd = optimizers.sgd(momentum=0.9)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f974249e5a64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0myhat_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#Predict and calculate accuracy\n",
    "yhat_val = model.predict(X_val)\n",
    "\n",
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "\n",
    "print('Keras model accuracy : {}').format(accuracy_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

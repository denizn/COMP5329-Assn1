{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import logsumexp\n",
    "%pdb on\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  (a * (1 - a ))\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        y = np.atleast_2d(x)\n",
    "        axis = -1\n",
    "        y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "        y = np.exp(y)\n",
    "        ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "        p = y / ax_sum\n",
    "        if len(X.shape) == 1: p = p.flatten()    \n",
    "        return p    \n",
    "    \n",
    "    def __softmax_deriv(self, a):\n",
    "        #a = softmax(x)\n",
    "        return a * (1 - a)\n",
    "    \n",
    "    def __ReLU(self,x):\n",
    "        return x * (x > 0)\n",
    "    \n",
    "    def __ReLU_deriv(self,a):\n",
    "        return 1 * (a > 0)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__ReLU\n",
    "            self.f_deriv = self.__ReLU_deriv\n",
    "            \n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', dropout=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout=dropout\n",
    "        self.dropout_vector = None\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        self.vel_W = np.zeros(self.W.shape)\n",
    "        self.vel_b = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, mode):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        if (mode=='train' and self.dropout>0):\n",
    "            self.dropout_vector = np.random.binomial(1, 1-self.dropout, size=input.shape)/(1-self.dropout)\n",
    "            lin_output = np.dot(self.dropout_vector*input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):\n",
    "        \n",
    "        self.grad_W = np.atleast_2d(self.dropout_vector*self.input if self.dropout>0 else self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = np.sum(delta,axis=0)\n",
    "        \n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], dropout=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.mode = 'train'\n",
    "        self.activation=activation\n",
    "        self.dropout=dropout\n",
    "        self.batch_size = 1\n",
    "        self.weight_decay = 0\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],self.dropout[i]))\n",
    "            \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input=input, mode=self.mode)\n",
    "            input=output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=error**2\n",
    "        # calculate the delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)\n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    def criterion_CELoss(self,y,y_hat):\n",
    "        error = y * np.log(y_hat)\n",
    "        loss = -np.sum(error)\n",
    "        delta = y_hat-y\n",
    "        return loss,delta\n",
    "        \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            if self.momentum!=0:\n",
    "                layer.vel_W = layer.vel_W * self.momentum + layer.grad_W * self.lr\n",
    "                layer.vel_b = layer.vel_b * self.momentum + layer.grad_b * self.lr\n",
    "                \n",
    "                layer.W -= (layer.vel_W + layer.W * self.weight_decay)\n",
    "                layer.b -= (layer.vel_b + layer.b * self.weight_decay)\n",
    "            else:\n",
    "                layer.W -= (lr * layer.grad_W + layer.W * self.weight_decay)\n",
    "                layer.b -= (lr * layer.grad_b + layer.b * self.weight_decay)\n",
    "            \n",
    "    def get_batches(self,X, y, batch_size):\n",
    "        batches = []\n",
    "\n",
    "        X, y = shuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_batch = X[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "            \n",
    "            batches.append((X_batch, y_batch))\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=10, batch_size=1, momentum=0, weight_decay=0):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.batch_size=batch_size\n",
    "        self.train()\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        self.momentum = momentum\n",
    "        self.lr = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Differentiate Stochastic Gradient Descent vs Batch Gradient Descent\n",
    "        if batch_size>1:\n",
    "            batches = self.get_batches(X, y_dummies, batch_size)\n",
    "            for k in range(epochs):\n",
    "                loss = np.zeros(X.shape[0])\n",
    "                for X,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        loss,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                        \n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                to_return[k] = np.mean(loss)\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss=np.zeros(X.shape[0])\n",
    "                for it in range(X.shape[0]):\n",
    "                    i=np.random.randint(X.shape[0])\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X[i])\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y_dummies[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y_dummies[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                to_return[k] = np.mean(loss)\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.test()\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        output = self.forward(x)\n",
    "        return output\n",
    "    \n",
    "    def optimize(self, X, y, learning_rate=0.01, test_size=0.25, batch_size=False,epochs=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        losses = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        if batch_size:\n",
    "            batches = self.get_batches(X_train, y_train, batch_size)\n",
    "            for k in range(epochs):\n",
    "                loss = []\n",
    "                for X,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        l,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        l,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                    \n",
    "                    loss.append(l)                        \n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                losses[k] = np.sum(loss)/X_train.shape[1]\n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "                if verbose:\n",
    "                    print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                          format(k, accuracy_train, accuracy_val, losses[k]))\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss=np.zeros(X.shape[0])\n",
    "                \n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                self.train()\n",
    "                for it in range(X.shape[0]):\n",
    "                    i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X[i])\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y[i],y_hat_train)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y[i],y_hat_train)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                \n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "                \n",
    "                losses[k] = np.mean(loss)\n",
    "\n",
    "                if verbose:\n",
    "                    print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                          format(k, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "            \n",
    "                losses[k] = np.mean(loss)\n",
    "        return losses, accuracies_val, accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "X=np.array(data)\n",
    "y=np.array(label)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.8, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "    \n",
    "mlp = MLP([128,512,128,32,10],activation=[None,'ReLU','ReLU','ReLU','softmax'], dropout=[0, 0, 0, 0])\n",
    "\n",
    "mlp.fit(X_train, y_train, learning_rate=0.0001, batch_size=10, momentum=0.9, weight_decay=0.1, epochs=100)\n",
    "predictions = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy for these settings : {}'.format((np.argmax(predictions,axis=1)==y_val).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.6924 - acc: 0.7761 - categorical_accuracy: 0.7761\n",
      "Epoch 2/10\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3802 - acc: 0.8643 - categorical_accuracy: 0.8643\n",
      "Epoch 3/10\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3382 - acc: 0.8790 - categorical_accuracy: 0.8790\n",
      "Epoch 4/10\n",
      "45000/45000 [==============================] - 1s 21us/step - loss: 0.3134 - acc: 0.8856 - categorical_accuracy: 0.8856\n",
      "Epoch 5/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2941 - acc: 0.8928 - categorical_accuracy: 0.8928\n",
      "Epoch 6/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2786 - acc: 0.8996 - categorical_accuracy: 0.8996\n",
      "Epoch 7/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2652 - acc: 0.9025 - categorical_accuracy: 0.9025\n",
      "Epoch 8/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2534 - acc: 0.9081 - categorical_accuracy: 0.9081\n",
      "Epoch 9/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2440 - acc: 0.9110 - categorical_accuracy: 0.9110\n",
      "Epoch 10/10\n",
      "45000/45000 [==============================] - 1s 20us/step - loss: 0.2352 - acc: 0.9141 - categorical_accuracy: 0.9141\n",
      "Keras model accuracy : 0.8776666666666667\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras import optimizers, metrics, Sequential\n",
    "\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tensorflow.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \n",
    "sess = tensorflow.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "y = label\n",
    "X = data\n",
    "\n",
    "y_dummies = np.array(pd.get_dummies(y))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=0.25, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "with tf.device('GPU'):\n",
    "    sgd = optimizers.sgd(lr=0.01)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=10)\n",
    "\n",
    "#Predict and calculate accuracy\n",
    "yhat_val = model.predict(X_val)\n",
    "\n",
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "\n",
    "print('Keras model accuracy : {}'.format(accuracy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:189: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\dnuho\\Anaconda3\\envs\\data\\lib\\site-packages\\ipykernel_launcher.py:189: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: nan \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 6..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 7..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 8..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 9..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 10..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 11..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 12..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 13..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 14..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 15..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 16..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 17..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 18..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 19..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 20..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 21..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 22..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 23..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 24..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 25..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 26..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 27..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 28..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 29..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 30..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 31..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 32..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 33..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 34..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 35..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 36..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 37..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 38..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 39..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 40..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 41..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 42..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 43..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 44..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 45..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 46..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n",
      "Epoch: 47..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 48..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691597 \n",
      "\n",
      "Epoch: 49..\n",
      "train Accuracy: 0.09968888888888888 \n",
      "Validation Accuracy: 0.10093333333333333 \n",
      "Loss: 811.0136328691596 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,512,128,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, batch_size=1000,learning_rate=0.01,epochs=50)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'yhat_train' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-21e7dcd9f6ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-129-77e6a3e7d998>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, batch_size, epochs, verbose)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;31m# Calculate train and Test Accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m         \u001b[0maccuracy_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m         \u001b[0maccuracy_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'yhat_train' referenced before assignment"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.1,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(32,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (32,) and (1,1) not aligned: 32 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-9e969c7d6dab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, epochs, verbose)\u001b[0m\n\u001b[0;32m    314\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion_MSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[1;31m# update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta, output_layer)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_vector\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (32,) and (1,1) not aligned: 32 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,512,128,32,10],activation=[None, 'logistic', 'logistic', 'logistic', 'softmax'], dropout=[0.0, 0.0, 0.0, 0.0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.02,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tensorflow.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \n",
    "sess = tensorflow.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_24 to have shape (10,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-8f69afa4341a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical_accuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#Predict and calculate accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\data\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_24 to have shape (10,) but got array with shape (1,)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\dnuho\\anaconda3\\envs\\data\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m(138)\u001b[0;36mstandardize_input_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m    136 \u001b[1;33m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    137 \u001b[1;33m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m--> 138 \u001b[1;33m                            str(data_shape))\n",
      "\u001b[0m\u001b[1;32m    139 \u001b[1;33m    \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    140 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras import optimizers, metrics, Sequential\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "y = label\n",
    "X = data\n",
    "\n",
    "y_dummies = np.array(pd.get_dummies(y))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=0.25, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "with tf.device('GPU'):\n",
    "    sgd = optimizers.sgd(lr=0.1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=20)\n",
    "\n",
    "#Predict and calculate accuracy\n",
    "yhat_val = model.predict(X_val)\n",
    "\n",
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "\n",
    "print('Keras model accuracy : {}'.format(accuracy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosscheck with Keras Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-4c2f6c2ef006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0msgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-117-4c2f6c2ef006>\u001b[0m(1)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m----> 1 \u001b[1;33m\u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      2 \u001b[1;33m    \u001b[0msgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      3 \u001b[1;33m    \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      4 \u001b[1;33m    \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      5 \u001b[1;33m    \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with tf.device('GPU'):\n",
    "    sgd = optimizers.sgd()\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict and calculate accuracy\n",
    "yhat_val = model.predict(X_val)\n",
    "\n",
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "\n",
    "print('Keras model accuracy : {}'.format(accuracy_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

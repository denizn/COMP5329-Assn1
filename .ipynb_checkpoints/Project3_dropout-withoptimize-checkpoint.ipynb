{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import logsumexp\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  (a * (1 - a ))\n",
    "    \n",
    "    def __softmax(self, z):\n",
    "        z=np.atleast_2d(z)\n",
    "        s = np.max(z, axis=1)\n",
    "        s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "        e_x = np.exp(z - s)\n",
    "        div = np.sum(e_x, axis=1)\n",
    "        div = div[:, np.newaxis] # dito\n",
    "        return e_x / div\n",
    "    \n",
    "    def __softmax_deriv(self, a):\n",
    "        #a = softmax(x)\n",
    "        return (a * (1 - a))\n",
    "    \n",
    "    def __ReLU(self,x):\n",
    "        return np.vectorize(lambda x:x if x>0 else 0)(x)\n",
    "    \n",
    "    def __ReLU_deriv(self,a):\n",
    "        #a = ReLU()\n",
    "        return np.vectorize(lambda x:1 if x>0 else 0)(a)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__ReLU\n",
    "            self.f_deriv = self.__ReLU_deriv\n",
    "            \n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', dropout=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout=dropout\n",
    "        self.dropout_vector = None\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, mode):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        if (mode=='train' and self.dropout>0):\n",
    "            self.dropout_vector = np.random.binomial(1, 1-self.dropout, size=input.shape)/(1-self.dropout)\n",
    "            lin_output = np.dot(self.dropout_vector*input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):\n",
    "        self.grad_W = np.atleast_2d(self.dropout_vector*self.input if self.dropout>0 else self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = np.sum(delta,axis=0)\n",
    "        \n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], batch_size=False, dropout=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.mode = 'train'\n",
    "        self.activation=activation\n",
    "        self.dropout=dropout\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],self.dropout[i]))\n",
    "            \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input=input, mode=self.mode)\n",
    "            input=output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=error**2\n",
    "        # calculate the delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)\n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    def criterion_CELoss(self,y,y_hat):\n",
    "        error = y * np.log(y_hat)\n",
    "        loss = -np.sum(error)\n",
    "        delta = y_hat-y\n",
    "        return loss,delta\n",
    "        \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "            \n",
    "    def get_batches(self,X, y, batch_size):\n",
    "        batches = []\n",
    "\n",
    "        X, y = shuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_batch = X[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "            \n",
    "            batches.append((X_batch, y_batch))\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=10, batch_size=False):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.batch_size=batch_size\n",
    "        self.train()\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        \n",
    "        # Differentiate Stochastic Gradient Descent vs Batch Gradient Descent\n",
    "        if batch_size:\n",
    "            batches = self.get_batches(X, y_dummies, batch_size)\n",
    "            for k in range(epochs):\n",
    "                loss = np.zeros(X.shape[0])\n",
    "                for X,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        loss,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                        \n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate/batch_size)\n",
    "                to_return[k] = np.mean(loss)\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss=np.zeros(X.shape[0])\n",
    "                for it in range(X.shape[0]):\n",
    "                    i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X[i])\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                to_return[k] = np.mean(loss)\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.test()\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        output = self.forward(x)\n",
    "        return output\n",
    "    \n",
    "    def optimize(self, X, y, learning_rate=0.01, test_size=0.25, batch_size=False,epochs=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        losses = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        if batch_size:\n",
    "            batches = self.get_batches(X_train, y_train, batch_size)\n",
    "            for k in range(epochs):\n",
    "                loss = []\n",
    "                for X,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        l,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        l,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                    \n",
    "                    loss.append(l)\n",
    "                        \n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate/batch_size)\n",
    "                losses[k] = np.sum(loss)/X_train.shape[1]\n",
    "                print(X_train.shape)\n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "                if verbose:\n",
    "                    print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                          format(k, accuracy_train, accuracy_val, losses[k]))\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss=np.zeros(X.shape[1])\n",
    "                for it in range(X.shape[0]):\n",
    "                    i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X[i])\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                losses[k] = np.mean(loss)\n",
    "                \n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "\n",
    "                if verbose:\n",
    "                    print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                          format(k, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "            \n",
    "                losses[k] = np.mean(loss)\n",
    "        return losses, accuracies_val, accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "X=np.array(data)\n",
    "y=np.array(label)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.8, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "    \n",
    "mlp = MLP([128,128,32,10],activation=[None, 'ReLU','ReLU', 'softmax'], dropout=[0.0, 0.0, 0.0])\n",
    "\n",
    "mlp.fit(X_train, y_train, learning_rate=0.1, batch_size=100, epochs=10)\n",
    "predictions = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for these settings : 0.8364791666666667\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for these settings : {}'.format((np.argmax(predictions,axis=1)==y_val).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.4550444444444444 \n",
      "Validation Accuracy: 0.4452 \n",
      "Loss: 755.0970435363752 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.7208888888888889 \n",
      "Validation Accuracy: 0.7123333333333334 \n",
      "Loss: 511.2415521779537 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.7930888888888888 \n",
      "Validation Accuracy: 0.7892 \n",
      "Loss: 296.10926291737434 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.8199555555555555 \n",
      "Validation Accuracy: 0.8125333333333333 \n",
      "Loss: 224.92535052224 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.8331333333333333 \n",
      "Validation Accuracy: 0.8276666666666667 \n",
      "Loss: 197.13198597455923 \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.8417777777777777 \n",
      "Validation Accuracy: 0.835 \n",
      "Loss: 180.64339519032046 \n",
      "\n",
      "Epoch: 6..\n",
      "train Accuracy: 0.849 \n",
      "Validation Accuracy: 0.8404 \n",
      "Loss: 169.95818030798674 \n",
      "\n",
      "Epoch: 7..\n",
      "train Accuracy: 0.8543777777777778 \n",
      "Validation Accuracy: 0.8436 \n",
      "Loss: 161.93992272896827 \n",
      "\n",
      "Epoch: 8..\n",
      "train Accuracy: 0.8578444444444444 \n",
      "Validation Accuracy: 0.8468 \n",
      "Loss: 155.5216169498074 \n",
      "\n",
      "Epoch: 9..\n",
      "train Accuracy: 0.8616666666666667 \n",
      "Validation Accuracy: 0.8498666666666667 \n",
      "Loss: 150.6866068945078 \n",
      "\n",
      "Epoch: 10..\n",
      "train Accuracy: 0.8643111111111111 \n",
      "Validation Accuracy: 0.8516666666666667 \n",
      "Loss: 146.56273684412204 \n",
      "\n",
      "Epoch: 11..\n",
      "train Accuracy: 0.8663777777777778 \n",
      "Validation Accuracy: 0.8530666666666666 \n",
      "Loss: 142.77598555451084 \n",
      "\n",
      "Epoch: 12..\n",
      "train Accuracy: 0.8685333333333334 \n",
      "Validation Accuracy: 0.8546666666666667 \n",
      "Loss: 139.21091046438156 \n",
      "\n",
      "Epoch: 13..\n",
      "train Accuracy: 0.8697333333333334 \n",
      "Validation Accuracy: 0.8552 \n",
      "Loss: 136.1858927377842 \n",
      "\n",
      "Epoch: 14..\n",
      "train Accuracy: 0.8716222222222222 \n",
      "Validation Accuracy: 0.8575333333333334 \n",
      "Loss: 133.20444589570914 \n",
      "\n",
      "Epoch: 15..\n",
      "train Accuracy: 0.8738 \n",
      "Validation Accuracy: 0.8591333333333333 \n",
      "Loss: 131.04808805026784 \n",
      "\n",
      "Epoch: 16..\n",
      "train Accuracy: 0.8753333333333333 \n",
      "Validation Accuracy: 0.8610666666666666 \n",
      "Loss: 128.18919906297276 \n",
      "\n",
      "Epoch: 17..\n",
      "train Accuracy: 0.8762 \n",
      "Validation Accuracy: 0.8616 \n",
      "Loss: 126.15964479551866 \n",
      "\n",
      "Epoch: 18..\n",
      "train Accuracy: 0.8780222222222223 \n",
      "Validation Accuracy: 0.8619333333333333 \n",
      "Loss: 124.26196607392643 \n",
      "\n",
      "Epoch: 19..\n",
      "train Accuracy: 0.8788444444444444 \n",
      "Validation Accuracy: 0.8624666666666667 \n",
      "Loss: 122.63415558208804 \n",
      "\n",
      "Epoch: 20..\n",
      "train Accuracy: 0.8805777777777778 \n",
      "Validation Accuracy: 0.8636 \n",
      "Loss: 121.81177408611381 \n",
      "\n",
      "Epoch: 21..\n",
      "train Accuracy: 0.8816666666666667 \n",
      "Validation Accuracy: 0.8644666666666667 \n",
      "Loss: 120.29307594949537 \n",
      "\n",
      "Epoch: 22..\n",
      "train Accuracy: 0.8829555555555556 \n",
      "Validation Accuracy: 0.8662 \n",
      "Loss: 118.45034137672451 \n",
      "\n",
      "Epoch: 23..\n",
      "train Accuracy: 0.884 \n",
      "Validation Accuracy: 0.8668666666666667 \n",
      "Loss: 116.66859728788151 \n",
      "\n",
      "Epoch: 24..\n",
      "train Accuracy: 0.8851333333333333 \n",
      "Validation Accuracy: 0.8677333333333334 \n",
      "Loss: 115.37253686050389 \n",
      "\n",
      "Epoch: 25..\n",
      "train Accuracy: 0.8867111111111111 \n",
      "Validation Accuracy: 0.8678 \n",
      "Loss: 114.03598753771753 \n",
      "\n",
      "Epoch: 26..\n",
      "train Accuracy: 0.8878444444444444 \n",
      "Validation Accuracy: 0.8661333333333333 \n",
      "Loss: 113.10393270889075 \n",
      "\n",
      "Epoch: 27..\n",
      "train Accuracy: 0.8886666666666667 \n",
      "Validation Accuracy: 0.8666 \n",
      "Loss: 111.80885168286278 \n",
      "\n",
      "Epoch: 28..\n",
      "train Accuracy: 0.8894666666666666 \n",
      "Validation Accuracy: 0.8674 \n",
      "Loss: 110.72389311926976 \n",
      "\n",
      "Epoch: 29..\n",
      "train Accuracy: 0.8903111111111112 \n",
      "Validation Accuracy: 0.8668666666666667 \n",
      "Loss: 109.77238916857405 \n",
      "\n",
      "Epoch: 30..\n",
      "train Accuracy: 0.8910888888888889 \n",
      "Validation Accuracy: 0.868 \n",
      "Loss: 108.8536052880193 \n",
      "\n",
      "Epoch: 31..\n",
      "train Accuracy: 0.8920888888888889 \n",
      "Validation Accuracy: 0.8681333333333333 \n",
      "Loss: 107.99674298148508 \n",
      "\n",
      "Epoch: 32..\n",
      "train Accuracy: 0.8926 \n",
      "Validation Accuracy: 0.8659333333333333 \n",
      "Loss: 106.88550093116102 \n",
      "\n",
      "Epoch: 33..\n",
      "train Accuracy: 0.8931777777777777 \n",
      "Validation Accuracy: 0.8672666666666666 \n",
      "Loss: 106.04234160437355 \n",
      "\n",
      "Epoch: 34..\n",
      "train Accuracy: 0.8940888888888889 \n",
      "Validation Accuracy: 0.8674 \n",
      "Loss: 105.33576329235567 \n",
      "\n",
      "Epoch: 35..\n",
      "train Accuracy: 0.8947333333333334 \n",
      "Validation Accuracy: 0.8681333333333333 \n",
      "Loss: 104.59582581580683 \n",
      "\n",
      "Epoch: 36..\n",
      "train Accuracy: 0.8958666666666667 \n",
      "Validation Accuracy: 0.8711333333333333 \n",
      "Loss: 103.59219952892632 \n",
      "\n",
      "Epoch: 37..\n",
      "train Accuracy: 0.8959555555555555 \n",
      "Validation Accuracy: 0.8718 \n",
      "Loss: 103.3320854890238 \n",
      "\n",
      "Epoch: 38..\n",
      "train Accuracy: 0.8963333333333333 \n",
      "Validation Accuracy: 0.8714 \n",
      "Loss: 102.43972184835513 \n",
      "\n",
      "Epoch: 39..\n",
      "train Accuracy: 0.8969777777777778 \n",
      "Validation Accuracy: 0.8718 \n",
      "Loss: 101.69732095688592 \n",
      "\n",
      "Epoch: 40..\n",
      "train Accuracy: 0.8976222222222222 \n",
      "Validation Accuracy: 0.8726666666666667 \n",
      "Loss: 100.7290295244695 \n",
      "\n",
      "Epoch: 41..\n",
      "train Accuracy: 0.8980888888888889 \n",
      "Validation Accuracy: 0.8731333333333333 \n",
      "Loss: 100.2994314410755 \n",
      "\n",
      "Epoch: 42..\n",
      "train Accuracy: 0.8982888888888889 \n",
      "Validation Accuracy: 0.8727333333333334 \n",
      "Loss: 99.54224047502046 \n",
      "\n",
      "Epoch: 43..\n",
      "train Accuracy: 0.8990444444444444 \n",
      "Validation Accuracy: 0.8726 \n",
      "Loss: 98.96871574273571 \n",
      "\n",
      "Epoch: 44..\n",
      "train Accuracy: 0.9001555555555556 \n",
      "Validation Accuracy: 0.8727333333333334 \n",
      "Loss: 97.88744347030591 \n",
      "\n",
      "Epoch: 45..\n",
      "train Accuracy: 0.9006666666666666 \n",
      "Validation Accuracy: 0.8737333333333334 \n",
      "Loss: 97.20246528853247 \n",
      "\n",
      "Epoch: 46..\n",
      "train Accuracy: 0.9011555555555556 \n",
      "Validation Accuracy: 0.8744666666666666 \n",
      "Loss: 96.55529856018825 \n",
      "\n",
      "Epoch: 47..\n",
      "train Accuracy: 0.9016888888888889 \n",
      "Validation Accuracy: 0.8751333333333333 \n",
      "Loss: 95.79971606522494 \n",
      "\n",
      "Epoch: 48..\n",
      "train Accuracy: 0.9021555555555556 \n",
      "Validation Accuracy: 0.8741333333333333 \n",
      "Loss: 95.37300101510579 \n",
      "\n",
      "Epoch: 49..\n",
      "train Accuracy: 0.9022222222222223 \n",
      "Validation Accuracy: 0.8747333333333334 \n",
      "Loss: 94.74893296581298 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, batch_size=1000,learning_rate=0.2,epochs=50)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'yhat_train' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-21e7dcd9f6ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-129-77e6a3e7d998>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, batch_size, epochs, verbose)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;31m# Calculate train and Test Accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m         \u001b[0maccuracy_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m         \u001b[0maccuracy_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'yhat_train' referenced before assignment"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.1,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(32,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (32,) and (1,1) not aligned: 32 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-9e969c7d6dab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, epochs, verbose)\u001b[0m\n\u001b[0;32m    314\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion_MSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[1;31m# update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta, output_layer)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_vector\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (32,) and (1,1) not aligned: 32 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,512,128,32,10],activation=[None, 'logistic', 'logistic', 'logistic', 'softmax'], dropout=[0.0, 0.0, 0.0, 0.0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.02,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tensorflow.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \n",
    "sess = tensorflow.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras import optimizers, metrics, Sequential\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "y = label\n",
    "X = data\n",
    "\n",
    "y_dummies = np.array(pd.get_dummies(y))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=0.25, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosscheck with Keras Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45000/45000 [==============================] - 2s 42us/step - loss: 0.9828 - acc: 0.6622 - categorical_accuracy: 0.6622\n",
      "Epoch 2/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.5128 - acc: 0.8221 - categorical_accuracy: 0.8221\n",
      "Epoch 3/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.4623 - acc: 0.8380 - categorical_accuracy: 0.8380\n",
      "Epoch 4/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.4270 - acc: 0.8482 - categorical_accuracy: 0.8482 0s - loss: 0.4199 - acc:\n",
      "Epoch 5/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.4106 - acc: 0.8544 - categorical_accuracy: 0.8544\n",
      "Epoch 6/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3929 - acc: 0.8596 - categorical_accuracy: 0.8596\n",
      "Epoch 7/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3740 - acc: 0.8663 - categorical_accuracy: 0.8663\n",
      "Epoch 8/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3626 - acc: 0.8705 - categorical_accuracy: 0.8705\n",
      "Epoch 9/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.3487 - acc: 0.8740 - categorical_accuracy: 0.8740 1s - loss: 0.3495 - \n",
      "Epoch 10/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.3452 - acc: 0.8761 - categorical_accuracy: 0.8761\n",
      "Epoch 11/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.3330 - acc: 0.8787 - categorical_accuracy: 0.8787\n",
      "Epoch 12/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3254 - acc: 0.8820 - categorical_accuracy: 0.8820\n",
      "Epoch 13/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3232 - acc: 0.8825 - categorical_accuracy: 0.8825\n",
      "Epoch 14/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3121 - acc: 0.8869 - categorical_accuracy: 0.8869\n",
      "Epoch 15/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3076 - acc: 0.8879 - categorical_accuracy: 0.8879\n",
      "Epoch 16/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3045 - acc: 0.8894 - categorical_accuracy: 0.8894\n",
      "Epoch 17/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.2975 - acc: 0.8918 - categorical_accuracy: 0.8918\n",
      "Epoch 18/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.2938 - acc: 0.8924 - categorical_accuracy: 0.8924\n",
      "Epoch 19/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.2911 - acc: 0.8944 - categorical_accuracy: 0.8944\n",
      "Epoch 20/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.2862 - acc: 0.8959 - categorical_accuracy: 0.8959\n"
     ]
    }
   ],
   "source": [
    "with tf.device('GPU'):\n",
    "    sgd = optimizers.sgd(momentum=0.9)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f974249e5a64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0myhat_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#Predict and calculate accuracy\n",
    "yhat_val = model.predict(X_val)\n",
    "\n",
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "\n",
    "print('Keras model accuracy : {}').format(accuracy_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

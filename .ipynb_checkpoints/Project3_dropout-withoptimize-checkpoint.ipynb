{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.misc import logsumexp\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  (a * (1 - a ))\n",
    "    \n",
    "    def __softmax(self, z):\n",
    "        z=np.atleast_2d(z)\n",
    "        s = np.max(z, axis=1)\n",
    "        s = s[:, np.newaxis] # necessary step to do broadcasting\n",
    "        e_x = np.exp(z - s)\n",
    "        div = np.sum(e_x, axis=1)\n",
    "        div = div[:, np.newaxis] # dito\n",
    "        return e_x / div\n",
    "    \n",
    "    def __softmax_deriv(self, a):\n",
    "        #a = softmax(x)\n",
    "        return (a * (1 - a))\n",
    "    \n",
    "    def __ReLU(self,x):\n",
    "        return np.vectorize(lambda x:x if x>0 else 0)(x)\n",
    "    \n",
    "    def __ReLU_deriv(self,a):\n",
    "        #a = ReLU()\n",
    "        return np.vectorize(lambda x:1 if x>0 else 0)(a)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__ReLU\n",
    "            self.f_deriv = self.__ReLU_deriv\n",
    "            \n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', dropout=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout=dropout\n",
    "        self.dropout_vector = None\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, mode):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        if (mode=='train' and self.dropout>0):\n",
    "            self.dropout_vector = np.random.binomial(1, 1-self.dropout, size=input.shape)/(1-self.dropout)\n",
    "            lin_output = np.dot(self.dropout_vector*input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):\n",
    "        self.grad_W = np.atleast_2d(self.dropout_vector*self.input if self.dropout>0 else self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = np.sum(delta,axis=0)\n",
    "        \n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], batch_size=False, dropout=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.mode = 'train'\n",
    "        self.activation=activation\n",
    "        self.dropout=dropout\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],self.dropout[i]))\n",
    "            \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input=input, mode=self.mode)\n",
    "            input=output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=error**2\n",
    "        # calculate the delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)\n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    def criterion_CELoss(self,y,y_hat):\n",
    "        error = y * np.log(y_hat)\n",
    "        loss = -np.sum(error)\n",
    "        delta = y_hat-y\n",
    "        return loss,delta\n",
    "        \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "            \n",
    "    def get_batches(self,X, y, batch_size):\n",
    "        batches = []\n",
    "\n",
    "        X, y = shuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_batch = X[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "            \n",
    "            batches.append((X_batch, y_batch))\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=10, batch_size=False):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.batch_size=batch_size\n",
    "        self.train()\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        \n",
    "        # Differentiate Stochastic Gradient Descent vs Batch Gradient Descent\n",
    "        if batch_size:\n",
    "            batches = self.get_batches(X, y_dummies, batch_size)\n",
    "            for k in range(epochs):\n",
    "                loss = np.zeros(X.shape[0])\n",
    "                for X,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        loss,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                        \n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate/batch_size)\n",
    "                to_return[k] = np.mean(loss)\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss=np.zeros(X.shape[0])\n",
    "                for it in range(X.shape[0]):\n",
    "                    i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X[i])\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                to_return[k] = np.mean(loss)\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.test()\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        output = self.forward(x)\n",
    "        return output\n",
    "    \n",
    "    def optimize(self, X, y, learning_rate=0.01, test_size=0.25, batch_size=False,epochs=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        losses = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        if batch_size:\n",
    "            batches = self.get_batches(X_train, y_train, batch_size)\n",
    "            for k in range(epochs):\n",
    "                loss = np.zeros(X.shape[0])\n",
    "                for X,y_dummies in batches:\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss,delta=self.criterion_CELoss(y_dummies,y_hat)\n",
    "                    else:\n",
    "                        loss,delta=self.criterion_MSE(y_dummies,y_hat)\n",
    "                        \n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate/batch_size)\n",
    "                losses[k] = np.mean(loss)\n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "                if verbose:\n",
    "                    print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                          format(k, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "        else:\n",
    "            for k in range(epochs):\n",
    "                loss=np.zeros(X.shape[0])\n",
    "                for it in range(X.shape[0]):\n",
    "                    i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X[i])\n",
    "                \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta=self.criterion_CELoss(y[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                losses[k] = np.mean(loss)\n",
    "                \n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "\n",
    "                if verbose:\n",
    "                    print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                          format(k, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "            \n",
    "                losses[k] = np.mean(loss)\n",
    "        return losses, accuracies_val, accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "X=np.array(data)\n",
    "y=np.array(label)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.8, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "    \n",
    "mlp = MLP([128,128,32,10],activation=[None, 'ReLU','ReLU', 'softmax'], dropout=[0.0, 0.0, 0.0])\n",
    "\n",
    "mlp.fit(X_train, y_train, learning_rate=0.1, batch_size=100, epochs=10)\n",
    "predictions = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8242916666666666"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.argmax(predictions,axis=1)==y_val).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.239 \n",
      "Validation Accuracy: 0.23173333333333335 \n",
      "Loss: 2095.671429382566 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.5562222222222222 \n",
      "Validation Accuracy: 0.5497333333333333 \n",
      "Loss: 1346.8225529561762 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.7346888888888888 \n",
      "Validation Accuracy: 0.7219333333333333 \n",
      "Loss: 755.4185429317045 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.7862222222222223 \n",
      "Validation Accuracy: 0.7683333333333333 \n",
      "Loss: 571.2409607354468 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.8223777777777778 \n",
      "Validation Accuracy: 0.7908 \n",
      "Loss: 489.8053627254782 \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.8344222222222222 \n",
      "Validation Accuracy: 0.8006666666666666 \n",
      "Loss: 444.0999798338627 \n",
      "\n",
      "Epoch: 6..\n",
      "train Accuracy: 0.8415111111111111 \n",
      "Validation Accuracy: 0.8108 \n",
      "Loss: 415.62357293949515 \n",
      "\n",
      "Epoch: 7..\n",
      "train Accuracy: 0.8466444444444444 \n",
      "Validation Accuracy: 0.8156666666666667 \n",
      "Loss: 390.68263837948484 \n",
      "\n",
      "Epoch: 8..\n",
      "train Accuracy: 0.8517555555555556 \n",
      "Validation Accuracy: 0.82 \n",
      "Loss: 370.21287370003404 \n",
      "\n",
      "Epoch: 9..\n",
      "train Accuracy: 0.8557555555555556 \n",
      "Validation Accuracy: 0.8243333333333334 \n",
      "Loss: 356.03870570597377 \n",
      "\n",
      "Epoch: 10..\n",
      "train Accuracy: 0.8583111111111111 \n",
      "Validation Accuracy: 0.8290666666666666 \n",
      "Loss: 347.92228096330683 \n",
      "\n",
      "Epoch: 11..\n",
      "train Accuracy: 0.8606222222222222 \n",
      "Validation Accuracy: 0.8320666666666666 \n",
      "Loss: 339.9882127212721 \n",
      "\n",
      "Epoch: 12..\n",
      "train Accuracy: 0.8641333333333333 \n",
      "Validation Accuracy: 0.8345333333333333 \n",
      "Loss: 334.562437652194 \n",
      "\n",
      "Epoch: 13..\n",
      "train Accuracy: 0.8659777777777777 \n",
      "Validation Accuracy: 0.8345333333333333 \n",
      "Loss: 332.81370101737264 \n",
      "\n",
      "Epoch: 14..\n",
      "train Accuracy: 0.8668444444444444 \n",
      "Validation Accuracy: 0.8371333333333333 \n",
      "Loss: 327.4158314113673 \n",
      "\n",
      "Epoch: 15..\n",
      "train Accuracy: 0.8687777777777778 \n",
      "Validation Accuracy: 0.8385333333333334 \n",
      "Loss: 318.2975326492424 \n",
      "\n",
      "Epoch: 16..\n",
      "train Accuracy: 0.8716666666666667 \n",
      "Validation Accuracy: 0.8392 \n",
      "Loss: 310.8780177838828 \n",
      "\n",
      "Epoch: 17..\n",
      "train Accuracy: 0.8727333333333334 \n",
      "Validation Accuracy: 0.8406 \n",
      "Loss: 305.20956021877817 \n",
      "\n",
      "Epoch: 18..\n",
      "train Accuracy: 0.8744 \n",
      "Validation Accuracy: 0.8424 \n",
      "Loss: 304.8090858906764 \n",
      "\n",
      "Epoch: 19..\n",
      "train Accuracy: 0.8748888888888889 \n",
      "Validation Accuracy: 0.8426666666666667 \n",
      "Loss: 297.6938849421688 \n",
      "\n",
      "Epoch: 20..\n",
      "train Accuracy: 0.8774666666666666 \n",
      "Validation Accuracy: 0.8422 \n",
      "Loss: 290.16837959367484 \n",
      "\n",
      "Epoch: 21..\n",
      "train Accuracy: 0.879 \n",
      "Validation Accuracy: 0.8435333333333334 \n",
      "Loss: 286.1700618289522 \n",
      "\n",
      "Epoch: 22..\n",
      "train Accuracy: 0.8801333333333333 \n",
      "Validation Accuracy: 0.8418 \n",
      "Loss: 281.92367415589257 \n",
      "\n",
      "Epoch: 23..\n",
      "train Accuracy: 0.8812444444444445 \n",
      "Validation Accuracy: 0.8415333333333334 \n",
      "Loss: 278.7295997091987 \n",
      "\n",
      "Epoch: 24..\n",
      "train Accuracy: 0.8814666666666666 \n",
      "Validation Accuracy: 0.8428 \n",
      "Loss: 281.6257714772088 \n",
      "\n",
      "Epoch: 25..\n",
      "train Accuracy: 0.8819555555555556 \n",
      "Validation Accuracy: 0.8406 \n",
      "Loss: 280.8122769585039 \n",
      "\n",
      "Epoch: 26..\n",
      "train Accuracy: 0.8832888888888889 \n",
      "Validation Accuracy: 0.8406 \n",
      "Loss: 277.59717627471866 \n",
      "\n",
      "Epoch: 27..\n",
      "train Accuracy: 0.8842 \n",
      "Validation Accuracy: 0.843 \n",
      "Loss: 272.33833532745655 \n",
      "\n",
      "Epoch: 28..\n",
      "train Accuracy: 0.8857333333333334 \n",
      "Validation Accuracy: 0.8443333333333334 \n",
      "Loss: 273.0954242812843 \n",
      "\n",
      "Epoch: 29..\n",
      "train Accuracy: 0.8867555555555555 \n",
      "Validation Accuracy: 0.8454 \n",
      "Loss: 269.2078066705845 \n",
      "\n",
      "Epoch: 30..\n",
      "train Accuracy: 0.8868 \n",
      "Validation Accuracy: 0.8452666666666667 \n",
      "Loss: 266.27160531604096 \n",
      "\n",
      "Epoch: 31..\n",
      "train Accuracy: 0.8870666666666667 \n",
      "Validation Accuracy: 0.8456666666666667 \n",
      "Loss: 263.02266693238744 \n",
      "\n",
      "Epoch: 32..\n",
      "train Accuracy: 0.8874 \n",
      "Validation Accuracy: 0.8464666666666667 \n",
      "Loss: 265.9339139970949 \n",
      "\n",
      "Epoch: 33..\n",
      "train Accuracy: 0.8887333333333334 \n",
      "Validation Accuracy: 0.8466666666666667 \n",
      "Loss: 266.8680634730709 \n",
      "\n",
      "Epoch: 34..\n",
      "train Accuracy: 0.8890666666666667 \n",
      "Validation Accuracy: 0.8474666666666667 \n",
      "Loss: 267.893502427147 \n",
      "\n",
      "Epoch: 35..\n",
      "train Accuracy: 0.8896444444444445 \n",
      "Validation Accuracy: 0.8470666666666666 \n",
      "Loss: 266.2297284531735 \n",
      "\n",
      "Epoch: 36..\n",
      "train Accuracy: 0.8906 \n",
      "Validation Accuracy: 0.8473333333333334 \n",
      "Loss: 263.41006707077287 \n",
      "\n",
      "Epoch: 37..\n",
      "train Accuracy: 0.8914 \n",
      "Validation Accuracy: 0.8482666666666666 \n",
      "Loss: 261.24705811129155 \n",
      "\n",
      "Epoch: 38..\n",
      "train Accuracy: 0.8917555555555555 \n",
      "Validation Accuracy: 0.8494 \n",
      "Loss: 258.2978718679467 \n",
      "\n",
      "Epoch: 39..\n",
      "train Accuracy: 0.8930666666666667 \n",
      "Validation Accuracy: 0.8482 \n",
      "Loss: 254.25083703536774 \n",
      "\n",
      "Epoch: 40..\n",
      "train Accuracy: 0.8939111111111111 \n",
      "Validation Accuracy: 0.8462 \n",
      "Loss: 250.78119064335294 \n",
      "\n",
      "Epoch: 41..\n",
      "train Accuracy: 0.8943111111111111 \n",
      "Validation Accuracy: 0.8582666666666666 \n",
      "Loss: 251.33873992349703 \n",
      "\n",
      "Epoch: 42..\n",
      "train Accuracy: 0.8955333333333333 \n",
      "Validation Accuracy: 0.8578666666666667 \n",
      "Loss: 249.74052700980747 \n",
      "\n",
      "Epoch: 43..\n",
      "train Accuracy: 0.8960666666666667 \n",
      "Validation Accuracy: 0.8567333333333333 \n",
      "Loss: 251.73559064169126 \n",
      "\n",
      "Epoch: 44..\n",
      "train Accuracy: 0.8964444444444445 \n",
      "Validation Accuracy: 0.8576666666666667 \n",
      "Loss: 251.53260511652311 \n",
      "\n",
      "Epoch: 45..\n",
      "train Accuracy: 0.8965333333333333 \n",
      "Validation Accuracy: 0.8588 \n",
      "Loss: 250.28892296477676 \n",
      "\n",
      "Epoch: 46..\n",
      "train Accuracy: 0.8968444444444444 \n",
      "Validation Accuracy: 0.8584 \n",
      "Loss: 245.14737340018115 \n",
      "\n",
      "Epoch: 47..\n",
      "train Accuracy: 0.8975555555555556 \n",
      "Validation Accuracy: 0.8592666666666666 \n",
      "Loss: 249.1232272243337 \n",
      "\n",
      "Epoch: 48..\n",
      "train Accuracy: 0.8981555555555556 \n",
      "Validation Accuracy: 0.8589333333333333 \n",
      "Loss: 243.36227770314446 \n",
      "\n",
      "Epoch: 49..\n",
      "train Accuracy: 0.8984888888888889 \n",
      "Validation Accuracy: 0.8592666666666666 \n",
      "Loss: 239.31016160032866 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, batch_size=1000,learning_rate=0.2,epochs=50)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'yhat_train' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-21e7dcd9f6ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-129-77e6a3e7d998>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, batch_size, epochs, verbose)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;31m# Calculate train and Test Accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m         \u001b[0maccuracy_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m         \u001b[0maccuracy_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'yhat_train' referenced before assignment"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.1, 0.1, 0.1, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.1,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "(32,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (32,) and (1,1) not aligned: 32 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-9e969c7d6dab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, epochs, verbose)\u001b[0m\n\u001b[0;32m    314\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion_MSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[1;31m# update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-cac0158b0f54>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta, output_layer)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_vector\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (32,) and (1,1) not aligned: 32 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,512,128,32,10],activation=[None, 'logistic', 'logistic', 'logistic', 'softmax'], dropout=[0.0, 0.0, 0.0, 0.0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.02,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tensorflow.ConfigProto( device_count = {'GPU': 1 , 'CPU': 12} ) \n",
    "sess = tensorflow.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        \n",
    "        ### new version\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        #y_train_dummies = np.array(pd.get_dummies(y_train))\n",
    "        #y_val_dummies = np.array(pd.get_dummies(y_train))\n",
    "\n",
    "        losses = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        \n",
    "        # Mini-Batch/Batch Gradient Descent\n",
    "        if batch_size:\n",
    "            batches = self.get_batches(X_train, y_train, batch_size)\n",
    "            for k in range(epochs):\n",
    "                \n",
    "                loss = []\n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "            \n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "                \n",
    "                self.train()\n",
    "                for X_batch,y_batch in batches:\n",
    "                    # forward pass\n",
    "                    y_hat_batch = self.forward(X_batch)\n",
    "                    \n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        l,delta=self.criterion_CELoss(y_batch, y_hat_batch)\n",
    "                        loss.append(l)\n",
    "                    else:\n",
    "                        l,delta=self.criterion_MSE(y_batch, y_hat_batch)\n",
    "                        loss.append(l)\n",
    "                        \n",
    "                    self.backward(delta)\n",
    "                    \n",
    "                    # update\n",
    "                    self.update(learning_rate/batch_size)\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "                losses[k] = np.mean(loss)\n",
    "        \n",
    "        # Stochastic Gradient Descent\n",
    "        else:\n",
    "            for e in range(epochs):\n",
    "                loss=np.zeros(X_train.shape[0])         \n",
    "            \n",
    "                self.test()\n",
    "                yhat_train = self.forward(X_train)\n",
    "                yhat_val = self.forward(X_val)\n",
    "            \n",
    "                # Calculate train and Test Accuracy\n",
    "                accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "                accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "            \n",
    "                self.train()\n",
    "                for it in range(X_train.shape[0]):\n",
    "                    i=np.random.randint(X_train.shape[0])\n",
    "                \n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X_train[i])\n",
    "\n",
    "                    # backward pass\n",
    "                    if self.activation[-1] == 'softmax':\n",
    "                        loss[it],delta = self.criterion_CELoss(y_train[i],y_hat)\n",
    "                    else:\n",
    "                        loss[it],delta = self.criterion_MSE(y_train[i],y_hat)\n",
    "                \n",
    "                    self.backward(delta)\n",
    "\n",
    "                    # update\n",
    "                    self.update(learning_rate)\n",
    "                    \n",
    "                self.test()\n",
    "                accuracies_val.append(accuracy_train)\n",
    "                accuracies_test.append(accuracy_val)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras import optimizers, metrics, Sequential\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "\n",
    "y = label\n",
    "X = data\n",
    "\n",
    "y_dummies = np.array(pd.get_dummies(y))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=0.25, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "#scaler = Normalizer()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45000/45000 [==============================] - 2s 42us/step - loss: 0.9828 - acc: 0.6622 - categorical_accuracy: 0.6622\n",
      "Epoch 2/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.5128 - acc: 0.8221 - categorical_accuracy: 0.8221\n",
      "Epoch 3/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.4623 - acc: 0.8380 - categorical_accuracy: 0.8380\n",
      "Epoch 4/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.4270 - acc: 0.8482 - categorical_accuracy: 0.8482 0s - loss: 0.4199 - acc:\n",
      "Epoch 5/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.4106 - acc: 0.8544 - categorical_accuracy: 0.8544\n",
      "Epoch 6/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3929 - acc: 0.8596 - categorical_accuracy: 0.8596\n",
      "Epoch 7/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3740 - acc: 0.8663 - categorical_accuracy: 0.8663\n",
      "Epoch 8/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3626 - acc: 0.8705 - categorical_accuracy: 0.8705\n",
      "Epoch 9/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.3487 - acc: 0.8740 - categorical_accuracy: 0.8740 1s - loss: 0.3495 - \n",
      "Epoch 10/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.3452 - acc: 0.8761 - categorical_accuracy: 0.8761\n",
      "Epoch 11/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.3330 - acc: 0.8787 - categorical_accuracy: 0.8787\n",
      "Epoch 12/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3254 - acc: 0.8820 - categorical_accuracy: 0.8820\n",
      "Epoch 13/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3232 - acc: 0.8825 - categorical_accuracy: 0.8825\n",
      "Epoch 14/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3121 - acc: 0.8869 - categorical_accuracy: 0.8869\n",
      "Epoch 15/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3076 - acc: 0.8879 - categorical_accuracy: 0.8879\n",
      "Epoch 16/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.3045 - acc: 0.8894 - categorical_accuracy: 0.8894\n",
      "Epoch 17/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.2975 - acc: 0.8918 - categorical_accuracy: 0.8918\n",
      "Epoch 18/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.2938 - acc: 0.8924 - categorical_accuracy: 0.8924\n",
      "Epoch 19/20\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.2911 - acc: 0.8944 - categorical_accuracy: 0.8944\n",
      "Epoch 20/20\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.2862 - acc: 0.8959 - categorical_accuracy: 0.8959\n"
     ]
    }
   ],
   "source": [
    "with tf.device('GPU'):\n",
    "    sgd = optimizers.sgd(momentum=0.9)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy',metrics.categorical_accuracy])\n",
    "    model.fit(X_train, y_train, batch_size=100, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f974249e5a64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0myhat_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "yhat_val = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "assert len(backend.tensorflow_backend._get_available_gpus()) > 0\n",
    "\n",
    "# confirm PyTorch sees the GPU\n",
    "#from torch import cuda\n",
    "#assert cuda.is_available()\n",
    "#assert cuda.device_count() > 0\n",
    "#print(cuda.get_device_name(cuda.current_device()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  (a * (1 - a ))\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        #return np.exp(x)/(np.sum(np.exp(x),axis=1)[:,None])\n",
    "        return (np.exp(x)/(np.sum(np.exp(x))))\n",
    "    \n",
    "    def __softmax_deriv(self, a):\n",
    "        #a = softmax(x)\n",
    "        return (a * (1 - a))\n",
    "    \n",
    "    def __ReLU(self,x):\n",
    "        return np.vectorize(lambda x:x if x>0 else 0)(x)\n",
    "    \n",
    "    def __ReLU_deriv(self,a):\n",
    "        #a = ReLU()\n",
    "        return np.vectorize(lambda x:1 if x>0 else 0)(a)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__ReLU\n",
    "            self.f_deriv = self.__ReLU_deriv\n",
    "            \n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', dropout=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout=dropout\n",
    "        self.dropout_vector = None\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, mode):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        if (mode=='train' and self.dropout>0):\n",
    "            self.dropout_vector = np.random.binomial(1, 1-self.dropout, size=input.shape)/(1-self.dropout)\n",
    "            lin_output = np.dot(self.dropout_vector*input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):\n",
    "        self.grad_W = (np.atleast_2d(self.dropout_vector*self.input if self.dropout>0 else self.input).T.dot(np.atleast_2d(delta)))\n",
    "        self.grad_b = delta\n",
    "        \n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], dropout=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.mode = 'train'\n",
    "        self.activation=activation\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],self.dropout[i]))\n",
    "            \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input=input, mode=self.mode)\n",
    "            input=output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=error**2\n",
    "        # calculate the delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)    \n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    def criterion_CELoss(self,y,y_hat):\n",
    "        error = y*np.log(y_hat)\n",
    "        loss = -np.sum(error)\n",
    "        delta = (y_hat-y)\n",
    "        return loss,delta\n",
    "        \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=10):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            loss=np.zeros(X.shape[0])\n",
    "            for it in range(X.shape[0]):\n",
    "                i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                # forward pass\n",
    "                y_hat = self.forward(X[i])\n",
    "                \n",
    "                # backward pass\n",
    "                if self.activation[-1] == 'softmax':\n",
    "                    loss[it],delta=self.criterion_CELoss(y[i],y_hat)\n",
    "                else:\n",
    "                    loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
    "                \n",
    "                self.backward(delta)\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "            to_return[k] = np.mean(loss)\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.test()\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = self.forward(x[i,:])\n",
    "        return output\n",
    "    \n",
    "    def optimize(self, X, y, learning_rate=0.01, test_size=0.25, epochs=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        losses = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            loss=np.zeros(X_train.shape[0])         \n",
    "            \n",
    "            self.test()\n",
    "            yhat_train = self.forward(X_train)\n",
    "            yhat_val = self.forward(X_val)\n",
    "            \n",
    "            # Calculate train and Test Accuracy\n",
    "            accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "            accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "            \n",
    "            self.train()\n",
    "            for it in range(X_train.shape[0]):\n",
    "                i=np.random.randint(X_train.shape[0])\n",
    "                \n",
    "                \n",
    "                # forward pass\n",
    "                y_hat = self.forward(X_train[i])\n",
    "\n",
    "                # backward pass\n",
    "                if self.activation[-1] == 'softmax':\n",
    "                    loss[it],delta = self.criterion_CELoss(y_train[i],y_hat)\n",
    "                else:\n",
    "                    loss[it],delta = self.criterion_MSE(y_train[i],y_hat)\n",
    "                \n",
    "                self.backward(delta)\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "                \n",
    "            self.test()\n",
    "            yhat_train = self.forward(X_train)\n",
    "            yhat_val = self.forward(X_val)\n",
    "            accuracies_val.append(accuracy_train)\n",
    "            accuracies_test.append(accuracy_val)\n",
    "            \n",
    "            if verbose:\n",
    "                print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                      format(e, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "            \n",
    "            losses[e] = np.mean(loss)\n",
    "        return losses, accuracies_val, accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.10335555555555556 \n",
      "Validation Accuracy: 0.12133333333333333 \n",
      "Loss: 0.5532555242731952 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.8624444444444445 \n",
      "Validation Accuracy: 0.8476 \n",
      "Loss: 0.37749771970350426 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.8786222222222222 \n",
      "Validation Accuracy: 0.8607333333333334 \n",
      "Loss: 0.34724235141318954 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.8777555555555555 \n",
      "Validation Accuracy: 0.8522 \n",
      "Loss: 0.3258745821826974 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.8846888888888889 \n",
      "Validation Accuracy: 0.8623333333333333 \n",
      "Loss: 0.3154830288160891 \n",
      "\n",
      "Epoch: 5..\n",
      "train Accuracy: 0.8840222222222223 \n",
      "Validation Accuracy: 0.8590666666666666 \n",
      "Loss: 0.3015172914055102 \n",
      "\n",
      "Epoch: 6..\n",
      "train Accuracy: 0.8864222222222222 \n",
      "Validation Accuracy: 0.8550666666666666 \n",
      "Loss: 0.2997706931625835 \n",
      "\n",
      "Epoch: 7..\n",
      "train Accuracy: 0.896 \n",
      "Validation Accuracy: 0.8651333333333333 \n",
      "Loss: 0.2874445842110733 \n",
      "\n",
      "Epoch: 8..\n",
      "train Accuracy: 0.8951111111111111 \n",
      "Validation Accuracy: 0.8610666666666666 \n",
      "Loss: 0.2829739796502393 \n",
      "\n",
      "Epoch: 9..\n",
      "train Accuracy: 0.9034222222222222 \n",
      "Validation Accuracy: 0.8659333333333333 \n",
      "Loss: 0.28053984068422455 \n",
      "\n",
      "Epoch: 10..\n",
      "train Accuracy: 0.9030222222222222 \n",
      "Validation Accuracy: 0.8686 \n",
      "Loss: 0.2735993293433253 \n",
      "\n",
      "Epoch: 11..\n",
      "train Accuracy: 0.9030222222222222 \n",
      "Validation Accuracy: 0.8621333333333333 \n",
      "Loss: 0.2707044452122889 \n",
      "\n",
      "Epoch: 12..\n",
      "train Accuracy: 0.9046888888888889 \n",
      "Validation Accuracy: 0.8623333333333333 \n",
      "Loss: 0.26307320098229986 \n",
      "\n",
      "Epoch: 13..\n",
      "train Accuracy: 0.9066444444444445 \n",
      "Validation Accuracy: 0.8572666666666666 \n",
      "Loss: 0.2582650011840468 \n",
      "\n",
      "Epoch: 14..\n",
      "train Accuracy: 0.9088222222222222 \n",
      "Validation Accuracy: 0.8679333333333333 \n",
      "Loss: 0.25837061537209344 \n",
      "\n",
      "Epoch: 15..\n",
      "train Accuracy: 0.9069555555555555 \n",
      "Validation Accuracy: 0.8641333333333333 \n",
      "Loss: 0.2538596109721026 \n",
      "\n",
      "Epoch: 16..\n",
      "train Accuracy: 0.9108666666666667 \n",
      "Validation Accuracy: 0.8655333333333334 \n",
      "Loss: 0.2523245778270437 \n",
      "\n",
      "Epoch: 17..\n",
      "train Accuracy: 0.9142888888888889 \n",
      "Validation Accuracy: 0.8652666666666666 \n",
      "Loss: 0.2527750593484879 \n",
      "\n",
      "Epoch: 18..\n",
      "train Accuracy: 0.9155555555555556 \n",
      "Validation Accuracy: 0.8667333333333334 \n",
      "Loss: 0.2486751582987458 \n",
      "\n",
      "Epoch: 19..\n",
      "train Accuracy: 0.9122444444444444 \n",
      "Validation Accuracy: 0.864 \n",
      "Loss: 0.24786185588027249 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0, 0, 0, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.01,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0..\n",
      "train Accuracy: 0.09351111111111111 \n",
      "Validation Accuracy: 0.09346666666666667 \n",
      "Loss: 0.5388548534102287 \n",
      "\n",
      "Epoch: 1..\n",
      "train Accuracy: 0.8551111111111112 \n",
      "Validation Accuracy: 0.8465333333333334 \n",
      "Loss: 0.3802074027278582 \n",
      "\n",
      "Epoch: 2..\n",
      "train Accuracy: 0.8582 \n",
      "Validation Accuracy: 0.8448 \n",
      "Loss: 0.3581316120003945 \n",
      "\n",
      "Epoch: 3..\n",
      "train Accuracy: 0.8690222222222223 \n",
      "Validation Accuracy: 0.8458666666666667 \n",
      "Loss: 0.3527146564401976 \n",
      "\n",
      "Epoch: 4..\n",
      "train Accuracy: 0.8786 \n",
      "Validation Accuracy: 0.8554 \n",
      "Loss: 0.3339477350807514 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,64,32,10],activation=[None, 'ReLU', 'ReLU', 'softmax'], dropout=[0.2, 0.2, 0.2, 0])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.01,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

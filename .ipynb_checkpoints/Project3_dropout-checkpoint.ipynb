{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return (1.0 / (1.0 + np.exp(-x)))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  (a * (1 - a ))\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        #return np.exp(x)/(np.sum(np.exp(x),axis=1)[:,None])\n",
    "        return (np.exp(x)/(np.sum(np.exp(x))))\n",
    "    \n",
    "    def __softmax_deriv(self, a):\n",
    "        #a = softmax(x)\n",
    "        return (a * (1 - a))\n",
    "    \n",
    "    def __ReLU(self,x):\n",
    "        return np.vectorize(lambda x:x if x>0 else 0)(x)\n",
    "    \n",
    "    def __ReLU_deriv(self,a):\n",
    "        #a = ReLU()\n",
    "        return np.vectorize(lambda x:1 if x>0 else 0)(a)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__ReLU\n",
    "            self.f_deriv = self.__ReLU_deriv\n",
    "            \n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', dropout=None, W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout=dropout\n",
    "        self.dropout_vector = None\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "    def forward(self, input, mode):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        if (mode=='train' and self.dropout>0):\n",
    "            self.dropout_vector = np.random.binomial(1, 1-self.dropout, size=input.shape)/(1-self.dropout)\n",
    "            lin_output = np.dot(self.dropout_vector*input, self.W) + self.b\n",
    "            self.output = (\n",
    "                lin_output if self.activation is None\n",
    "                else self.activation(lin_output)\n",
    "            )\n",
    "\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        print(input)\n",
    "        print('selfinput',self.input)\n",
    "        print(self.dropout_vector)\n",
    "        self.input=input\n",
    "        print(self.input)\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):\n",
    "        self.mode='train'\n",
    "        if self.dropout > 0:\n",
    "            delta *= self.dropout_vector\n",
    "        \n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = delta\n",
    "        \n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "\n",
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"      \n",
    "    def __init__(self, layers, activation=[None,'tanh','tanh'], dropout=None):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.mode = 'train'\n",
    "        self.activation=activation\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],self.dropout[i]))\n",
    "            \n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input=input, mode=self.mode)\n",
    "            input=output\n",
    "        return output\n",
    "\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=error**2\n",
    "        # calculate the delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)    \n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    def criterion_CELoss(self,y,y_hat):\n",
    "        error = y*np.log(y_hat)\n",
    "        loss = -np.sum(error)\n",
    "        delta = (y_hat-y)\n",
    "        return loss,delta\n",
    "        \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for layer in self.layers:\n",
    "            layer.W -= lr * layer.grad_W\n",
    "            layer.b -= lr * layer.grad_b\n",
    "\n",
    "    def fit(self,X,y,learning_rate=0.1, epochs=10):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            loss=np.zeros(X.shape[0])\n",
    "            for it in range(X.shape[0]):\n",
    "                i=np.random.randint(X.shape[0])\n",
    "                \n",
    "                # forward pass\n",
    "                y_hat = self.forward(X[i])\n",
    "                \n",
    "                # backward pass\n",
    "                if self.activation[-1] == 'softmax':\n",
    "                    loss[it],delta=self.criterion_CELoss(y[i],y_hat)\n",
    "                else:\n",
    "                    loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
    "                \n",
    "                self.backward(delta)\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "            to_return[k] = np.mean(loss)\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.test()\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = self.forward(x[i,:])\n",
    "        return output\n",
    "    \n",
    "    def optimize(self, X, y, learning_rate=0.01, test_size=0.25, epochs=10, verbose=True):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        \"\"\"\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        y_dummies = np.array(pd.get_dummies(y))\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y_dummies, test_size=test_size, shuffle=True)\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = Normalizer()\n",
    "        #scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        losses = np.zeros(epochs)\n",
    "        accuracies_val = []\n",
    "        accuracies_test = []\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            loss=np.zeros(X_train.shape[0])         \n",
    "            \n",
    "            #yhat_train = self.forward(X_train)\n",
    "            #yhat_val = self.forward(X_val)\n",
    "            \n",
    "            # Calculate train and Test Accuracy\n",
    "            #accuracy_train = (np.sum(np.argmax(np.array(y_train),axis=1)==np.argmax(yhat_train,axis=1)))/(y_train.shape[0])\n",
    "            #accuracy_val = (np.sum(np.argmax(np.array(y_val),axis=1)==np.argmax(yhat_val,axis=1)))/(y_val.shape[0])\n",
    "            \n",
    "            for it in range(X_train.shape[0]):\n",
    "                i=np.random.randint(X_train.shape[0])\n",
    "                \n",
    "                self.train()\n",
    "                # forward pass\n",
    "                y_hat = self.forward(X_train[i])\n",
    "\n",
    "                # backward pass\n",
    "                if self.activation[-1] == 'softmax':\n",
    "                    loss[it],delta = self.criterion_CELoss(y_train[i],y_hat)\n",
    "                else:\n",
    "                    loss[it],delta=self.criterion_MSE(y_train[i],y_hat)\n",
    "                \n",
    "                self.backward(delta)\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "                \n",
    "            #yhat_train = self.forward(X_train)\n",
    "            #yhat_val = self.forward(X_val)\n",
    "\n",
    "            accuracies_val.append(accuracy_train)\n",
    "            accuracies_test.append(accuracy_val)\n",
    "            \n",
    "            self.train()\n",
    "            \n",
    "            if verbose:\n",
    "                print('Epoch: {}..\\ntrain Accuracy: {} \\nValidation Accuracy: {} \\nLoss: {} \\n'.\n",
    "                      format(e, accuracy_train, accuracy_val, np.mean(loss)))\n",
    "            \n",
    "            losses[e] = np.mean(loss)\n",
    "        return losses, accuracies_val, accuracies_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-e383d4b5e272>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#mlp = MLP([128,32,10],activation=[None, 'logistic', 'softmax'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-147-1d126b090cfd>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layers, activation, dropout)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHiddenLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "#mlp = MLP([128,32,10],activation=[None, 'logistic', 'softmax'])\n",
    "mlp = MLP([128,32,10],activation=[None, 'ReLU', 'softmax'])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.01,epochs=10)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.05778516  0.473979    1.30842024 -0.4751532  -0.57902824 -1.26005558\n",
      " -1.04704671  0.48388604  0.37575033  0.66095703  1.17493792  0.57578042\n",
      " -0.29310547 -0.62536469  0.2442653  -0.73312024 -0.52946692  0.18123304\n",
      "  1.68951533 -0.71163625 -0.2034722   0.22573052 -0.75961942  0.65940552\n",
      "  0.91735877  0.51433631  0.42766449 -0.88326779 -0.15511747 -1.00288678\n",
      " -1.39356599 -0.29283443 -0.90515907 -0.16275315  0.63808467 -0.43505649\n",
      "  0.76398826 -2.26756638  0.72358976  0.33745921  0.22530707 -0.65888664\n",
      "  1.14345481  0.29907087  0.38691209  1.40419022  0.4453685   1.21220925\n",
      " -0.51607733 -1.29167199 -0.39479709 -0.40576701  0.8253675  -0.73374311\n",
      "  1.15314528 -0.65281253 -0.14220278  0.29070868 -0.35013578  2.48518638\n",
      " -1.06700246  1.32632607  0.34402067  1.22272509 -0.96487919 -1.70813368\n",
      "  0.54510194 -0.74637    -0.05374896 -0.49385284 -0.10574678  1.15076542\n",
      "  1.43349247  1.03318065  0.19081451 -0.49215808 -0.20377494  0.12594166\n",
      "  0.42243633 -0.96857765  0.26784373 -0.83710436  0.7075111   1.0497428\n",
      " -0.39321468  0.85796144  0.33404852  1.42750823 -0.40689806  0.40190024\n",
      " -0.17515613  0.66904459  0.82141653  0.99255297  0.33441494 -0.67767153\n",
      "  0.1871074  -0.61242559  0.21606161 -0.13489866  0.61924954 -1.23752663\n",
      "  0.85737847 -0.82955162 -0.85120711  1.74613706 -0.78840128  0.51597689\n",
      " -0.29258731  0.18183201 -1.38754332 -0.25823574  0.18272409 -0.62073279\n",
      " -0.41506572 -1.02218201  1.4933733  -0.82815475 -0.17456298  0.20282167\n",
      " -1.61535967 -0.09803558  0.955005   -0.26352063 -0.83038884 -0.44665228\n",
      " -0.71515071 -0.69767067]\n",
      "selfinput None\n",
      "[2. 2. 0. 0. 2. 0. 2. 0. 0. 0. 0. 2. 0. 0. 2. 0. 0. 0. 0. 0. 2. 0. 0. 2.\n",
      " 2. 2. 2. 0. 2. 0. 2. 2. 2. 0. 0. 0. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 2.\n",
      " 0. 0. 0. 2. 2. 0. 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 0. 0. 2. 2. 2. 0.\n",
      " 0. 0. 2. 0. 0. 2. 0. 2. 2. 0. 0. 0. 0. 2. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2.\n",
      " 2. 0. 0. 2. 2. 2. 2. 0. 0. 0. 0. 2. 2. 0. 0. 2. 2. 2. 0. 0. 2. 0. 0. 2.\n",
      " 2. 0. 0. 0. 2. 2. 0. 2.]\n",
      "[ 1.05778516  0.473979    1.30842024 -0.4751532  -0.57902824 -1.26005558\n",
      " -1.04704671  0.48388604  0.37575033  0.66095703  1.17493792  0.57578042\n",
      " -0.29310547 -0.62536469  0.2442653  -0.73312024 -0.52946692  0.18123304\n",
      "  1.68951533 -0.71163625 -0.2034722   0.22573052 -0.75961942  0.65940552\n",
      "  0.91735877  0.51433631  0.42766449 -0.88326779 -0.15511747 -1.00288678\n",
      " -1.39356599 -0.29283443 -0.90515907 -0.16275315  0.63808467 -0.43505649\n",
      "  0.76398826 -2.26756638  0.72358976  0.33745921  0.22530707 -0.65888664\n",
      "  1.14345481  0.29907087  0.38691209  1.40419022  0.4453685   1.21220925\n",
      " -0.51607733 -1.29167199 -0.39479709 -0.40576701  0.8253675  -0.73374311\n",
      "  1.15314528 -0.65281253 -0.14220278  0.29070868 -0.35013578  2.48518638\n",
      " -1.06700246  1.32632607  0.34402067  1.22272509 -0.96487919 -1.70813368\n",
      "  0.54510194 -0.74637    -0.05374896 -0.49385284 -0.10574678  1.15076542\n",
      "  1.43349247  1.03318065  0.19081451 -0.49215808 -0.20377494  0.12594166\n",
      "  0.42243633 -0.96857765  0.26784373 -0.83710436  0.7075111   1.0497428\n",
      " -0.39321468  0.85796144  0.33404852  1.42750823 -0.40689806  0.40190024\n",
      " -0.17515613  0.66904459  0.82141653  0.99255297  0.33441494 -0.67767153\n",
      "  0.1871074  -0.61242559  0.21606161 -0.13489866  0.61924954 -1.23752663\n",
      "  0.85737847 -0.82955162 -0.85120711  1.74613706 -0.78840128  0.51597689\n",
      " -0.29258731  0.18183201 -1.38754332 -0.25823574  0.18272409 -0.62073279\n",
      " -0.41506572 -1.02218201  1.4933733  -0.82815475 -0.17456298  0.20282167\n",
      " -1.61535967 -0.09803558  0.955005   -0.26352063 -0.83038884 -0.44665228\n",
      " -0.71515071 -0.69767067]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-225-4bd76f21834f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-220-68e91ffabb34>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, X, y, learning_rate, test_size, epochs, verbose)\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m                 \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m                 \u001b[1;31m# backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-220-68e91ffabb34>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-220-68e91ffabb34>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, mode)\u001b[0m\n\u001b[0;32m    103\u001b[0m         '''\n\u001b[0;32m    104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'train'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m             \u001b[0mlin_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout_vector\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             self.output = (\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/train_128.h5','r') as H:\n",
    "    data = np.copy(H['data'])\n",
    "with h5py.File('data/train_label.h5','r') as H:\n",
    "    label = np.copy(H['label'])\n",
    "    \n",
    "mlp = MLP([128,32,10],activation=[None, 'ReLU', 'softmax'], dropout=[0.5, 0.5, None])\n",
    "\n",
    "losses, accuracies_train, accuracies_test = mlp.optimize(data, label, learning_rate=0.01,epochs=20)\n",
    "\n",
    "plt.plot(accuracies_train, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_sigmoid.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
